{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88bc1073",
   "metadata": {},
   "source": [
    "### Lv2 튜닝 1/4 python 파이썬 랜덤 포레스트 변수 중요도 확인\n",
    "\n",
    "Once we finish the training using .fit(), we can analyze the importance of the parameters using `model.feature_importances_`.<br>\n",
    "\n",
    "The concept of the importance of the parameters is that how much each feature does important role when we decide the prediction parameter.<br>\n",
    "\n",
    "If a feature does nothing important while the training process, we're gonna get rid of it to improve the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea028248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-09-08 18:17:42--  https://bit.ly/3gLj0Q6\n",
      "Resolving bit.ly (bit.ly)... 67.199.248.11, 67.199.248.10\n",
      "Connecting to bit.ly (bit.ly)|67.199.248.11|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://drive.google.com/uc?export=download&id=1or_QN1ksv81DNog6Tu_kWcZ5jJWf5W9E [following]\n",
      "--2022-09-08 18:17:43--  https://drive.google.com/uc?export=download&id=1or_QN1ksv81DNog6Tu_kWcZ5jJWf5W9E\n",
      "Resolving drive.google.com (drive.google.com)... 172.217.161.238, 2404:6800:400a:80c::200e\n",
      "Connecting to drive.google.com (drive.google.com)|172.217.161.238|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://doc-0c-10-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/3qush354o474mtq0l0fekt0uqhqm9klc/1662628650000/17946651057176172524/*/1or_QN1ksv81DNog6Tu_kWcZ5jJWf5W9E?e=download&uuid=6b3796ac-6cab-484d-8a35-90bab60a6021 [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2022-09-08 18:17:43--  https://doc-0c-10-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/3qush354o474mtq0l0fekt0uqhqm9klc/1662628650000/17946651057176172524/*/1or_QN1ksv81DNog6Tu_kWcZ5jJWf5W9E?e=download&uuid=6b3796ac-6cab-484d-8a35-90bab60a6021\n",
      "Resolving doc-0c-10-docs.googleusercontent.com (doc-0c-10-docs.googleusercontent.com)... 142.250.206.193, 2404:6800:400a:80a::2001\n",
      "Connecting to doc-0c-10-docs.googleusercontent.com (doc-0c-10-docs.googleusercontent.com)|142.250.206.193|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 39208 (38K) [application/zip]\n",
      "Saving to: ‘3gLj0Q6’\n",
      "\n",
      "3gLj0Q6             100%[===================>]  38.29K  --.-KB/s    in 0.05s   \n",
      "\n",
      "2022-09-08 18:17:44 (846 KB/s) - ‘3gLj0Q6’ saved [39208/39208]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Downloading data\n",
    "!wget 'https://bit.ly/3gLj0Q6'\n",
    "\n",
    "# Unzip the downloaded data\n",
    "import zipfile\n",
    "with zipfile.ZipFile('3gLj0Q6', 'r') as existing_zip:\n",
    "    existing_zip.extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfc219dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas and RandomForestRegressor\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f07ade30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "157183fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Train Data ============\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1459 entries, 0 to 1458\n",
      "Data columns (total 11 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   id                      1459 non-null   int64  \n",
      " 1   hour                    1459 non-null   int64  \n",
      " 2   hour_bef_temperature    1457 non-null   float64\n",
      " 3   hour_bef_precipitation  1457 non-null   float64\n",
      " 4   hour_bef_windspeed      1450 non-null   float64\n",
      " 5   hour_bef_humidity       1457 non-null   float64\n",
      " 6   hour_bef_visibility     1457 non-null   float64\n",
      " 7   hour_bef_ozone          1383 non-null   float64\n",
      " 8   hour_bef_pm10           1369 non-null   float64\n",
      " 9   hour_bef_pm2.5          1342 non-null   float64\n",
      " 10  count                   1459 non-null   float64\n",
      "dtypes: float64(9), int64(2)\n",
      "memory usage: 125.5 KB\n",
      "Train Data Information\n",
      " None \n",
      "\n",
      "Train Data Shape:  (1459, 11) \n",
      "\n",
      "============ Test Data ============\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 715 entries, 0 to 714\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   id                      715 non-null    int64  \n",
      " 1   hour                    715 non-null    int64  \n",
      " 2   hour_bef_temperature    714 non-null    float64\n",
      " 3   hour_bef_precipitation  714 non-null    float64\n",
      " 4   hour_bef_windspeed      714 non-null    float64\n",
      " 5   hour_bef_humidity       714 non-null    float64\n",
      " 6   hour_bef_visibility     714 non-null    float64\n",
      " 7   hour_bef_ozone          680 non-null    float64\n",
      " 8   hour_bef_pm10           678 non-null    float64\n",
      " 9   hour_bef_pm2.5          679 non-null    float64\n",
      "dtypes: float64(8), int64(2)\n",
      "memory usage: 56.0 KB\n",
      "Test Data Information\n",
      " None \n",
      "\n",
      "Test Data Shape:  (715, 10) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if the data loading is successful\n",
    "print('============ Train Data ============\\n')\n",
    "print('Train Data Information\\n', train.info(), '\\n')\n",
    "print('Train Data Shape: ', train.shape, '\\n')\n",
    "\n",
    "print('============ Test Data ============')\n",
    "print('Test Data Information\\n', test.info(), '\\n')\n",
    "print('Test Data Shape: ', test.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "782a8bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                          0\n",
      "hour                        0\n",
      "hour_bef_temperature        2\n",
      "hour_bef_precipitation      2\n",
      "hour_bef_windspeed          9\n",
      "hour_bef_humidity           2\n",
      "hour_bef_visibility         2\n",
      "hour_bef_ozone             76\n",
      "hour_bef_pm10              90\n",
      "hour_bef_pm2.5            117\n",
      "count                       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check if there are missing values\n",
    "print(train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08f0fa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the missing values using linear interpolation\n",
    "train.interpolate(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9090724e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                        0\n",
      "hour                      0\n",
      "hour_bef_temperature      0\n",
      "hour_bef_precipitation    0\n",
      "hour_bef_windspeed        0\n",
      "hour_bef_humidity         0\n",
      "hour_bef_visibility       0\n",
      "hour_bef_ozone            0\n",
      "hour_bef_pm10             0\n",
      "hour_bef_pm2.5            0\n",
      "count                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the result of the linear interpolation\n",
    "print(train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c5c9d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                         0\n",
      "hour                       0\n",
      "hour_bef_temperature       1\n",
      "hour_bef_precipitation     1\n",
      "hour_bef_windspeed         1\n",
      "hour_bef_humidity          1\n",
      "hour_bef_visibility        1\n",
      "hour_bef_ozone            35\n",
      "hour_bef_pm10             37\n",
      "hour_bef_pm2.5            36\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the test data has null values\n",
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a74da7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the null value in test dataset with zero.\n",
    "test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef3c9b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                        0\n",
      "hour                      0\n",
      "hour_bef_temperature      0\n",
      "hour_bef_precipitation    0\n",
      "hour_bef_windspeed        0\n",
      "hour_bef_humidity         0\n",
      "hour_bef_visibility       0\n",
      "hour_bef_ozone            0\n",
      "hour_bef_pm10             0\n",
      "hour_bef_pm2.5            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the replacement is done properly\n",
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "439c6e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare the model\n",
    "X_train = train.drop(['count'], axis=1)\n",
    "Y_train = train['count']\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestRegressor(criterion = 'squared_error')\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e40b21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02538698, 0.59011586, 0.18453969, 0.01953541, 0.026807  ,\n",
       "       0.03696223, 0.0313451 , 0.0331706 , 0.03178317, 0.02035396])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the feature importances\n",
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1af510e",
   "metadata": {},
   "source": [
    "### Lv2 튜닝 2/4 python 파이썬 변수 제거\n",
    "\n",
    "After we evaluate the feature importances, we can train the model alongside the deletion of the less important features.<br>\n",
    "\n",
    "First of all, 'id' and 'count' are not important for prediction at all.<br>\n",
    "Thus, we will create a new dataframe by droping 'id' and 'count' feature.<br>\n",
    "\n",
    "When we predict something, the test dataset must have exactly the same features as the training dataset, thus we will drop the same features from the existing test dataset if there are any.<br>\n",
    "\n",
    "Also, we can create another new dataset by dropping another less important feature.<br>\n",
    "\n",
    "Let's say we're gonna drop 'hour_bef_windspeed' and 'hour_bef_pm2.5' as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3850fb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train datasets\n",
    "X_train1 = train.drop(['count', 'id'], axis=1)\n",
    "X_train2 = train.drop(['count', 'id', 'hour_bef_windspeed'], axis=1)\n",
    "X_train3 = train.drop(['count', 'id', 'hour_bef_windspeed', 'hour_bef_pm2.5'], axis=1)\n",
    "\n",
    "# Create test datasets\n",
    "test1 = test.drop(['id'], axis=1)\n",
    "test2 = test.drop(['id', 'hour_bef_windspeed'], axis=1)\n",
    "test3 = test.drop(['id', 'hour_bef_windspeed', 'hour_bef_pm2.5'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "608e6525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train1.shape : (1459, 9)\n",
      "X_train2.shape : (1459, 8)\n",
      "X_train3.shape : (1459, 7)\n",
      "test1.shape : (715, 9)\n",
      "test2.shape : (715, 8)\n",
      "test3.shape : (715, 7)\n"
     ]
    }
   ],
   "source": [
    "# Check if the datasets formed properly\n",
    "print('X_train1.shape :', X_train1.shape)\n",
    "print('X_train2.shape :', X_train2.shape)\n",
    "print('X_train3.shape :', X_train3.shape)\n",
    "print('test1.shape :', test1.shape)\n",
    "print('test2.shape :', test2.shape)\n",
    "print('test3.shape :', test3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "777a9e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for each datasets\n",
    "# We're going to create separate models for each dataset\n",
    "model1 = RandomForestRegressor(criterion = 'squared_error')\n",
    "model2 = RandomForestRegressor(criterion = 'squared_error')\n",
    "model3 = RandomForestRegressor(criterion = 'squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2285d907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train each model\n",
    "model1.fit(X_train1, Y_train)\n",
    "model2.fit(X_train2, Y_train)\n",
    "model3.fit(X_train3, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c5b8125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with models\n",
    "prediction1 = model1.predict(test1)\n",
    "prediction2 = model2.predict(test2)\n",
    "prediction3 = model3.predict(test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "71d30fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predictions\n",
    "result1 = pd.read_csv('data/submission.csv')\n",
    "result2 = pd.read_csv('data/submission.csv')\n",
    "result3 = pd.read_csv('data/submission.csv')\n",
    "\n",
    "result1['count'] = prediction1\n",
    "result2['count'] = prediction2\n",
    "result3['count'] = prediction3\n",
    "\n",
    "result1.to_csv('result1.csv', index=False)\n",
    "result2.to_csv('result2.csv', index=False)\n",
    "result3.to_csv('result3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe115fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}