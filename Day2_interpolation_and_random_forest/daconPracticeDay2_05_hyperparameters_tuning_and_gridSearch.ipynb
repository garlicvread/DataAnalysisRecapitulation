{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d912416",
   "metadata": {},
   "source": [
    "### The concept of hyperparameter\n",
    "Hyperparameter tuning is one of the most important parts of a machine learning pipeline. A wrong choice of the hyperparameters‚Äô values may lead to wrong results and a model with poor performance.<br>\n",
    "\n",
    "Hyperparameters are model parameters whose values are set **before** training.<br> These hyperparameters might address model design questions such as:\n",
    "\n",
    "- What **degree of polynomial features** should I use for my linear model?\n",
    "- What should be the **maximum depth** allowed for my decision tree?\n",
    "- What should be the **minimum number of samples** required at a leaf node in my decision tree?\n",
    "- **How many trees** should I include in my random forest?\n",
    "- **How many neurons** should I have in my neural network layer?\n",
    "- **How many layers** should I have in my neural network?\n",
    "- What should I set my **learning rate** to for gradient descent?\n",
    "\n",
    "Let's make it simple. For example, **the number of neurons** of a feed-forward neural network is a hyperparameter, because we set it before training. Another example of hyperparameter is **the number of trees** in a random forest or the penalty intensity of a Lasso regression. As you can see, the hyperparameters are all numbers that are set before the training phase and their values affect the behavior of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb8eaee",
   "metadata": {},
   "source": [
    "### IMPORTANT!\n",
    "Hyperparameters are **not** model parameters and they cannot be directly trained from the data. Model parameters are **learned** during training when we optimize a loss function using something like gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76db0ac3",
   "metadata": {},
   "source": [
    "\n",
    "### The reason for tuning the hyperparameters\n",
    "Why should we tune the hyperparameters of a model?<br>\n",
    "\n",
    "That is because we don‚Äôt really know the models' optimal values in advance. A model with different hyperparameters is, actually, a different model so it may have a lower performance.<br>\n",
    "\n",
    "In the case of neural networks, a low number of neurons could lead to underfitting and a high number could lead to overfitting.<br>\n",
    "\n",
    "In both cases, the model is not good, so we need to find the intermediate number of neurons that leads to the best performance.<br>\n",
    "\n",
    "If the model has several hyperparameters, we need to find the best combination of values of the hyperparameters searching in a multi-dimensional space. That‚Äôs why hyperparameter tuning, which is the process of finding the right values of the hyperparameters, is a very complex and time-expensive task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfeee4d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Hyperparameter tuning in practice\n",
    "Tuning hyperparameters means making decisions on the **stopping criteria**. There are several stopping criteria, but we're going to deal with four first, such as:\n",
    "1. The max_depth\n",
    "2. The minimum size of the node: min_samples_split\n",
    "3. The minimum lift: min_impurity_decrease\n",
    "4. The cost-complexity<br>\n",
    "---\n",
    "The **max depth** means the maximum number of depth in the decision tree. The tree structure cannot be deeper than this value we set using **`max_depth`**. The smaller it is, the smaller the tree will be.<br>\n",
    "\n",
    "The **minimum size of the node** is the number of data(samples) to split. The smaller the value, the larger the tree will be, and its default value is 2.<br>\n",
    "\n",
    "We can set this using **`min_samples_split`** A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The equation for min_sample_split is:<br>\n",
    "\n",
    "$$\\frac{N_t}{N} \\times (impurity - \\frac{N_{tR}}{N_t} \\times right\\;impurity - \\frac{N_{tl}}{N_t} \\times left\\;impurity)$$\n",
    "\n",
    "Where<br>\n",
    "$N$ is the total number of samples<br>\n",
    "$N_t$ is the total number of samples in current node<br>\n",
    "$N_{tL}$ is the number of samples in the left child<br>\n",
    "$N_{tR}$ is the number of samples in the right child<br>\n",
    "$N$, $N_t$, $N_{tL}$, $N_{tR}$ are all refer to the weighted sum, if `sample_weight` is passed.<br>\n",
    "\n",
    "The **minimum lift** is a criterion to see if the association rules between the items are coincidental or not. We can set the minimum lift using **`min_impurity_decrease`**.<br>\n",
    "\n",
    "When the lift is the same or smaller than the value set, the tree will not split more. The smaller the value, the larger the tree will be.<br>\n",
    "\n",
    "For pruning, we can think of two types of it. The first is **pre-pruning**, and the other is **post-pruning**. Pre-pruning is also called **early stopping**. It means literally stopping the training early. And we can do it by setting the max depth or the number of branches. Post-pruning is the process of performing pruning after we train the model. We can do post-pruning using the cost-complexity pruning technique.<br>\n",
    "\n",
    "The **cost complexity** is a concept that is used in **cost complexity pruning**. Pruning is a technique to prevent overfitting by limiting the model by setting penalty coefficients for the impurity and for the decision tree being larger.<br>\n",
    "\n",
    "In practice, we can do cost complexity pruning by finding the **$\\alpha$** value with the least influence and prune the node with that value. The equation for cost complexity pruning is:\n",
    "\n",
    "$$R_\\alpha (T) = R(T) + \\alpha |T|$$\n",
    "\n",
    "where<br>\n",
    "$R(T)$ is the learning errors of the leaf nodes<br>\n",
    "$|T|$ is the number of leaf nodes<br>\n",
    "$\\alpha$ is the complexity parameter\n",
    "\n",
    "When we focus on reducing the ¬†ùëÖ(ùëá) ¬†value only, the size of the tree gets bigger. It means the tree structure has more branches. ¬†ùõº decides the number of leaf nodes to be remained, thus we need to modify it to prevent overfitting. The bigger the ¬†ùõº ¬†value, the more nodes being pruned will be.<br>\n",
    "\n",
    "Note that we need to calculate the $R_\\alpha (T_t)$ for the sub-trees. The equation is very similar to above one.\n",
    "\n",
    "$$R_\\alpha (T_t) = R(T_t) + \\alpha |T_t|$$\n",
    "\n",
    "---\n",
    "Using the stopping criteria such as above, we can set the optimal conditions for model training, and this process is called hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfc1e54",
   "metadata": {},
   "source": [
    "### GridSearch\n",
    "\n",
    "Amongst the hyperparameter tuning techniques, GridSearch, a sort of exhaustive search, shows the best performance. GridSearch is a technique that finds the best combination among the possible combinations. However, GridSearch also has cons because the training consumes a lot of time.<br>\n",
    "\n",
    "For now, we wikk implement an exhaustive search using GreadSearCV module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6962d5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-09-09 14:50:31--  https://bit.ly/3gLj0Q6\n",
      "Resolving bit.ly (bit.ly)... 67.199.248.11, 67.199.248.10\n",
      "Connecting to bit.ly (bit.ly)|67.199.248.11|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://drive.google.com/uc?export=download&id=1or_QN1ksv81DNog6Tu_kWcZ5jJWf5W9E [following]\n",
      "--2022-09-09 14:50:31--  https://drive.google.com/uc?export=download&id=1or_QN1ksv81DNog6Tu_kWcZ5jJWf5W9E\n",
      "Resolving drive.google.com (drive.google.com)... 142.251.42.174, 2404:6800:4004:81f::200e\n",
      "Connecting to drive.google.com (drive.google.com)|142.251.42.174|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://doc-0c-10-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/4lmbksq3d47utu7tgmd34l658f6ij1np/1662702600000/17946651057176172524/*/1or_QN1ksv81DNog6Tu_kWcZ5jJWf5W9E?e=download&uuid=3dbd5e33-6ec0-4d37-8c6f-858a08f03c4c [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2022-09-09 14:50:32--  https://doc-0c-10-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/4lmbksq3d47utu7tgmd34l658f6ij1np/1662702600000/17946651057176172524/*/1or_QN1ksv81DNog6Tu_kWcZ5jJWf5W9E?e=download&uuid=3dbd5e33-6ec0-4d37-8c6f-858a08f03c4c\n",
      "Resolving doc-0c-10-docs.googleusercontent.com (doc-0c-10-docs.googleusercontent.com)... 142.250.196.97, 2404:6800:4004:821::2001\n",
      "Connecting to doc-0c-10-docs.googleusercontent.com (doc-0c-10-docs.googleusercontent.com)|142.250.196.97|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 39208 (38K) [application/zip]\n",
      "Saving to: ‚Äò3gLj0Q6‚Äô\n",
      "\n",
      "3gLj0Q6             100%[===================>]  38.29K  --.-KB/s    in 0.05s   \n",
      "\n",
      "2022-09-09 14:50:33 (836 KB/s) - ‚Äò3gLj0Q6‚Äô saved [39208/39208]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Downloading data\n",
    "!wget 'https://bit.ly/3gLj0Q6'\n",
    "\n",
    "# Unzip the downloaded data\n",
    "import zipfile\n",
    "with zipfile.ZipFile('3gLj0Q6', 'r') as existing_zip:\n",
    "    existing_zip.extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7a0e87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas and RandomForestRegressor\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e92700d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feca6915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Train Data ============\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1459 entries, 0 to 1458\n",
      "Data columns (total 11 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   id                      1459 non-null   int64  \n",
      " 1   hour                    1459 non-null   int64  \n",
      " 2   hour_bef_temperature    1457 non-null   float64\n",
      " 3   hour_bef_precipitation  1457 non-null   float64\n",
      " 4   hour_bef_windspeed      1450 non-null   float64\n",
      " 5   hour_bef_humidity       1457 non-null   float64\n",
      " 6   hour_bef_visibility     1457 non-null   float64\n",
      " 7   hour_bef_ozone          1383 non-null   float64\n",
      " 8   hour_bef_pm10           1369 non-null   float64\n",
      " 9   hour_bef_pm2.5          1342 non-null   float64\n",
      " 10  count                   1459 non-null   float64\n",
      "dtypes: float64(9), int64(2)\n",
      "memory usage: 125.5 KB\n",
      "Train Data Information\n",
      " None \n",
      "\n",
      "Train Data Shape:  (1459, 11) \n",
      "\n",
      "============ Test Data ============\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 715 entries, 0 to 714\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   id                      715 non-null    int64  \n",
      " 1   hour                    715 non-null    int64  \n",
      " 2   hour_bef_temperature    714 non-null    float64\n",
      " 3   hour_bef_precipitation  714 non-null    float64\n",
      " 4   hour_bef_windspeed      714 non-null    float64\n",
      " 5   hour_bef_humidity       714 non-null    float64\n",
      " 6   hour_bef_visibility     714 non-null    float64\n",
      " 7   hour_bef_ozone          680 non-null    float64\n",
      " 8   hour_bef_pm10           678 non-null    float64\n",
      " 9   hour_bef_pm2.5          679 non-null    float64\n",
      "dtypes: float64(8), int64(2)\n",
      "memory usage: 56.0 KB\n",
      "Test Data Information\n",
      " None \n",
      "\n",
      "Test Data Shape:  (715, 10) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if the data loading is successful\n",
    "print('============ Train Data ============\\n')\n",
    "print('Train Data Information\\n', train.info(), '\\n')\n",
    "print('Train Data Shape: ', train.shape, '\\n')\n",
    "\n",
    "print('============ Test Data ============')\n",
    "print('Test Data Information\\n', test.info(), '\\n')\n",
    "print('Test Data Shape: ', test.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a09a9ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                          0\n",
      "hour                        0\n",
      "hour_bef_temperature        2\n",
      "hour_bef_precipitation      2\n",
      "hour_bef_windspeed          9\n",
      "hour_bef_humidity           2\n",
      "hour_bef_visibility         2\n",
      "hour_bef_ozone             76\n",
      "hour_bef_pm10              90\n",
      "hour_bef_pm2.5            117\n",
      "count                       0\n",
      "dtype: int64 \n",
      "\n",
      "id                         0\n",
      "hour                       0\n",
      "hour_bef_temperature       1\n",
      "hour_bef_precipitation     1\n",
      "hour_bef_windspeed         1\n",
      "hour_bef_humidity          1\n",
      "hour_bef_visibility        1\n",
      "hour_bef_ozone            35\n",
      "hour_bef_pm10             37\n",
      "hour_bef_pm2.5            36\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check if there are missing values\n",
    "print(train.isnull().sum(), '\\n')\n",
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df1595e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the missing values using linear interpolation\n",
    "train.interpolate(inplace=True)\n",
    "test.interpolate(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62d2a959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                        0\n",
      "hour                      0\n",
      "hour_bef_temperature      0\n",
      "hour_bef_precipitation    0\n",
      "hour_bef_windspeed        0\n",
      "hour_bef_humidity         0\n",
      "hour_bef_visibility       0\n",
      "hour_bef_ozone            0\n",
      "hour_bef_pm10             0\n",
      "hour_bef_pm2.5            0\n",
      "count                     0\n",
      "dtype: int64 \n",
      "\n",
      "id                        0\n",
      "hour                      0\n",
      "hour_bef_temperature      0\n",
      "hour_bef_precipitation    0\n",
      "hour_bef_windspeed        0\n",
      "hour_bef_humidity         0\n",
      "hour_bef_visibility       0\n",
      "hour_bef_ozone            0\n",
      "hour_bef_pm10             0\n",
      "hour_bef_pm2.5            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check if the null values are replaced well.\n",
    "print(train.isnull().sum(), '\\n')\n",
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac77eb6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare the model\n",
    "X_train = train.drop(['count'], axis=1)\n",
    "Y_train = train['count']\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestRegressor(criterion = 'squared_error')\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a206751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02521903, 0.58832904, 0.18330378, 0.01971992, 0.02660254,\n",
       "       0.03499978, 0.03188481, 0.0380771 , 0.0310252 , 0.0208388 ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the feature importances\n",
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77253a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train datasets by removing the less important features\n",
    "X_train1 = train.drop(['count', 'id'], axis=1)\n",
    "X_train2 = train.drop(['count', 'id', 'hour_bef_windspeed'], axis=1)\n",
    "X_train3 = train.drop(['count', 'id', 'hour_bef_windspeed', 'hour_bef_pm2.5'], axis=1)\n",
    "\n",
    "Y_train = train['count']\n",
    "\n",
    "# Create test datasets\n",
    "test1 = test.drop(['id'], axis=1)\n",
    "test2 = test.drop(['id', 'hour_bef_windspeed'], axis=1)\n",
    "test3 = test.drop(['id', 'hour_bef_windspeed', 'hour_bef_pm2.5'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "253704f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train1.shape:  (1459, 9) \n",
      "\n",
      "X_train2.shape:  (1459, 8) \n",
      "\n",
      "X_train3.shape:  (1459, 7) \n",
      "\n",
      "Y_train.shape:  (1459,) \n",
      "\n",
      "test1.shape (715, 9) \n",
      "\n",
      "test2.shape (715, 8) \n",
      "\n",
      "test3.shape (715, 7) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of training and test data\n",
    "print('X_train1.shape: ', X_train1.shape, '\\n')\n",
    "print('X_train2.shape: ', X_train2.shape, '\\n')\n",
    "print('X_train3.shape: ', X_train3.shape, '\\n')\n",
    "print('Y_train.shape: ', Y_train.shape, '\\n')\n",
    "print('test1.shape', test1.shape, '\\n')\n",
    "print('test2.shape', test2.shape, '\\n')\n",
    "print('test3.shape', test3.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dc3bb8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare separate models\n",
    "model1 = RandomForestRegressor(criterion = 'squared_error')\n",
    "model2 = RandomForestRegressor(criterion = 'squared_error')\n",
    "model3 = RandomForestRegressor(criterion = 'squared_error')\n",
    "\n",
    "# Train the saparated models\n",
    "model1.fit(X_train1, Y_train)\n",
    "model2.fit(X_train2, Y_train)\n",
    "model3.fit(X_train3, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb07143",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### RandomForest Hyperparameters\n",
    "\n",
    "**n_estimators:** Number of decision making tree\n",
    "- Default = 10\n",
    "- When increase it, the performance may get better, but may cause too much train time.<br>\n",
    "\n",
    "**min_samples_split**: The minimum number of sample used to split node\n",
    "- Used to control overfitting\n",
    "- Default = 2: The smaller the value, the greater possibility of overfitting because of the increasing node split<br>\n",
    "\n",
    "**min_samples_leaf**: The minimum number of samples to be leaf node\n",
    "- Along to min_samples_split, it is used to control the overfitting\n",
    "- When the data is imbalanced, some data of a specific class may extremely small, thus it needs to be kept the small value<br>\n",
    "\n",
    "**max_features**: Maximum number of features for optimal split\n",
    "- Default = 'auto'\n",
    "    - Note: The default value of max_feature is none in decision tree\n",
    "- When specified in int type: The number of features\n",
    "- When specified in float type: The ratio of features\n",
    "- 'sqrt' or 'auto': Samples as many as $\\sqrt{The\\;number\\;of\\;whole\\;features}$\n",
    "- log : Samples as many as $\\log_2{(The\\;number\\;of\\;whole\\;features)}$<br>\n",
    "\n",
    "**max_depth**: Maximum depth of the tree\n",
    "- Default = none\n",
    "    - Split until the class value is completely determined\n",
    "    - Or until the number of data is less than min_samples_split\n",
    "- As the depth increases, it may overfit, so proper control is required.<br>\n",
    "\n",
    "**max_leaf_nodes**: The maximum number of leaf nodes\n",
    "\n",
    "### GridSearchCV initializer\n",
    "- estimator: classifier, regressor, pipeline, and so on.\n",
    "\n",
    "- param_grid: In the dictionary type, input the parameters that are going to be used for parameter tuning.\n",
    "\n",
    "- scoring: Method to evaluate the prediction performance. Usually set to accuracy.\n",
    "\n",
    "- cv: Specifies the number of divisions in cross-validation(The number of fold).\n",
    "\n",
    "- refit: The default value is True. When it is set default, it finds the optimal hyperparameter and retrains it.\n",
    "\n",
    "- n_jobs: The default value is 1, Set -1 to use all cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "model = RandomForestRegressor(criterion = 'mse',\n",
    "                              random_state=2022)\n",
    "\n",
    "params = {'n_estimators': [200, 300, 500],\n",
    "          'max_features': [5, 6, 8],\n",
    "          'min_samples_leaf': [1, 3, 5]}\n",
    "\n",
    "# Declare GridSearchCV for each model \n",
    "greedy_CV1 = GridSearchCV(model1,\n",
    "                          param_grid=params,\n",
    "                          cv = 3,\n",
    "                          n_jobs = -1)\n",
    "\n",
    "greedy_CV2 = GridSearchCV(model2,\n",
    "                          param_grid=params,\n",
    "                          cv = 3,\n",
    "                          n_jobs = -1)\n",
    "\n",
    "greedy_CV3 = GridSearchCV(model3,\n",
    "                          param_grid=params,\n",
    "                          cv = 3,\n",
    "                          n_jobs = -1)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train for each dataset\n",
    "greedy_CV1.fit(X_train1, Y_train)\n",
    "greedy_CV2.fit(X_train2, Y_train)\n",
    "greedy_CV3.fit(X_train3, Y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Processing time: \", end_time-start_time, 'seconds.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Predict with each trained model\n",
    "prediction1 = greedy_CV1.predict(test1)\n",
    "prediction2 = greedy_CV2.predict(test2)\n",
    "prediction3 = greedy_CV3.predict(test3)\n",
    "\n",
    "print(prediction1)\n",
    "print(prediction2)\n",
    "print(prediction3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the prediction results\n",
    "GridSearchCV1 = pd.read_csv('data/submission.csv')\n",
    "GridSearchCV2 = pd.read_csv('data/submission.csv')\n",
    "GridSearchCV3 = pd.read_csv('data/submission.csv')\n",
    "\n",
    "import numpy as np\n",
    "GridSearchCV1['count'] = np.round(prediction1, 2)\n",
    "GridSearchCV2['count'] = np.round(prediction2, 2)\n",
    "GridSearchCV3['count'] = np.round(prediction3, 2)\n",
    "\n",
    "print(GridSearchCV1.head(), '\\n\\n', GridSearchCV2.head(), '\\n\\n', GridSearchCV3.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the results\n",
    "GridSearchCV1.to_csv('GridSearchCV1_result.csv')\n",
    "GridSearchCV2.to_csv('GridSearchCV2_result.csv')\n",
    "GridSearchCV3.to_csv('GridSearchCV3_result.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}