{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d912416",
   "metadata": {},
   "source": [
    "### The concept of hyperparameter\n",
    "Hyperparameter tuning is one of the most important parts of a machine learning pipeline. A wrong choice of the hyperparameters‚Äô values may lead to wrong results and a model with poor performance.<br>\n",
    "\n",
    "Hyperparameters are model parameters whose values are set **before** training.<br> These hyperparameters might address model design questions such as:\n",
    "\n",
    "- What **degree of polynomial features** should I use for my linear model?\n",
    "- What should be the **maximum depth** allowed for my decision tree?\n",
    "- What should be the **minimum number of samples** required at a leaf node in my decision tree?\n",
    "- **How many trees** should I include in my random forest?\n",
    "- **How many neurons** should I have in my neural network layer?\n",
    "- **How many layers** should I have in my neural network?\n",
    "- What should I set my **learning rate** to for gradient descent?\n",
    "\n",
    "Let's make it simple. For example, **the number of neurons** of a feed-forward neural network is a hyperparameter, because we set it before training. Another example of hyperparameter is **the number of trees** in a random forest or the penalty intensity of a Lasso regression. As you can see, the hyperparameters are all numbers that are set before the training phase and their values affect the behavior of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb8eaee",
   "metadata": {},
   "source": [
    "### IMPORTANT!\n",
    "Hyperparameters are **not** model parameters and they cannot be directly trained from the data. Model parameters are **learned** during training when we optimize a loss function using something like gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76db0ac3",
   "metadata": {},
   "source": [
    "\n",
    "### The reason for tuning the hyperparameters\n",
    "Why should we tune the hyperparameters of a model?<br>\n",
    "\n",
    "That is because we don‚Äôt really know the models' optimal values in advance. A model with different hyperparameters is, actually, a different model so it may have a lower performance.<br>\n",
    "\n",
    "In the case of neural networks, a low number of neurons could lead to underfitting and a high number could lead to overfitting.<br>\n",
    "\n",
    "In both cases, the model is not good, so we need to find the intermediate number of neurons that leads to the best performance.<br>\n",
    "\n",
    "If the model has several hyperparameters, we need to find the best combination of values of the hyperparameters searching in a multi-dimensional space. That‚Äôs why hyperparameter tuning, which is the process of finding the right values of the hyperparameters, is a very complex and time-expensive task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfeee4d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Hyperparameter tuning in practice\n",
    "Tuning hyperparameters means making decisions on the **stopping criteria**. There are several stopping criteria, but we're going to deal with four first, such as:\n",
    "1. The max_depth\n",
    "2. The minimum size of the node: min_samples_split\n",
    "3. The minimum lift: min_impurity_decrease\n",
    "4. The cost-complexity<br>\n",
    "---\n",
    "The **max depth** means the maximum number of depth in the decision tree. The tree structure cannot be deeper than this value we set using **`max_depth`**. The smaller it is, the smaller the tree will be.<br>\n",
    "\n",
    "The **minimum size of the node** is the number of data(samples) to split. The smaller the value, the larger the tree will be, and its default value is 2.<br>\n",
    "\n",
    "We can set this using **`min_samples_split`** A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The equation for min_sample_split is:<br>\n",
    "\n",
    "$$\\frac{N_t}{N} \\times (impurity - \\frac{N_{tR}}{N_t} \\times right\\;impurity - \\frac{N_{tl}}{N_t} \\times left\\;impurity)$$\n",
    "\n",
    "Where<br>\n",
    "$N$ is the total number of samples<br>\n",
    "$N_t$ is the total number of samples in current node<br>\n",
    "$N_{tL}$ is the number of samples in the left child<br>\n",
    "$N_{tR}$ is the number of samples in the right child<br>\n",
    "$N$, $N_t$, $N_{tL}$, $N_{tR}$ are all refer to the weighted sum, if `sample_weight` is passed.<br>\n",
    "\n",
    "The **minimum lift** is a criterion to see if the association rules between the items are coincidental or not. We can set the minimum lift using **`min_impurity_decrease`**.<br>\n",
    "\n",
    "When the lift is the same or smaller than the value set, the tree will not split more. The smaller the value, the larger the tree will be.<br>\n",
    "\n",
    "For pruning, we can think of two types of it. The first is **pre-pruning**, and the other is **post-pruning**. Pre-pruning is also called **early stopping**. It means literally stopping the training early. And we can do it by setting the max depth or the number of branches. Post-pruning is the process of performing pruning after we train the model. We can do post-pruning using the cost-complexity pruning technique.<br>\n",
    "\n",
    "The **cost complexity** is a concept that is used in **cost complexity pruning**. Pruning is a technique to prevent overfitting by limiting the model by setting penalty coefficients for the impurity and for the decision tree being larger.<br>\n",
    "\n",
    "In practice, we can do cost complexity pruning by finding the **$\\alpha$** value with the least influence and prune the node with that value. The equation for cost complexity pruning is:\n",
    "\n",
    "$$R_\\alpha (T) = R(T) + \\alpha |T|$$\n",
    "\n",
    "where<br>\n",
    "$R(T)$ is the learning errors of the leaf nodes<br>\n",
    "$|T|$ is the number of leaf nodes<br>\n",
    "$\\alpha$ is the complexity parameter\n",
    "\n",
    "When we focus on reducing the ¬†ùëÖ(ùëá) ¬†value only, the size of the tree gets bigger. It means the tree structure has more branches. ¬†ùõº decides the number of leaf nodes to be remained, thus we need to modify it to prevent overfitting. The bigger the ¬†ùõº ¬†value, the more nodes being pruned will be.<br>\n",
    "\n",
    "Note that we need to calculate the $R_\\alpha (T_t)$ for the sub-trees. The equation is very similar to above one.\n",
    "\n",
    "$$R_\\alpha (T_t) = R(T_t) + \\alpha |T_t|$$\n",
    "\n",
    "---\n",
    "Using the stopping criteria such as above, we can set the optimal conditions for model training, and this process is called hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfc1e54",
   "metadata": {},
   "source": [
    "### GridSearch\n",
    "\n",
    "Amongst the hyperparameter tuning techniques, GridSearch, a sort of exhaustive search, shows the best performance. GridSearch is a technique that finds the best combination among the possible combinations. However, GridSearch also has cons because the training consumes a lot of time.<br>\n",
    "\n",
    "For now, we wikk implement an exhaustive search using GreadSearCV module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6962d5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-09-09 15:26:09--  https://bit.ly/3gLj0Q6\n",
      "Resolving bit.ly (bit.ly)... 67.199.248.10, 67.199.248.11\n",
      "Connecting to bit.ly (bit.ly)|67.199.248.10|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://drive.google.com/uc?export=download&id=1or_QN1ksv81DNog6Tu_kWcZ5jJWf5W9E [following]\n",
      "--2022-09-09 15:26:10--  https://drive.google.com/uc?export=download&id=1or_QN1ksv81DNog6Tu_kWcZ5jJWf5W9E\n",
      "Resolving drive.google.com (drive.google.com)... 142.251.42.142, 2404:6800:4004:81d::200e\n",
      "Connecting to drive.google.com (drive.google.com)|142.251.42.142|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://doc-0c-10-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ibo6ol0ant2ejsram50360eiraf020h2/1662704700000/17946651057176172524/*/1or_QN1ksv81DNog6Tu_kWcZ5jJWf5W9E?e=download&uuid=cafeca96-1204-4b01-a9f7-9928fbbec87c [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2022-09-09 15:26:10--  https://doc-0c-10-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/ibo6ol0ant2ejsram50360eiraf020h2/1662704700000/17946651057176172524/*/1or_QN1ksv81DNog6Tu_kWcZ5jJWf5W9E?e=download&uuid=cafeca96-1204-4b01-a9f7-9928fbbec87c\n",
      "Resolving doc-0c-10-docs.googleusercontent.com (doc-0c-10-docs.googleusercontent.com)... 142.250.196.97, 2404:6800:4004:81c::2001\n",
      "Connecting to doc-0c-10-docs.googleusercontent.com (doc-0c-10-docs.googleusercontent.com)|142.250.196.97|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 39208 (38K) [application/zip]\n",
      "Saving to: ‚Äò3gLj0Q6‚Äô\n",
      "\n",
      "3gLj0Q6             100%[===================>]  38.29K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2022-09-09 15:26:11 (896 KB/s) - ‚Äò3gLj0Q6‚Äô saved [39208/39208]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Downloading data\n",
    "!wget 'https://bit.ly/3gLj0Q6'\n",
    "\n",
    "# Unzip the downloaded data\n",
    "import zipfile\n",
    "with zipfile.ZipFile('3gLj0Q6', 'r') as existing_zip:\n",
    "    existing_zip.extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7a0e87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas and RandomForestRegressor\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e92700d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feca6915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Train Data ============\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1459 entries, 0 to 1458\n",
      "Data columns (total 11 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   id                      1459 non-null   int64  \n",
      " 1   hour                    1459 non-null   int64  \n",
      " 2   hour_bef_temperature    1457 non-null   float64\n",
      " 3   hour_bef_precipitation  1457 non-null   float64\n",
      " 4   hour_bef_windspeed      1450 non-null   float64\n",
      " 5   hour_bef_humidity       1457 non-null   float64\n",
      " 6   hour_bef_visibility     1457 non-null   float64\n",
      " 7   hour_bef_ozone          1383 non-null   float64\n",
      " 8   hour_bef_pm10           1369 non-null   float64\n",
      " 9   hour_bef_pm2.5          1342 non-null   float64\n",
      " 10  count                   1459 non-null   float64\n",
      "dtypes: float64(9), int64(2)\n",
      "memory usage: 125.5 KB\n",
      "Train Data Information\n",
      " None \n",
      "\n",
      "Train Data Shape:  (1459, 11) \n",
      "\n",
      "============ Test Data ============\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 715 entries, 0 to 714\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   id                      715 non-null    int64  \n",
      " 1   hour                    715 non-null    int64  \n",
      " 2   hour_bef_temperature    714 non-null    float64\n",
      " 3   hour_bef_precipitation  714 non-null    float64\n",
      " 4   hour_bef_windspeed      714 non-null    float64\n",
      " 5   hour_bef_humidity       714 non-null    float64\n",
      " 6   hour_bef_visibility     714 non-null    float64\n",
      " 7   hour_bef_ozone          680 non-null    float64\n",
      " 8   hour_bef_pm10           678 non-null    float64\n",
      " 9   hour_bef_pm2.5          679 non-null    float64\n",
      "dtypes: float64(8), int64(2)\n",
      "memory usage: 56.0 KB\n",
      "Test Data Information\n",
      " None \n",
      "\n",
      "Test Data Shape:  (715, 10) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if the data loading is successful\n",
    "print('============ Train Data ============\\n')\n",
    "print('Train Data Information\\n', train.info(), '\\n')\n",
    "print('Train Data Shape: ', train.shape, '\\n')\n",
    "\n",
    "print('============ Test Data ============')\n",
    "print('Test Data Information\\n', test.info(), '\\n')\n",
    "print('Test Data Shape: ', test.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a09a9ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                          0\n",
      "hour                        0\n",
      "hour_bef_temperature        2\n",
      "hour_bef_precipitation      2\n",
      "hour_bef_windspeed          9\n",
      "hour_bef_humidity           2\n",
      "hour_bef_visibility         2\n",
      "hour_bef_ozone             76\n",
      "hour_bef_pm10              90\n",
      "hour_bef_pm2.5            117\n",
      "count                       0\n",
      "dtype: int64 \n",
      "\n",
      "id                         0\n",
      "hour                       0\n",
      "hour_bef_temperature       1\n",
      "hour_bef_precipitation     1\n",
      "hour_bef_windspeed         1\n",
      "hour_bef_humidity          1\n",
      "hour_bef_visibility        1\n",
      "hour_bef_ozone            35\n",
      "hour_bef_pm10             37\n",
      "hour_bef_pm2.5            36\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check if there are missing values\n",
    "print(train.isnull().sum(), '\\n')\n",
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df1595e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the missing values using linear interpolation\n",
    "train.interpolate(inplace=True)\n",
    "test.interpolate(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62d2a959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                        0\n",
      "hour                      0\n",
      "hour_bef_temperature      0\n",
      "hour_bef_precipitation    0\n",
      "hour_bef_windspeed        0\n",
      "hour_bef_humidity         0\n",
      "hour_bef_visibility       0\n",
      "hour_bef_ozone            0\n",
      "hour_bef_pm10             0\n",
      "hour_bef_pm2.5            0\n",
      "count                     0\n",
      "dtype: int64 \n",
      "\n",
      "id                        0\n",
      "hour                      0\n",
      "hour_bef_temperature      0\n",
      "hour_bef_precipitation    0\n",
      "hour_bef_windspeed        0\n",
      "hour_bef_humidity         0\n",
      "hour_bef_visibility       0\n",
      "hour_bef_ozone            0\n",
      "hour_bef_pm10             0\n",
      "hour_bef_pm2.5            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check if the null values are replaced well.\n",
    "print(train.isnull().sum(), '\\n')\n",
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac77eb6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare the model\n",
    "X_train = train.drop(['count'], axis=1)\n",
    "Y_train = train['count']\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestRegressor(criterion = 'squared_error')\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a206751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02398086, 0.59643309, 0.17837677, 0.01758   , 0.02619895,\n",
       "       0.0376788 , 0.03186152, 0.03681893, 0.03085461, 0.02021648])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the feature importances\n",
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11606f48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the feature importances\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "importance_values = model.feature_importances_\n",
    "importances = pd.Series(importance_values, index = X_train.columns)\n",
    "importance_top10 = importances.sort_values(ascending=False)[:10]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title('Top 10 feature importances')\n",
    "sns.barplot(x = importance_top10, y = importance_top10.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77253a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train datasets by removing the less important features\n",
    "X_train1 = train.drop(['count', 'hour_bef_precipitation'], axis=1)\n",
    "X_train2 = train.drop(['count', 'hour_bef_precipitation', 'hour_bef_pm2.5'], axis=1)\n",
    "X_train3 = train.drop(['count', 'hour_bef_precipitation', 'hour_bef_pm2.5', 'id'], axis=1)\n",
    "X_train4 = train.drop(['count', 'hour_bef_precipitation', 'hour_bef_pm2.5', 'id', 'hour_bef_windspeed'], axis=1)\n",
    "\n",
    "Y_train = train['count']\n",
    "\n",
    "# Create test datasets\n",
    "test1 = test.drop(['hour_bef_precipitation'], axis=1)\n",
    "test2 = test.drop(['hour_bef_precipitation', 'hour_bef_pm2.5'], axis=1)\n",
    "test3 = test.drop(['hour_bef_precipitation', 'hour_bef_pm2.5', 'id'], axis=1)\n",
    "test4 = test.drop(['hour_bef_precipitation', 'hour_bef_pm2.5', 'id', 'hour_bef_windspeed'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "253704f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train1.shape:  (1459, 9) \n",
      "\n",
      "X_train2.shape:  (1459, 8) \n",
      "\n",
      "X_train3.shape:  (1459, 7) \n",
      "\n",
      "X_train4.shape:  (1459, 6) \n",
      "\n",
      "Y_train.shape:  (1459,) \n",
      "\n",
      "test1.shape (715, 9) \n",
      "\n",
      "test2.shape (715, 8) \n",
      "\n",
      "test3.shape (715, 7) \n",
      "\n",
      "test4.shape (715, 6) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of training and test data\n",
    "print('X_train1.shape: ', X_train1.shape, '\\n')\n",
    "print('X_train2.shape: ', X_train2.shape, '\\n')\n",
    "print('X_train3.shape: ', X_train3.shape, '\\n')\n",
    "print('X_train4.shape: ', X_train4.shape, '\\n')\n",
    "print('Y_train.shape: ', Y_train.shape, '\\n')\n",
    "print('test1.shape', test1.shape, '\\n')\n",
    "print('test2.shape', test2.shape, '\\n')\n",
    "print('test3.shape', test3.shape, '\\n')\n",
    "print('test4.shape', test4.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dc3bb8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare separate models\n",
    "model1 = RandomForestRegressor(criterion = 'squared_error')\n",
    "model2 = RandomForestRegressor(criterion = 'squared_error')\n",
    "model3 = RandomForestRegressor(criterion = 'squared_error')\n",
    "model4 = RandomForestRegressor(criterion = 'squared_error')\n",
    "\n",
    "# Train the saparated models\n",
    "model1.fit(X_train1, Y_train)\n",
    "model2.fit(X_train2, Y_train)\n",
    "model3.fit(X_train3, Y_train)\n",
    "model4.fit(X_train4, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb07143",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### RandomForest Hyperparameters\n",
    "\n",
    "**n_estimators:** Number of decision making tree\n",
    "- Default = 10\n",
    "- When increase it, the performance may get better, but may cause too much train time.<br>\n",
    "\n",
    "**min_samples_split**: The minimum number of sample used to split node\n",
    "- Used to control overfitting\n",
    "- Default = 2: The smaller the value, the greater possibility of overfitting because of the increasing node split<br>\n",
    "\n",
    "**min_samples_leaf**: The minimum number of samples to be leaf node\n",
    "- Along to min_samples_split, it is used to control the overfitting\n",
    "- When the data is imbalanced, some data of a specific class may extremely small, thus it needs to be kept the small value<br>\n",
    "\n",
    "**max_features**: Maximum number of features for optimal split\n",
    "- Default = 'auto'\n",
    "    - Note: The default value of max_feature is none in decision tree\n",
    "- When specified in int type: The number of features\n",
    "- When specified in float type: The ratio of features\n",
    "- 'sqrt' or 'auto': Samples as many as $\\sqrt{The\\;number\\;of\\;whole\\;features}$\n",
    "- log : Samples as many as $\\log_2{(The\\;number\\;of\\;whole\\;features)}$<br>\n",
    "\n",
    "**max_depth**: Maximum depth of the tree\n",
    "- Default = none\n",
    "    - Split until the class value is completely determined\n",
    "    - Or until the number of data is less than min_samples_split\n",
    "- As the depth increases, it may overfit, so proper control is required.<br>\n",
    "\n",
    "**max_leaf_nodes**: The maximum number of leaf nodes\n",
    "\n",
    "### GridSearchCV initializer\n",
    "- estimator: classifier, regressor, pipeline, and so on.\n",
    "\n",
    "- param_grid: In the dictionary type, input the parameters that are going to be used for parameter tuning.\n",
    "\n",
    "- scoring: Method to evaluate the prediction performance. Usually set to accuracy.\n",
    "\n",
    "- cv: Specifies the number of divisions in cross-validation(The number of fold).\n",
    "\n",
    "- refit: The default value is True. When it is set default, it finds the optimal hyperparameter and retrains it.\n",
    "\n",
    "- n_jobs: The default value is 1, Set -1 to use all cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01f7dd87",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "27 fits failed out of a total of 81.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "27 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 1315, in fit\n",
      "    super().fit(\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 308, in fit\n",
      "    raise ValueError(\"max_features must be in (0, n_features]\")\n",
      "ValueError: max_features must be in (0, n_features]\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.75300998 0.75316183 0.75781405 0.75016785 0.75490767 0.75447819\n",
      " 0.74602515 0.74660866 0.7468722  0.75700443 0.75515678 0.75871143\n",
      " 0.75496467 0.7578231  0.75642675 0.74695241 0.74783477 0.74823382\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "27 fits failed out of a total of 81.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "27 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 1315, in fit\n",
      "    super().fit(\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 308, in fit\n",
      "    raise ValueError(\"max_features must be in (0, n_features]\")\n",
      "ValueError: max_features must be in (0, n_features]\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.76010755 0.76193657 0.76253386 0.76068757 0.76138532 0.75853969\n",
      " 0.75317194 0.75004245 0.75424315 0.76065356 0.76071689 0.76094257\n",
      " 0.76239981 0.75908339 0.76066684 0.75251558 0.7533568  0.75448943\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time:  28.71129870414734 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "model = RandomForestRegressor(criterion = 'mse',\n",
    "                              random_state=2022)\n",
    "\n",
    "params = {'n_estimators': [200, 300, 500],\n",
    "          'max_features': [5, 6, 8],\n",
    "          'min_samples_leaf': [1, 3, 5]}\n",
    "\n",
    "# Declare GridSearchCV for each model \n",
    "greedy_CV1 = GridSearchCV(model1,\n",
    "                          param_grid=params,\n",
    "                          cv = 3,\n",
    "                          n_jobs = -1)\n",
    "\n",
    "greedy_CV2 = GridSearchCV(model2,\n",
    "                          param_grid=params,\n",
    "                          cv = 3,\n",
    "                          n_jobs = -1)\n",
    "\n",
    "greedy_CV3 = GridSearchCV(model3,\n",
    "                          param_grid=params,\n",
    "                          cv = 3,\n",
    "                          n_jobs = -1)\n",
    "\n",
    "greedy_CV4 = GridSearchCV(model4,\n",
    "                          param_grid=params,\n",
    "                          cv = 3,\n",
    "                          n_jobs = -1)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train for each dataset\n",
    "greedy_CV1.fit(X_train1, Y_train)\n",
    "greedy_CV2.fit(X_train2, Y_train)\n",
    "greedy_CV3.fit(X_train3, Y_train)\n",
    "greedy_CV4.fit(X_train4, Y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Processing time: \", end_time-start_time, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f62ebe21",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.87153649 220.93630534  94.74674515  40.16582333  46.89640974\n",
      " 129.17611169 172.96752756 306.91949059  40.08967793 120.39274619\n",
      " 307.55572328 253.67508004 109.05305689  42.08068823 213.03201374\n",
      " 156.13451645  26.19274961 192.47906568 335.60966189 159.77826325\n",
      " 228.11882918  85.81709269  26.22435233 144.39028535 142.00388172\n",
      " 116.27053084  26.67613608 120.67401443 115.98845622 151.20036597\n",
      "  78.62782933  36.78833187  63.58824601 132.50679971 274.55173764\n",
      "  37.93326922 133.66000758 169.13166349 215.88838517  81.47139861\n",
      "  59.58768424 123.53177932 163.7918612   86.12102417 327.01138881\n",
      " 187.81312811  87.12307421  61.93339339  18.6079084   89.04025159\n",
      " 248.81614503  94.78168483 147.10532389 123.27784601 203.0182515\n",
      " 149.29850743  50.36643307 183.57377055  25.01972794  18.96481873\n",
      "  99.64686546  89.90431812 254.01167332 303.56361644 149.68239941\n",
      " 312.21936204  24.93445471 213.09923207 142.32289196  32.93879334\n",
      " 105.2914438   32.94174903 148.99882181  15.31479477 319.20233705\n",
      " 231.61262594  35.16406263 185.28987731 239.82232043  28.21702057\n",
      " 255.69907027 131.26579787  95.30708851  83.81973799  91.69016546\n",
      " 321.94729675  56.32035083 159.31078983  98.08694882 277.90830382\n",
      " 284.82544667 163.51707524  66.15886706 105.98600007  41.57595512\n",
      "  84.65885004 102.36180783  28.83135354 222.24762262 150.05361846\n",
      "  19.25876259 159.08277432  41.91133913 117.84793442  70.09579307\n",
      "  49.59148906 103.41548661  26.00446975 173.56759782 137.96129584\n",
      " 184.76192907 240.87676247 177.04431605 117.08627354  62.5292366\n",
      " 126.23096278 241.05138254  39.58091058 215.17830438  19.89182666\n",
      " 106.75359736 107.01815361 197.2473506  111.30524327  50.04276482\n",
      " 121.87041162  56.29437131  16.85905188 213.17347468  59.13269291\n",
      " 131.44691898 149.33163949  19.23036644 133.93125905 302.42028889\n",
      " 130.00960554  43.20014428 144.45449661 244.54031198 350.82060604\n",
      " 166.54323687  33.96754348  34.63638891  91.72451948  98.92528034\n",
      " 120.11731344  87.1379518  126.76553593 150.48129674 192.70253615\n",
      " 131.25800635 295.55354969 309.48885843 219.52295538 170.94019434\n",
      "  14.03230209 121.69064776  23.34428908  88.6670103  138.56717879\n",
      " 136.70102316  26.627356   208.29629281 214.60787817 124.89544784\n",
      "  24.47531435 229.30548189 153.85181212 289.16758185  25.94785492\n",
      " 121.95278038 133.83721407  88.74760134 127.21378571  99.8057534\n",
      " 100.92453103 225.54993056 204.6548102  162.35592529 108.92509829\n",
      " 207.44882876 171.64633156 156.40854484 246.5888072  140.22140996\n",
      "  89.43855921  98.44578576 303.078856   165.71585765  49.68051917\n",
      "  39.1081854  137.8272269  121.12444697 109.95447078 134.28596232\n",
      " 271.56580461 327.43398725 139.23509196 122.87316379 104.99263066\n",
      "  25.84308149 227.7634298   25.63461437 326.44662073 142.45468924\n",
      "  40.74933442 125.99868146 243.88718391 227.11537683  62.22002046\n",
      " 284.78367369 154.4642697  131.8323065  214.56779675 248.85908763\n",
      " 125.2153469  240.35462144 119.60932475 135.41086532  93.55035272\n",
      "  92.36790573  33.19267328 107.82422836  65.81354106 145.82518498\n",
      "  39.8445924  204.04972857 185.31878116  35.40798719 169.3270513\n",
      " 269.17984747 117.28118711  27.28663487  75.79868771 203.21821199\n",
      " 103.25205925 355.65100751 319.07231286 163.30088564 201.52859826\n",
      "  19.75679297 218.05147654  32.6613524  135.76201751 149.50830758\n",
      "  99.95502747 142.36332803 229.23482622 103.12901113  24.14233443\n",
      "  94.88161192 263.08994591 232.47885832 156.45971883 136.3165456\n",
      " 167.46435548  34.9070517  346.28759096  84.6255095   19.41586007\n",
      " 168.22722023  33.8024755  226.58430105  32.32038812 145.03565704\n",
      " 236.56506165  26.15771113 232.97223806  85.58017581 193.94035097\n",
      " 130.57035751 109.25058758  55.12099325 316.57449613  29.28348302\n",
      " 276.925814    83.71644293 173.39840358 175.32368876  37.04148468\n",
      " 290.49455025 104.22724211  16.85545393 167.96774171 160.50810985\n",
      " 113.92861851 250.94100635 135.50191472  68.98401161  96.61572173\n",
      " 117.25465249 267.18605945 146.64181836 118.98489822 133.62380583\n",
      "  38.27435842 145.5014186   45.07158558 225.55076576  24.54591443\n",
      " 105.4053453  215.14827702 214.50429138 212.9567795  340.64345004\n",
      " 217.68649841 129.34865718  61.03516499 167.26946197  91.76856466\n",
      " 274.60972922 251.05400884 244.07322191  44.04972645 259.37299639\n",
      " 210.84495851 141.77988911  39.68169998  32.05463997  19.07604824\n",
      " 220.38163047  32.99489459 128.74275031 123.37152705  96.693388\n",
      " 347.30228749 269.98874577 100.71438128  83.6133702   91.3183542\n",
      "  26.69923072  95.69561728 113.62105573 183.46905708 141.54468216\n",
      "  36.61228176 247.64216166 227.21695493 142.6021211  340.46508664\n",
      "  20.86456825  29.32333891 311.69805237 195.51192142 120.18448823\n",
      "  24.34145521  30.57899187  99.66189821 117.95534502 294.89413987\n",
      " 217.79576039 232.60843793  62.67007595  49.65605151 197.68033324\n",
      "  62.83679424  20.78092349  29.42473609 100.69719985 114.08946746\n",
      "  64.2672547  199.20725346 241.75609343 157.48918879 144.4886965\n",
      "  37.67337047  75.11215229 102.90691806  24.40609226 188.42137489\n",
      "  99.82013156  81.50435972 231.93776515 300.53888357 219.88124471\n",
      "  18.30187504  39.11748593 172.84027915 102.06829871  43.7360159\n",
      " 123.18725629 115.19137347 311.46685829 289.79441818 118.51828395\n",
      "  78.0210633  131.7557065  172.85603065 118.48135538 228.5442638\n",
      "  40.13027061 150.51482122 289.1048295   55.59710555 117.04069822\n",
      " 112.06643579 276.85022121  36.9722352   30.16208393 199.90964662\n",
      "  20.64031712  90.67271178  24.69212077 261.12987233  81.63011845\n",
      " 121.92866926 170.4436197  153.39372291 118.69793662 115.4061771\n",
      " 175.07355166 282.76257779 136.34189761 122.4117206   50.99761424\n",
      " 123.47605976 176.83170046 115.76006861  55.57160608  54.66118603\n",
      "  30.99290694  32.97882731  19.0685745   35.1213819  162.07812395\n",
      " 111.85719661 101.30534545 185.32569098 114.8085443  336.54910271\n",
      " 127.2763313  189.94171997 247.74567886 111.54326624 115.43967244\n",
      " 225.1903974  320.7205854   40.33397413 115.80130445 342.67547448\n",
      " 262.07399971 153.66359227  44.2957684   95.76736794 113.19538516\n",
      "  95.7063902  110.59849462  36.63502693 317.98549369 291.58362514\n",
      " 235.61372021 119.74382327 179.47511999 215.51787254 135.12456965\n",
      " 127.00164381 240.87706697 101.22537105  59.27304479 121.30872243\n",
      " 194.51478489 128.74557771 114.7021495  115.6664687   89.6083009\n",
      " 140.84929628 270.70045017  96.14345087 179.00909734 177.13507461\n",
      " 238.15951955  54.6180291  316.45643049  89.49736156 154.25297561\n",
      " 174.78315605  24.59210561  28.95724691 222.49842362 276.59470292\n",
      " 104.48630534 101.39690241  33.85456329  25.05279014 237.45908361\n",
      " 142.06458628 326.51121999 239.83908312  90.6270709  121.24761669\n",
      " 212.44570422 177.93487283  68.28495981  77.07506254  48.49791484\n",
      " 155.02353124 112.83059015 224.55942342 286.64460083 226.68385368\n",
      "  16.80762391 221.22399025  56.46616979  98.69632767  43.76619512\n",
      " 238.42336371 242.33663159  30.11655399 157.12509648  80.7355249\n",
      " 218.22912987 200.04908482 213.62073781 200.50641089 336.20465496\n",
      "  96.34511241  32.36585417 237.50962098 147.01502525 167.8183394\n",
      "  34.71542465 230.17682738  19.01489668  55.50981741  28.47161718\n",
      "  84.01949071  33.15857785 184.42052482 200.31289592 150.71781572\n",
      "  41.37772872  85.72671704  37.9590325  302.73474885  95.15567102\n",
      "  39.03334664 101.71965702  24.9208137   42.70713672  33.49616846\n",
      " 308.2258158   17.27575596 185.88496782 243.20020303 310.77848312\n",
      " 214.29496701 103.80271118 133.6922969  107.97734319 104.48753667\n",
      " 214.7038877   40.79954823  19.94313266 143.51234096  19.20170653\n",
      " 147.92981333 140.59015597  44.58264858 167.83790356 194.27913789\n",
      " 209.23005165 337.91241557  28.25811721 101.80553641  40.58034762\n",
      "  42.8186768  101.93230001  98.47283222  35.22763653 164.78654026\n",
      "  88.90858335 191.61523297 123.33741713 333.37436952 178.08728563\n",
      " 279.20071803 134.10302363 285.7518429   65.18096145  32.5328924\n",
      "  27.97084304  36.69990355  86.80042162  24.84460402  25.32135553\n",
      " 103.80274708  27.55842904 264.19822419 222.02061346 144.17053304\n",
      "  85.94715582  96.94387588  36.62116955 151.29645308  99.20612601\n",
      " 131.19038379 183.95656076  24.2671466  226.57362353 119.81209957\n",
      "  14.67976607 258.85881321 108.03888893  42.349839    89.25766346\n",
      " 241.83914563 122.67886775 351.47370067 284.78796218 221.24173906\n",
      " 213.19300526 178.99169908  19.88740435 118.00094791 126.21206797\n",
      "  32.10150569  46.36911368  98.56470211 216.3610759  154.2630708\n",
      "  56.53915805 176.00099801  98.60160119 247.00837094 116.84799151\n",
      " 309.22994797  67.72712529  30.45902321  22.96311827 220.54971351\n",
      " 245.51781631  90.15746575 117.24161566  30.29484691  97.09836729\n",
      " 126.91518758 182.64666799 116.7322092  100.016931    43.54416619\n",
      " 118.476079   104.97710534  66.68320748 110.82892417 303.12272648\n",
      " 127.4383412  262.33761537 124.59467792 292.95981023 192.40897946\n",
      "  73.13275918 123.3414327   97.42533296 300.0370817  246.33778034\n",
      " 252.95901746 108.87500231 118.55188391 126.61319135  20.30896804\n",
      " 138.75062206 247.70022937  34.35902571  84.77762482 118.08042655\n",
      " 121.99073537 168.29101023 124.28736905 290.53538708  85.42038066\n",
      " 106.40803684 137.91676602 298.48803988  87.9797456  143.05780604\n",
      " 141.19076255  38.00239393 158.6149956   25.87511708  28.59677775\n",
      "  25.67999308 222.75439251 105.1009081  159.29552302 251.84085996\n",
      " 135.60619968 123.21679931  32.66715803 151.41538586  18.16670796\n",
      " 142.37907381  29.8510891   41.23162182  97.40640468  52.38554408\n",
      " 207.42404486  18.86599257  86.61610435 188.02243544  30.60136688\n",
      " 301.70291056 256.08656143  15.86357803 351.31377744 116.95207729\n",
      " 243.93589065  42.93772656  86.07982347  41.17200034 272.24490584\n",
      "  58.53728704  68.03746158 122.9442215  160.6445176  153.87373564]\n",
      "[102.83206133 220.68828895  96.24538082  30.95071892  51.0149663\n",
      " 123.58992101 164.18373036 319.17041354  34.32673108 119.1599323\n",
      " 298.1980285  251.42204245 102.07291775  37.29180093 213.39736789\n",
      " 151.25706457  26.5987471  202.08769432 356.46778836 160.73536544\n",
      " 226.2899268   81.0585766   15.61843122 139.82010144 143.7364121\n",
      " 114.22775469  27.32234969 118.54882419 115.00580738 150.10121362\n",
      "  78.47210606  30.20639129  59.3652281  127.62565348 264.75149579\n",
      "  30.27806953 132.79336032 179.30342134 224.97626828  70.6828071\n",
      "  56.51574487 122.49337329 166.97149728  83.16738756 329.29476132\n",
      " 202.60705387  80.88949573  59.0177385   18.19741858  88.61081602\n",
      " 238.25183477  95.88253818 156.23256205 114.35119664 200.40815986\n",
      " 148.94828119  43.93497589 181.24420903  15.73491135  17.85067124\n",
      "  99.55306578  88.169919   261.80560462 310.71245913 153.9586449\n",
      " 312.01217725  14.55088312 214.92852922 153.21238405  26.3683943\n",
      " 100.73372453  32.29237792 148.68545796  15.36016619 318.22649716\n",
      " 221.56335191  36.63138236 180.39668947 239.12260489  17.31126684\n",
      " 255.60163136 133.51308141  91.52272222  83.65543553  93.90245897\n",
      " 323.50275704  50.76851996 164.63766775  99.61234613 282.52243331\n",
      " 298.89864492 161.60117809  61.93455866 103.01952455  35.69726272\n",
      "  88.55339701  99.96750854  28.64225885 211.86557083 151.91508093\n",
      "  17.79698785 161.46437951  45.83604875 118.231724    62.59477675\n",
      "  54.33066051 102.10314574  16.50210991 177.63014346 144.63800334\n",
      " 189.58785068 236.7676039  169.76947835 114.80902946  61.638949\n",
      " 122.81217617 248.01650209  35.30074035 213.45518461  17.4207459\n",
      " 101.88877766 107.37651192 205.99172342 107.01493006  43.52479756\n",
      " 119.5723645   55.6807657   16.99935062 218.74593982  54.37623068\n",
      " 128.88177136 156.92719731  19.20645814 136.2549637  311.31461236\n",
      " 134.57547896  43.12637344 140.86162302 262.75356099 362.29758708\n",
      " 181.10085654  27.96147446  33.74824792  91.19829004  99.32107131\n",
      " 117.85100278  85.71100582 122.41585884 149.27406637 196.89187674\n",
      " 128.72499947 290.5825867  333.31823427 217.24926104 174.73434737\n",
      "  14.22377138 118.1216923   14.52536337  92.42840649 136.10621915\n",
      " 131.0973901   16.34933838 202.64599303 211.68848014 124.14326134\n",
      "  14.92645467 248.7208078  155.93674844 294.55988282  16.43026888\n",
      " 122.38131058 122.24221777  90.0203171  124.61025108  99.40595769\n",
      "  94.8175524  242.8060576  196.5729988  157.87005556 111.18757818\n",
      " 217.10264526 173.67483482 158.64777945 247.42869383 137.17350301\n",
      "  90.47090374 100.37672619 301.11254129 165.17232997  43.09493681\n",
      "  35.01520091 136.41765488 121.24515209 105.21541378 128.51545743\n",
      " 261.68008765 342.57323593 135.77339291 115.43299042 102.59853018\n",
      "  16.74241619 251.8105284   15.3849971  341.53794458 140.0814406\n",
      "  32.34132061 126.99694057 257.73126659 221.46355754  60.28200984\n",
      " 284.85873124 157.54542003 129.86071837 216.75791282 250.50693543\n",
      " 127.56910313 241.88894494 119.76724636 137.30650649  94.87201034\n",
      "  96.04260474  26.52426696 106.17552116  63.72333297 147.83093098\n",
      "  34.90690257 200.91735931 205.00613308  35.76263444 177.6128355\n",
      " 270.02762686 120.85093973  17.82881121  77.29373118 209.59206638\n",
      " 100.23044565 363.16884057 326.37646934 159.1958741  215.85483297\n",
      "  18.74739996 212.72230483  26.01763444 141.1536728  144.78002898\n",
      "  98.36106952 152.98129603 229.94995833 105.36921597  15.37048737\n",
      "  91.88472342 253.13297159 225.49846735 169.89801395 139.96929199\n",
      " 182.46980664  27.49348901 351.70746158  86.49217881  18.56790994\n",
      " 166.40581494  27.34302773 228.03868603  26.64969962 157.00373485\n",
      " 225.14039779  16.84659751 229.67470862  87.76331229 197.27527985\n",
      " 129.85250851 105.67201912  50.18395728 325.62776481  18.04908433\n",
      " 281.25231542  85.34748864 179.73553797 187.53415705  32.53812636\n",
      " 292.17824055 103.53912963  15.7984317  171.65070948 162.78077152\n",
      " 115.9289936  246.38924435 137.69611077  69.09180339 101.58961437\n",
      " 116.67098056 265.21843795 145.67647655 117.65599066 133.13576443\n",
      "  29.78958051 143.89139153  40.798296   227.4684911   24.78445695\n",
      " 102.7198645  228.51263181 213.5308068  215.53091532 350.04275734\n",
      " 211.30751662 132.07114333  57.23106536 163.88377717  95.94813406\n",
      " 274.64453283 248.64543374 243.61217846  36.9259046  249.57685137\n",
      " 194.56615019 152.02326407  35.2269449   28.31812842  18.48422872\n",
      " 222.02669901  31.37373927 137.23102414 112.55927277  92.33786032\n",
      " 359.00480666 279.81657215  98.67370228  86.53313805  95.06199995\n",
      "  27.21645346  94.42873912 103.68369912 185.42033105 142.46458405\n",
      "  36.93676872 237.05550096 242.51080729 138.04537127 359.88701046\n",
      "  19.10109764  19.64572391 316.38034405 188.15001638 118.23100385\n",
      "  15.41800625  29.32194628  98.42752034 117.68933785 300.27566558\n",
      " 222.41147306 232.26042327  62.52581554  45.26174437 198.65879954\n",
      "  59.40878588  18.6463207   18.26449054 101.90353728 108.66004702\n",
      "  57.5573629  197.97694067 242.34170014 164.53911231 147.65355322\n",
      "  29.0022484   72.28738329  99.59515298  15.21811748 188.7891722\n",
      "  96.71560293  76.72043482 229.9140113  309.84811267 218.98061149\n",
      "  17.51537807  33.4738131  172.15231097 104.95637975  36.84481475\n",
      " 132.57838119 113.58698292 311.26924056 293.79945291 116.00851892\n",
      "  75.31732822 128.89280738 172.29469288 120.58784882 225.48489247\n",
      "  34.75305986 165.82752177 305.76318675  48.95231113 117.07639273\n",
      " 109.82399579 276.07103439  29.31485445  30.69560762 214.90399423\n",
      "  20.8361271   91.76398894  15.49676251 245.29347455  74.53311159\n",
      " 116.72052658 195.15877477 160.75944661 114.85591162 115.75645185\n",
      " 171.69983141 283.57446445 135.45993561 122.53362994  43.26795629\n",
      " 124.79427465 174.7295855  115.81503644  48.99799884  50.54900586\n",
      "  18.39356927  27.79201126  17.49185548  35.41022146 165.50929723\n",
      " 115.85826129  99.03200387 182.95439033 117.2651586  343.41719601\n",
      " 129.27428018 180.09757347 265.01118494 113.47512953 116.74529434\n",
      " 227.14058911 329.35914526  31.53922205 114.5320077  345.52662879\n",
      " 249.30736777 148.2636722   39.11235654  95.60747236 116.03172765\n",
      "  86.91514399 109.84449086  29.90041916 323.91458273 297.53750283\n",
      " 237.31649062 120.40490141 164.12408718 222.77672567 133.98506018\n",
      " 120.13292879 238.68984007 101.51910662  54.2713908  119.07306397\n",
      " 207.01608381 127.90964113 118.14566594 114.34275349  92.75166412\n",
      " 137.83305359 282.85274808  97.41126284 179.12961312 175.25188374\n",
      " 230.24109764  49.37470225 322.90085209  90.81327214 156.27970243\n",
      " 175.83708032  15.6943154   27.13611563 218.08107998 283.19294812\n",
      " 105.53333478 100.43130495  28.42145285  15.19810546 228.67030641\n",
      " 143.79689099 335.81292647 246.77217292  92.34801231 126.89545332\n",
      " 212.78584855 165.76671356  63.13071379  71.16693915  41.76137536\n",
      " 165.86289033 110.93933646 227.19909527 294.78787302 218.3724199\n",
      "  15.63881914 216.70240079  49.65153303  94.04292404  33.099913\n",
      " 231.05450208 231.34519011  30.19708454 150.47612747  74.58346411\n",
      " 231.96159019 219.6409942  212.15491952 207.32893556 353.86306868\n",
      "  99.34114738  31.48201987 231.21389186 141.75887848 191.427826\n",
      "  28.28811857 232.36684223  17.91379308  47.55367366  17.21658805\n",
      "  90.31355951  31.18181361 199.05142136 191.03406843 151.81289461\n",
      "  32.78695475  84.26438209  30.85258652 312.00512013  94.74207741\n",
      "  31.3895466   99.79504391  15.4440772   42.23650421  31.89658968\n",
      " 313.98119396  15.74190164 181.44322138 241.05923834 316.65091174\n",
      " 221.48221799 100.9143183  134.58861248 105.12966005  96.13891024\n",
      " 208.55508002  33.25042157  18.68575566 145.58535137  18.38948596\n",
      " 155.33139334 139.90762722  36.34241174 168.17313841 193.98324865\n",
      " 205.40445718 339.75957149  29.04847463  99.21175026  37.02179074\n",
      "  35.73311213 101.28415248  95.82354913  29.64335426 163.0546633\n",
      "  92.19471423 184.9430037  123.41076757 342.85653608 172.53835112\n",
      " 280.98112135 128.25949893 287.03881457  63.95370232  31.36360781\n",
      "  27.76972323  35.22921236  87.66658826  14.59330061  15.60956178\n",
      " 100.34365584  17.34836893 275.11605892 230.53271368 149.63882018\n",
      "  88.65705916  99.24170837  30.74788985 149.29216077  96.84214028\n",
      " 133.34527532 182.97493801  15.00527165 233.55923165 124.47293922\n",
      "  14.61868229 252.89731228 104.54857816  39.41975178  81.51829384\n",
      " 238.58018119 128.94155219 370.21346062 282.7894992  220.45444853\n",
      " 211.75346716 173.4044166   18.81313214 114.43127537 117.58167407\n",
      "  18.10345056  48.07210889 102.91376563 215.49186962 162.75771044\n",
      "  51.53343999 175.79358766  98.403627   246.46944651 115.31068378\n",
      " 315.29771238  58.51195705  28.43396191  17.83970406 217.60529702\n",
      " 247.58584392  89.21211021 111.11962152  27.36899892  96.2463847\n",
      " 117.41437348 183.3650782  115.25981477 100.99518314  35.5553955\n",
      " 117.82435375 102.32549254  65.22875373 110.98352525 308.09732253\n",
      " 130.09660014 265.25943915 113.7693535  305.18615368 198.17201623\n",
      "  65.67386747 118.24206219  96.54547487 308.83923559 240.95857248\n",
      " 260.11679389 106.03924759 117.57029942 127.28475916  17.54525578\n",
      " 134.4083272  265.85787554  27.16441952  80.83942196 113.61385185\n",
      " 117.12797441 165.97661335 127.62274733 304.96406897  87.32110738\n",
      " 103.32012664 132.83936208 301.06320094  93.46337577 145.19178531\n",
      " 142.59259066  29.24354672 159.5422603   16.12755904  28.31879364\n",
      "  16.61255831 226.97030394 105.07714466 153.62916763 257.00167931\n",
      " 139.561175   120.33003042  27.44301684 157.5465216   16.4087156\n",
      " 141.61347095  29.98869024  34.00155396  95.99955384  47.94241623\n",
      " 206.5782363   19.37968441  82.24322899 176.75162772  29.76022145\n",
      " 312.12072104 253.72390065  14.97583646 362.19357031 121.80773413\n",
      " 242.83924696  39.59529422  86.48270159  31.30413865 274.51675759\n",
      "  53.47864483  66.20731871 123.83165702 157.87188077 160.63294873]\n",
      "[104.402 216.084  94.718  40.826  50.806 119.726 170.83  303.484  45.152\n",
      " 129.716 297.33  252.694  98.18   42.798 200.246 160.022  26.692 179.052\n",
      " 356.182 155.3   230.434  83.86   29.068 141.428 140.374 115.19   27.464\n",
      " 116.568 111.292 159.262  80.282  37.802  64.748 131.906 267.67   41.274\n",
      " 130.478 164.856 250.282  89.316  57.716 123.08  148.11   92.374 331.924\n",
      " 206.668  93.784  60.67   21.608  86.76  229.566  91.058 170.016 124.272\n",
      " 190.818 125.566  53.612 177.268  24.588  19.664  91.498  87.618 261.592\n",
      " 311.898 155.208 314.354  27.476 230.926 135.384  33.546 102.232  32.884\n",
      " 130.264  15.886 316.904 225.062  36.162 183.244 230.616  29.516 250.274\n",
      " 138.348  82.174  83.618  95.656 316.886  51.194 168.528 107.22  270.168\n",
      " 286.216 156.656  67.028 100.6    44.458  83.3   105.914  28.858 220.98\n",
      " 151.498  18.458 143.488  37.708 123.446  78.354  50.254  99.55   27.782\n",
      " 176.028 131.314 183.162 227.9   170.224 117.756  62.588 125.066 230.106\n",
      "  40.362 203.684  18.854 101.618 111.406 183.104 112.868  52.478 115.648\n",
      "  54.99   16.086 231.792  59.616 129.812 164.69   22.084 145.962 297.228\n",
      " 133.942  45.44  146.652 247.33  365.252 201.906  36.696  35.834 104.232\n",
      "  88.51  114.132  88.22  119.608 150.974 226.452 130.584 289.49  319.05\n",
      " 231.77  153.246  11.83  113.936  22.04   93.99  141.964 143.47   30.608\n",
      " 200.908 224.634 123.418  23.854 226.016 150.81  287.808  25.586 123.406\n",
      " 127.574  89.822 128.386 101.534 103.388 223.582 200.864 168.148 116.106\n",
      " 209.928 165.526 155.514 252.552 146.61  101.022 109.88  289.974 168.502\n",
      "  50.758  42.746 143.12  124.092 114.038 128.736 263.636 334.394 143.714\n",
      " 120.588 103.436  26.602 219.962  25.106 337.556 144.206  42.762 130.29\n",
      " 240.678 220.206  63.512 286.912 173.95  135.59  216.284 249.042 128.23\n",
      " 238.624 119.36  134.264  94.656 102.976  32.734 108.646  68.18  148.056\n",
      "  40.906 208.204 181.77   37.876 199.296 263.94  111.956  28.226  81.174\n",
      " 199.232 101.166 366.224 324.814 156.19  196.894  20.756 215.344  31.91\n",
      " 133.83  146.94  100.24  130.558 230.002 103.814  23.966  82.232 261.584\n",
      " 239.85  193.686 144.634 197.144  39.328 339.776  76.582  19.374 178.076\n",
      "  36.948 233.902  31.842 171.41  234.71   26.398 235.498  88.046 185.65\n",
      " 135.104 107.892  54.476 326.178  32.724 275.902  87.168 158.284 208.424\n",
      "  38.404 287.408 104.416  16.138 164.432 190.958 116.528 247.876 142.534\n",
      "  68.378 105.166 119.034 260.908 173.462 116.872 135.902  41.554 143.754\n",
      "  48.118 221.524  20.838 102.044 199.748 206.408 225.094 347.102 204.374\n",
      " 132.17   61.334 163.86   88.956 273.448 256.798 235.38   50.478 258.188\n",
      " 202.372 123.912  40.832  32.188  19.06  217.074  30.064 132.198 118.382\n",
      "  88.298 365.234 268.5    99.81   88.862 103.284  26.508  99.554 113.192\n",
      " 179.48  147.134  41.556 241.3   226.256 150.324 357.458  21.226  30.534\n",
      " 316.312 180.434 122.414  23.824  30.296 100.498 122.182 293.5   212.18\n",
      " 240.194  70.83   48.802 199.322  66.356  20.096  30.612 102.946 113.162\n",
      "  70.608 214.89  238.686 190.326 154.838  41.114  74.298 100.822  24.644\n",
      " 199.984  98.862  75.11  234.338 304.42  224.212  18.41   41.992 185.446\n",
      " 110.618  50.302 143.958 116.312 310.65  287.686 117.032  87.044 134.606\n",
      " 164.208 121.716 230.848  40.93  140.674 289.678  55.626 119.122 112.572\n",
      " 272.466  39.182  27.446 218.854  20.348  87.004  24.812 247.56   88.75\n",
      " 119.082 158.95  184.592 117.334 116.832 169.196 275.2   137.49  121.57\n",
      "  53.602 128.46  180.338 119.944  57.556  58.312  32.19   35.01   17.902\n",
      "  38.424 165.236 113.69  101.242 193.194 112.15  338.882 131.22  189.648\n",
      " 245.668 116.828 115.398 218.75  329.712  42.82  116.906 345.422 249.75\n",
      " 151.362  47.838  97.262 119.296 107.114 108.758  37.99  327.61  293.722\n",
      " 232.718 117.284 170.33  215.838 131.162 128.586 236.304 105.186  61.002\n",
      " 122.294 188.868 125.846 118.134 117.224  89.958 139.992 272.538 100.126\n",
      " 169.028 196.134 237.102  54.356 327.504  94.942 158.404 169.492  23.136\n",
      "  28.516 220.874 277.652 106.57  110.832  35.216  26.808 249.54  139.4\n",
      " 328.012 236.696  84.076 117.806 232.344 169.994  75.59   73.248  53.616\n",
      " 182.538 117.644 226.648 296.888 224.344  15.884 221.126  56.132  90.636\n",
      "  48.674 249.166 238.814  28.392 155.318  71.76  211.518 198.956 210.658\n",
      " 192.808 353.382 102.02   34.47  242.366 149.378 157.018  33.388 232.874\n",
      "  18.34   55.514  29.214  91.902  33.908 182.892 193.878 146.604  46.286\n",
      "  93.03   39.43  301.46   95.794  41.242  99.816  24.81   44.046  33.682\n",
      " 303.024  17.902 192.714 239.958 310.54  218.224 105.336 126.47  107.446\n",
      " 105.736 208.962  41.256  19.91  140.914  17.202 179.554 140.834  50.752\n",
      " 144.964 207.278 216.656 333.72   27.254 100.148  40.04   46.602 100.542\n",
      " 106.128  34.656 184.424  87.53  199.632 125.518 341.434 174.848 271.612\n",
      " 131.772 283.68   66.498  32.166  27.794  37.784  90.48   24.592  22.834\n",
      " 105.416  27.868 260.92  226.546 148.512  87.004 102.06   37.518 149.788\n",
      "  98.336 128.822 190.916  24.018 232.492 125.742  14.682 274.194 106.368\n",
      "  40.076  76.464 248.358 126.324 380.754 281.922 224.288 221.246 192.428\n",
      "  20.806 115.134 128.184  32.664  45.63   93.924 203.768 193.282  55.69\n",
      " 181.132  98.042 252.882 118.704 320.484  73.368  33.126  25.156 226.956\n",
      " 243.972  90.224 108.84   30.534  94.616 120.376 169.98  116.908 104.04\n",
      "  47.816 119.192 102.426  74.182 108.27  313.898 129.378 262.952 118.816\n",
      " 294.616 189.8    82.998 116.404  95.938 307.692 251.586 254.624 107.31\n",
      " 117.4   127.178  19.17  125.822 244.104  35.57   84.228 115.152 114.58\n",
      " 187.698 127.086 308.978  89.758 101.844 131.236 302.728  94.696 158.496\n",
      " 137.962  41.664 195.208  27.774  30.622  27.306 224.826 106.728 146.39\n",
      " 249.772 140.23  113.662  31.75  192.244  18.88  138.598  30.786  47.008\n",
      "  98.404  53.832 196.8    20.224  95.438 192.734  30.882 305.548 273.706\n",
      "  15.54  372.356 117.878 261.964  39.302  85.4    40.592 274.31   60.3\n",
      "  76.476 119.098 160.104 202.548]\n",
      "[110.514 212.978  91.74   42.524  44.818 120.442 165.604 308.846  42.516\n",
      " 125.218 294.73  254.268  90.996  43.498 210.918 149.502  26.046 180.418\n",
      " 341.332 152.346 217.208  86.356  27.896 140.208 147.406 116.576  28.966\n",
      " 115.924 113.402 152.52   76.844  37.036  63.38  132.434 266.986  42.04\n",
      " 130.014 167.686 240.638  91.662  55.602 120.732 153.9    93.268 329.224\n",
      " 206.254  95.688  62.428  19.042  84.898 228.77   92.718 176.628 123.878\n",
      " 193.234 130.132  53.79  174.96   25.484  20.242  92.146  90.014 254.716\n",
      " 294.994 152.42  312.764  28.602 238.794 127.376  33.966 102.49   32.29\n",
      " 130.28   15.868 312.362 233.186  34.622 175.61  234.57   29.422 251.34\n",
      " 136.266  82.948  85.05   95.15  325.26   50.97  169.33  108.598 278.358\n",
      " 274.864 160.91   69.45  102.85   42.386  82.21  104.334  27.942 219.74\n",
      " 144.648  19.692 151.804  41.8   120.356  80.674  50.354 101.544  27.198\n",
      " 172.864 127.07  175.968 240.046 160.542 117.01   61.216 119.504 235.234\n",
      "  38.916 200.02   19.1   101.576 105.842 186.792 112.452  54.274 113.968\n",
      "  57.014  15.498 238.88   61.692 129.832 156.722  18.646 142.444 298.266\n",
      " 136.654  46.238 143.34  242.852 349.21  206.158  35.054  35.734 104.256\n",
      "  90.598 114.672  88.594 121.608 153.206 229.744 128.878 288.602 309.354\n",
      " 245.352 149.756  11.946 113.408  22.814  93.176 141.748 141.508  29.968\n",
      " 208.134 241.636 123.372  24.358 226.53  152.68  282.498  27.816 124.262\n",
      " 124.576  92.044 128.462 100.862 101.048 225.8   202.896 152.8   105.982\n",
      " 208.86  170.396 159.266 256.804 151.482  99.29  112.322 301.822 164.85\n",
      "  51.52   41.18  138.484 120.848 104.244 128.834 275.132 316.884 137.21\n",
      " 118.502 103.95   26.276 216.952  25.242 331.952 142.736  46.396 131.996\n",
      " 234.122 214.618  64.034 280.852 185.616 134.39  212.614 253.344 126.104\n",
      " 236.356 119.566 135.422  93.494 103.028  33.2   110.648  72.932 143.478\n",
      "  40.278 202.948 183.678  34.714 196.26  251.976 109.132  31.17   70.276\n",
      " 197.38  101.164 368.08  308.936 159.74  196.422  19.852 214.954  33.906\n",
      " 132.546 139.232 100.17  126.292 234.746 104.42   24.982  85.528 254.16\n",
      " 234.894 192.968 145.404 197.56   38.09  339.034  71.612  19.72  163.606\n",
      "  36.944 228.234  33.45  183.92  246.112  25.384 237.934  85.338 187.372\n",
      " 130.164 107.01   55.258 318.182  34.864 264.992  86.346 157.412 211.634\n",
      "  37.456 285.258 106.514  15.992 164.538 192.066 116.884 250.97  131.114\n",
      "  71.21  101.61  117.35  259.634 177.776 111.624 132.864  42.272 143.176\n",
      "  50.05  218.09   19.472 100.87  195.732 219.254 234.662 358.078 208.684\n",
      " 132.796  63.776 159.364  92.064 263.012 253.548 253.166  50.914 258.714\n",
      " 187.014 133.706  38.706  30.25   19.232 219.086  31.174 126.71  109.85\n",
      "  93.856 351.164 261.054 100.97   76.014 100.62   27.194 103.766 114.47\n",
      " 176.406 148.632  38.472 240.184 219.636 151.286 341.604  21.114  32.402\n",
      " 312.098 185.372 122.934  25.268  31.608  99.652 119.11  284.556 217.53\n",
      " 236.204  58.734  50.124 203.338  70.458  21.66   33.862 100.924 114.078\n",
      "  66.006 210.826 246.178 184.82  163.558  44.012  76.308 100.87   24.286\n",
      " 195.654  91.226  67.324 229.904 287.77  218.944  18.754  40.97  175.412\n",
      " 112.256  47.49  136.442 114.25  303.52  286.536 114.348  81.918 130.95\n",
      " 165.768 121.096 245.938  39.042 133.154 273.898  55.092 118.782 111.332\n",
      " 265.026  38.912  28.956 223.812  20.084  91.592  24.526 257.966  93.87\n",
      " 118.014 165.486 185.518 115.8   114.842 162.542 287.98  135.248 121.596\n",
      "  54.458 129.74  167.958 118.342  60.768  61.928  36.522  34.368  18.634\n",
      "  37.32  166.194 114.212 102.044 181.106 110.082 327.212 128.348 174.772\n",
      " 243.782 109.026 114.416 215.35  318.312  47.62  116.256 348.12  256.406\n",
      " 151.608  47.592  97.484 110.91  113.814 111.176  34.95  320.192 283.494\n",
      " 223.698 112.156 157.878 224.802 138.878 125.32  243.528 103.586  62.484\n",
      " 120.12  189.074 125.642 115.194 116.908  91.132 144.576 260.796  99.362\n",
      " 175.372 186.422 234.88   56.428 323.584  87.258 153.684 164.45   24.47\n",
      "  27.854 231.574 276.596 107.534 111.604  34.162  27.802 269.224 140.824\n",
      " 330.818 239.542  84.872 110.102 220.41  169.276  77.678  74.646  55.654\n",
      " 187.014 111.374 230.94  293.95  225.518  15.658 224.694  55.514  82.184\n",
      "  52.712 240.964 237.24   30.656 152.394  69.938 214.078 200.354 208.058\n",
      " 202.39  343.722 101.152  34.248 254.864 145.13  162.052  35.722 231.312\n",
      "  19.174  53.204  31.956  90.83   33.672 186.91  196.776 154.736  43.262\n",
      "  93.848  38.216 288.966  95.33   43.29  100.996  25.666  43.396  30.322\n",
      " 292.748  20.728 191.64  255.268 300.972 216.208 104.76  121.366 109.69\n",
      " 109.328 205.496  42.582  18.95  139.656  17.2   180.898 138.254  51.136\n",
      " 145.822 198.628 217.012 333.5    28.    101.39   36.688  46.328 101.308\n",
      " 110.47   34.708 179.952  88.092 198.126 123.32  334.654 177.136 286.406\n",
      " 130.87  273.158  60.584  34.942  28.112  38.6    86.324  25.252  23.496\n",
      "  98.968  30.    258.43  221.802 150.85   84.196 101.136  41.762 141.342\n",
      "  96.148 126.654 192.428  25.374 235.746 122.942  13.904 272.72  107.898\n",
      "  37.602  81.644 247.336 120.176 364.834 281.974 218.082 215.204 198.024\n",
      "  19.552 114.508 128.266  35.268  46.596  92.272 211.86  191.88   56.656\n",
      " 176.308 100.038 255.876 116.636 310.916  74.844  33.854  24.202 232.65\n",
      " 237.684  91.104 107.756  34.132  93.974 122.62  172.864 106.61  104.648\n",
      "  45.844 118.802 102.98   69.158 107.43  327.136 133.696 250.626 118.446\n",
      " 277.966 195.558  86.5   116.884  96.844 298.24  249.916 243.482 106.236\n",
      " 116.354 126.068  20.066 122.74  243.99   38.208  84.178 113.576 117.88\n",
      " 186.386 127.382 313.594  86.002  99.828 131.334 302.348  91.214 154.942\n",
      " 134.78   41.45  193.41   27.128  29.086  27.206 225.33  104.176 144.278\n",
      " 257.902 136.614 115.964  33.87  191.87   19.896 134.888  29.802  43.214\n",
      "  96.632  53.98  199.372  19.034  93.25  183.754  32.224 291.996 278.642\n",
      "  15.858 363.704 116.916 270.918  38.44   86.402  43.656 271.766  60.952\n",
      "  76.64  120.056 157.63  206.806]\n"
     ]
    }
   ],
   "source": [
    "# Predict with each trained model\n",
    "prediction1 = greedy_CV1.predict(test1)\n",
    "prediction2 = greedy_CV2.predict(test2)\n",
    "prediction3 = greedy_CV3.predict(test3)\n",
    "prediction4 = greedy_CV4.predict(test4)\n",
    "\n",
    "print(prediction1)\n",
    "print(prediction2)\n",
    "print(prediction3)\n",
    "print(prediction4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f64b7f9d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id   count\n",
      "0   0  102.87\n",
      "1   1  220.94\n",
      "2   2   94.75\n",
      "3   4   40.17\n",
      "4   5   46.90 \n",
      "\n",
      "    id   count\n",
      "0   0  102.83\n",
      "1   1  220.69\n",
      "2   2   96.25\n",
      "3   4   30.95\n",
      "4   5   51.01 \n",
      "\n",
      "    id   count\n",
      "0   0  104.40\n",
      "1   1  216.08\n",
      "2   2   94.72\n",
      "3   4   40.83\n",
      "4   5   50.81 \n",
      "\n",
      "    id   count\n",
      "0   0  110.51\n",
      "1   1  212.98\n",
      "2   2   91.74\n",
      "3   4   42.52\n",
      "4   5   44.82\n"
     ]
    }
   ],
   "source": [
    "# Save the prediction results\n",
    "GridSearchCV1 = pd.read_csv('data/submission.csv')\n",
    "GridSearchCV2 = pd.read_csv('data/submission.csv')\n",
    "GridSearchCV3 = pd.read_csv('data/submission.csv')\n",
    "GridSearchCV4 = pd.read_csv('data/submission.csv')\n",
    "\n",
    "import numpy as np\n",
    "GridSearchCV1['count'] = np.round(prediction1, 2)\n",
    "GridSearchCV2['count'] = np.round(prediction2, 2)\n",
    "GridSearchCV3['count'] = np.round(prediction3, 2)\n",
    "GridSearchCV4['count'] = np.round(prediction4, 2)\n",
    "\n",
    "print(GridSearchCV1.head(), '\\n\\n',\n",
    "      GridSearchCV2.head(), '\\n\\n',\n",
    "      GridSearchCV3.head(), '\\n\\n',\n",
    "      GridSearchCV4.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "392713b2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save the results\n",
    "GridSearchCV1.to_csv('GridSearchCV1_result.csv')\n",
    "GridSearchCV2.to_csv('GridSearchCV2_result.csv')\n",
    "GridSearchCV3.to_csv('GridSearchCV3_result.csv')\n",
    "GridSearchCV4.to_csv('GridSearchCV4_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1b33f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}