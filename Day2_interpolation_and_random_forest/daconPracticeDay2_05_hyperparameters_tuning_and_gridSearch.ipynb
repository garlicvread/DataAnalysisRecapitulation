{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d912416",
   "metadata": {},
   "source": [
    "### The concept of hyperparameter\n",
    "Hyperparameter tuning is one of the most important parts of a machine learning pipeline. A wrong choice of the hyperparameters‚Äô values may lead to wrong results and a model with poor performance.<br>\n",
    "\n",
    "Hyperparameters are model parameters whose values are set **before** training.<br> These hyperparameters might address model design questions such as:\n",
    "\n",
    "- What **degree of polynomial features** should I use for my linear model?\n",
    "- What should be the **maximum depth** allowed for my decision tree?\n",
    "- What should be the **minimum number of samples** required at a leaf node in my decision tree?\n",
    "- **How many trees** should I include in my random forest?\n",
    "- **How many neurons** should I have in my neural network layer?\n",
    "- **How many layers** should I have in my neural network?\n",
    "- What should I set my **learning rate** to for gradient descent?\n",
    "\n",
    "Let's make it simple. For example, **the number of neurons** of a feed-forward neural network is a hyperparameter, because we set it before training. Another example of hyperparameter is **the number of trees** in a random forest or the penalty intensity of a Lasso regression. As you can see, the hyperparameters are all numbers that are set before the training phase and their values affect the behavior of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb8eaee",
   "metadata": {},
   "source": [
    "### IMPORTANT!\n",
    "Hyperparameters are **not** model parameters and they cannot be directly trained from the data. Model parameters are **learned** during training when we optimize a loss function using something like gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76db0ac3",
   "metadata": {},
   "source": [
    "\n",
    "### The reason for tuning the hyperparameters\n",
    "Why should we tune the hyperparameters of a model?<br>\n",
    "\n",
    "That is because we don‚Äôt really know the models' optimal values in advance. A model with different hyperparameters is, actually, a different model so it may have a lower performance.<br>\n",
    "\n",
    "In the case of neural networks, a low number of neurons could lead to underfitting and a high number could lead to overfitting.<br>\n",
    "\n",
    "In both cases, the model is not good, so we need to find the intermediate number of neurons that leads to the best performance.<br>\n",
    "\n",
    "If the model has several hyperparameters, we need to find the best combination of values of the hyperparameters searching in a multi-dimensional space. That‚Äôs why hyperparameter tuning, which is the process of finding the right values of the hyperparameters, is a very complex and time-expensive task."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyperparameter tuning in practice\n",
    "Tuning hyperparameters means making decisions on the **stopping criteria**. There are several stopping criteria, but we're going to deal with four first, such as:\n",
    "1. The max_depth\n",
    "2. The minimum size of the node: min_samples_split\n",
    "3. The minimum lift: min_impurity_decrease\n",
    "4. The cost-complexity<br>\n",
    "---\n",
    "The **max depth** means the maximum number of depth in the decision tree. The tree structure cannot be deeper than this value we set using **`max_depth`**. The smaller it is, the smaller the tree will be.<br>\n",
    "\n",
    "The **minimum size of the node** is the number of data(samples) to split. The smaller the value, the larger the tree will be, and its default value is 2.<br>\n",
    "\n",
    "We can set this using **`min_samples_split`** A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The equation for min_sample_split is:<br>\n",
    "\n",
    "$$\\frac{N_t}{N} \\times (impurity - \\frac{N_{tR}}{N_t} \\times right\\;impurity - \\frac{N_{tl}}{N_t} \\times left\\;impurity)$$\n",
    "\n",
    "Where<br>\n",
    "$N$ is the total number of samples<br>\n",
    "$N_t$ is the total number of samples in current node<br>\n",
    "$N_{tL}$ is the number of samples in the left child<br>\n",
    "$N_{tR}$ is the number of samples in the right child<br>\n",
    "$N$, $N_t$, $N_{tL}$, $N_{tR}$ are all refer to the weighted sum, if `sample_weight` is passed.<br>\n",
    "\n",
    "The **minimum lift** is a criterion to see if the association rules between the items are coincidental or not. We can set the minimum lift using **`min_impurity_decrease`**.<br>\n",
    "\n",
    "When the lift is the same or smaller than the value set, the tree will not split more. The smaller the value, the larger the tree will be.<br>\n",
    "\n",
    "For pruning, we can think of two types of it. The first is **pre-pruning**, and the other is **post-pruning**. Pre-pruning is also called **early stopping**. It means literally stopping the training early. And we can do it by setting the max depth or the number of branches. Post-pruning is the process of performing pruning after we train the model. We can do post-pruning using the cost-complexity pruning technique.<br>\n",
    "\n",
    "The **cost complexity** is a concept that is used in **cost complexity pruning**. Pruning is a technique to prevent overfitting by limiting the model by setting penalty coefficients for the impurity and for the decision tree being larger.<br>\n",
    "\n",
    "In practice, we can do cost complexity pruning by finding the **$\\alpha$** value with the least influence and prune the node with that value. The equation for cost complexity pruning is:\n",
    "\n",
    "$$R_\\alpha (T) = R(T) + \\alpha |T|$$\n",
    "\n",
    "where<br>\n",
    "$R(T)$ is the learning errors of the leaf nodes<br>\n",
    "$|T|$ is the number of leaf nodes<br>\n",
    "$\\alpha$ is the complexity parameter\n",
    "\n",
    "When we focus on reducing the ¬†ùëÖ(ùëá) ¬†value only, the size of the tree gets bigger. It means the tree structure has more branches. ¬†ùõº decides the number of leaf nodes to be remained, thus we need to modify it to prevent overfitting. The bigger the ¬†ùõº ¬†value, the more nodes being pruned will be.<br>\n",
    "\n",
    "Note that we need to calculate the $R_\\alpha (T_t)$ for the sub-trees. The equation is very similar to above one.\n",
    "\n",
    "$$R_\\alpha (T_t) = R(T_t) + \\alpha |T_t|$$\n",
    "\n",
    "---\n",
    "Using the stopping criteria such as above, we can set the optimal conditions for model training, and this process is called hyperparameter tuning."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GridSearch\n",
    "\n",
    "Amongst the hyperparameter tuning techniques, GridSearch, a sort of exhaustive search, shows the best performance. GridSearch is a technique that finds the best combination among the possible combinations. However, GridSearch also has cons because the training consumes a lot of time.<br>\n",
    "\n",
    "For now, we wikk implement an exhaustive search using GreadSearCV module."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Downloading data\n",
    "!wget 'https://bit.ly/3gLj0Q6'\n",
    "\n",
    "# Unzip the downloaded data\n",
    "import zipfile\n",
    "with zipfile.ZipFile('3gLj0Q6', 'r') as existing_zip:\n",
    "    existing_zip.extractall('data')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import pandas and RandomForestRegressor\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check if the data loading is successful\n",
    "print('============ Train Data ============\\n')\n",
    "print('Train Data Information\\n', train.info(), '\\n')\n",
    "print('Train Data Shape: ', train.shape, '\\n')\n",
    "\n",
    "print('============ Test Data ============')\n",
    "print('Test Data Information\\n', test.info(), '\\n')\n",
    "print('Test Data Shape: ', test.shape, '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check if there are missing values\n",
    "print(train.isnull().sum(), '\\n')\n",
    "print(test.isnull().sum())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove the missing values using linear interpolation\n",
    "train.interpolate(inplace=True)\n",
    "test.interpolate(inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check if the null values are replaced well.\n",
    "print(train.isnull().sum(), '\\n')\n",
    "print(test.isnull().sum())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Declare the model\n",
    "X_train = train.drop(['count'], axis=1)\n",
    "Y_train = train['count']\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestRegressor(criterion = 'squared_error')\n",
    "model.fit(X_train, Y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print the feature importances\n",
    "model.feature_importances_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create train datasets by removing the less important features\n",
    "X_train1 = train.drop(['count', 'id'], axis=1)\n",
    "X_train2 = train.drop(['count', 'id', 'hour_bef_windspeed'], axis=1)\n",
    "X_train3 = train.drop(['count', 'id', 'hour_bef_windspeed', 'hour_bef_pm2.5'], axis=1)\n",
    "\n",
    "Y_train = train['count']\n",
    "\n",
    "# Create test datasets\n",
    "test1 = test.drop(['id'], axis=1)\n",
    "test2 = test.drop(['id', 'hour_bef_windspeed'], axis=1)\n",
    "test3 = test.drop(['id', 'hour_bef_windspeed', 'hour_bef_pm2.5'], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check the shape of training and test data\n",
    "print('X_train1.shape: ', X_train1.shape, '\\n')\n",
    "print('X_train2.shape: ', X_train2.shape, '\\n')\n",
    "print('X_train3.shape: ', X_train3.shape, '\\n')\n",
    "print('Y_train.shape: ', Y_train.shape, '\\n')\n",
    "print('test1.shape', test1.shape, '\\n')\n",
    "print('test2.shape', test2.shape, '\\n')\n",
    "print('test3.shape', test3.shape, '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Declare separate models\n",
    "model1 = RandomForestRegressor(criterion = 'squared_error')\n",
    "model2 = RandomForestRegressor(criterion = 'squared_error')\n",
    "model3 = RandomForestRegressor(criterion = 'squared_error')\n",
    "\n",
    "# Train the saparated models\n",
    "model1.fit(X_train1, Y_train)\n",
    "model2.fit(X_train2, Y_train)\n",
    "model3.fit(X_train3, Y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### RandomForest Hyperparameters\n",
    "\n",
    "**n_estimators:** Number of decision making tree\n",
    "- Default = 10\n",
    "- When increase it, the performance may get better, but may cause too much train time.<br>\n",
    "\n",
    "**min_samples_split**: The minimum number of sample used to split node\n",
    "- Used to control overfitting\n",
    "- Default = 2: The smaller the value, the greater possibility of overfitting because of the increasing node split<br>\n",
    "\n",
    "**min_samples_leaf**: The minimum number of samples to be leaf node\n",
    "- Along to min_samples_split, it is used to control the overfitting\n",
    "- When the data is imbalanced, some data of a specific class may extremely small, thus it needs to be kept the small value<br>\n",
    "\n",
    "**max_features**: Maximum number of features for optimal split\n",
    "- Default = 'auto'\n",
    "    - Note: The default value of max_feature is none in decision tree\n",
    "- When specified in int type: The number of features\n",
    "- When specified in float type: The ratio of features\n",
    "- 'sqrt' or 'auto': Samples as many as $\\sqrt{The\\;number\\;of\\;whole\\;features}$\n",
    "- log : Samples as many as $\\log_2{(The\\;number\\;of\\;whole\\;features)}$<br>\n",
    "\n",
    "**max_depth**: Maximum depth of the tree\n",
    "- Default = none\n",
    "    - Split until the class value is completely determined\n",
    "    - Or until the number of data is less than min_samples_split\n",
    "- As the depth increases, it may overfit, so proper control is required.<br>\n",
    "\n",
    "**max_leaf_nodes**: The maximum number of leaf nodes\n",
    "\n",
    "### GridSearchCV initializer\n",
    "- estimator: classifier, regressor, pipeline, and so on.\n",
    "\n",
    "- param_grid: In the dictionary type, input the parameters that are going to be used for parameter tuning.\n",
    "\n",
    "- scoring: Method to evaluate the prediction performance. Usually set to accuracy.\n",
    "\n",
    "- cv: Specifies the number of divisions in cross-validation(The number of fold).\n",
    "\n",
    "- refit: The default value is True. When it is set default, it finds the optimal hyperparameter and retrains it.\n",
    "\n",
    "- n_jobs: The default value is 1, Set -1 to use all cores."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "model = RandomForestRegressor(criterion = 'squared_error',\n",
    "                              random_state=2022)\n",
    "\n",
    "# Preference for GridSearchCV\n",
    "params = {'n_estimators': [200, 300, 500],\n",
    "          'max_features': [5, 6, 8],\n",
    "          'min_samples_leaf': [1, 3, 5]}\n",
    "\n",
    "# Separate GridSearchCV for each model\n",
    "greedy_CV1 = GridSearchCV(model1,\n",
    "                          param_grid = params,\n",
    "                          cv = 3,\n",
    "                          n_jobs = -1,\n",
    "                          verbose=2)\n",
    "\n",
    "greedy_CV2 = GridSearchCV(model2,\n",
    "                          param_grid = params,\n",
    "                          cv = 3,\n",
    "                          n_jobs = -1,\n",
    "                          verbose=2)\n",
    "\n",
    "greedy_CV3 = GridSearchCV(model3,\n",
    "                          param_grid = params,\n",
    "                          cv = 3,\n",
    "                          n_jobs = -1,\n",
    "                          verbose=2)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model with each train data\n",
    "greedy_CV1.fit(X_train1, Y_train)\n",
    "greedy_CV2.fit(X_train2, Y_train)\n",
    "greedy_CV3.fit(X_train3, Y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(end_time-start_time)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Predict with the model\n",
    "prediction1 = greedy_CV1.predict(test1)\n",
    "prediction2 = greedy_CV2.predict(test2)\n",
    "prediction3 = greedy_CV3.predict(test3)\n",
    "\n",
    "print(prediction1)\n",
    "print(prediction2)\n",
    "print(prediction3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Save the results in csv files\n",
    "import numpy as np\n",
    "\n",
    "GridSearchCV_result1 = pd.read_csv('data/submission.csv')\n",
    "GridSearchCV_result1['count'] = np.round(prediction1, 2)\n",
    "\n",
    "GridSearchCV_result2 = pd.read_csv('data/submission.csv')\n",
    "GridSearchCV_result2['count'] = np.round(prediction2, 2)\n",
    "\n",
    "GridSearchCV_result3 = pd.read_csv('data/submission.csv')\n",
    "GridSearchCV_result3['count'] = np.round(prediction3, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(GridSearchCV_result1.head(), '\\n')\n",
    "print(GridSearchCV_result2.head(), '\\n')\n",
    "print(GridSearchCV_result3.head())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "GridSearchCV_result1.to_csv('GridSearchCV_result1.csv', index=False)\n",
    "GridSearchCV_result2.to_csv('GridSearchCV_result2.csv', index=False)\n",
    "GridSearchCV_result3.to_csv('GridSearchCV_result3.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "c09ee3a4",
   "metadata": {},
   "source": [
    "### GridSearch\n",
    "\n",
    "Amongst the hyperparameter tuning techniques, GridSearch, a sort of exhaustive search, shows the best performance. GridSearch is a technique that finds the best combination among the possible combinations. However, GridSearch also has cons because the training consumes a lot of time.<br>\n",
    "\n",
    "For now, we wikk implement an exhaustive search using GreadSearCV module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9a3af6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-09-09 14:04:57--  https://bit.ly/3gLj0Q6\n",
      "Resolving bit.ly (bit.ly)... 67.199.248.11, 67.199.248.10\n",
      "Connecting to bit.ly (bit.ly)|67.199.248.11|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://drive.google.com/uc?export=download&id=1or_QN1ksv81DNog6Tu_kWcZ5jJWf5W9E [following]\n",
      "--2022-09-09 14:04:57--  https://drive.google.com/uc?export=download&id=1or_QN1ksv81DNog6Tu_kWcZ5jJWf5W9E\n",
      "Resolving drive.google.com (drive.google.com)... 172.217.174.110, 2404:6800:4004:825::200e\n",
      "Connecting to drive.google.com (drive.google.com)|172.217.174.110|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://doc-0c-10-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/jevu56pm6ghc5grm1s630dvrmn1eot8k/1662699825000/17946651057176172524/*/1or_QN1ksv81DNog6Tu_kWcZ5jJWf5W9E?e=download&uuid=a9050603-3434-4aea-b16b-158a74693759 [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2022-09-09 14:04:58--  https://doc-0c-10-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/jevu56pm6ghc5grm1s630dvrmn1eot8k/1662699825000/17946651057176172524/*/1or_QN1ksv81DNog6Tu_kWcZ5jJWf5W9E?e=download&uuid=a9050603-3434-4aea-b16b-158a74693759\n",
      "Resolving doc-0c-10-docs.googleusercontent.com (doc-0c-10-docs.googleusercontent.com)... 142.251.42.129, 2404:6800:4004:825::2001\n",
      "Connecting to doc-0c-10-docs.googleusercontent.com (doc-0c-10-docs.googleusercontent.com)|142.251.42.129|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 39208 (38K) [application/zip]\n",
      "Saving to: ‚Äò3gLj0Q6‚Äô\n",
      "\n",
      "3gLj0Q6             100%[===================>]  38.29K  --.-KB/s    in 0.05s   \n",
      "\n",
      "2022-09-09 14:04:59 (829 KB/s) - ‚Äò3gLj0Q6‚Äô saved [39208/39208]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Downloading data\n",
    "!wget 'https://bit.ly/3gLj0Q6'\n",
    "\n",
    "# Unzip the downloaded data\n",
    "import zipfile\n",
    "with zipfile.ZipFile('3gLj0Q6', 'r') as existing_zip:\n",
    "    existing_zip.extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46bcff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas and RandomForestRegressor\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4ab4044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e42e6907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Train Data ============\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1459 entries, 0 to 1458\n",
      "Data columns (total 11 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   id                      1459 non-null   int64  \n",
      " 1   hour                    1459 non-null   int64  \n",
      " 2   hour_bef_temperature    1457 non-null   float64\n",
      " 3   hour_bef_precipitation  1457 non-null   float64\n",
      " 4   hour_bef_windspeed      1450 non-null   float64\n",
      " 5   hour_bef_humidity       1457 non-null   float64\n",
      " 6   hour_bef_visibility     1457 non-null   float64\n",
      " 7   hour_bef_ozone          1383 non-null   float64\n",
      " 8   hour_bef_pm10           1369 non-null   float64\n",
      " 9   hour_bef_pm2.5          1342 non-null   float64\n",
      " 10  count                   1459 non-null   float64\n",
      "dtypes: float64(9), int64(2)\n",
      "memory usage: 125.5 KB\n",
      "Train Data Information\n",
      " None \n",
      "\n",
      "Train Data Shape:  (1459, 11) \n",
      "\n",
      "============ Test Data ============\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 715 entries, 0 to 714\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   id                      715 non-null    int64  \n",
      " 1   hour                    715 non-null    int64  \n",
      " 2   hour_bef_temperature    714 non-null    float64\n",
      " 3   hour_bef_precipitation  714 non-null    float64\n",
      " 4   hour_bef_windspeed      714 non-null    float64\n",
      " 5   hour_bef_humidity       714 non-null    float64\n",
      " 6   hour_bef_visibility     714 non-null    float64\n",
      " 7   hour_bef_ozone          680 non-null    float64\n",
      " 8   hour_bef_pm10           678 non-null    float64\n",
      " 9   hour_bef_pm2.5          679 non-null    float64\n",
      "dtypes: float64(8), int64(2)\n",
      "memory usage: 56.0 KB\n",
      "Test Data Information\n",
      " None \n",
      "\n",
      "Test Data Shape:  (715, 10) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if the data loading is successful\n",
    "print('============ Train Data ============\\n')\n",
    "print('Train Data Information\\n', train.info(), '\\n')\n",
    "print('Train Data Shape: ', train.shape, '\\n')\n",
    "\n",
    "print('============ Test Data ============')\n",
    "print('Test Data Information\\n', test.info(), '\\n')\n",
    "print('Test Data Shape: ', test.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf731083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                          0\n",
      "hour                        0\n",
      "hour_bef_temperature        2\n",
      "hour_bef_precipitation      2\n",
      "hour_bef_windspeed          9\n",
      "hour_bef_humidity           2\n",
      "hour_bef_visibility         2\n",
      "hour_bef_ozone             76\n",
      "hour_bef_pm10              90\n",
      "hour_bef_pm2.5            117\n",
      "count                       0\n",
      "dtype: int64 \n",
      "\n",
      "id                         0\n",
      "hour                       0\n",
      "hour_bef_temperature       1\n",
      "hour_bef_precipitation     1\n",
      "hour_bef_windspeed         1\n",
      "hour_bef_humidity          1\n",
      "hour_bef_visibility        1\n",
      "hour_bef_ozone            35\n",
      "hour_bef_pm10             37\n",
      "hour_bef_pm2.5            36\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check if there are missing values\n",
    "print(train.isnull().sum(), '\\n')\n",
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d3c88ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the missing values using linear interpolation\n",
    "train.interpolate(inplace=True)\n",
    "test.interpolate(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a922f883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                        0\n",
      "hour                      0\n",
      "hour_bef_temperature      0\n",
      "hour_bef_precipitation    0\n",
      "hour_bef_windspeed        0\n",
      "hour_bef_humidity         0\n",
      "hour_bef_visibility       0\n",
      "hour_bef_ozone            0\n",
      "hour_bef_pm10             0\n",
      "hour_bef_pm2.5            0\n",
      "count                     0\n",
      "dtype: int64 \n",
      "\n",
      "id                        0\n",
      "hour                      0\n",
      "hour_bef_temperature      0\n",
      "hour_bef_precipitation    0\n",
      "hour_bef_windspeed        0\n",
      "hour_bef_humidity         0\n",
      "hour_bef_visibility       0\n",
      "hour_bef_ozone            0\n",
      "hour_bef_pm10             0\n",
      "hour_bef_pm2.5            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check if the null values are replaced well.\n",
    "print(train.isnull().sum(), '\\n')\n",
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "307026fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare the model\n",
    "X_train = train.drop(['count'], axis=1)\n",
    "Y_train = train['count']\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestRegressor(criterion = 'squared_error')\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb6a4348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02539346, 0.5968231 , 0.17926783, 0.01670445, 0.02576764,\n",
       "       0.03619231, 0.03358093, 0.03428622, 0.03153636, 0.02044771])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the feature importances\n",
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c700fb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train datasets by removing the less important features\n",
    "X_train1 = train.drop(['count', 'id'], axis=1)\n",
    "X_train2 = train.drop(['count', 'id', 'hour_bef_windspeed'], axis=1)\n",
    "X_train3 = train.drop(['count', 'id', 'hour_bef_windspeed', 'hour_bef_pm2.5'], axis=1)\n",
    "\n",
    "Y_train = train['count']\n",
    "\n",
    "# Create test datasets\n",
    "test1 = test.drop(['id'], axis=1)\n",
    "test2 = test.drop(['id', 'hour_bef_windspeed'], axis=1)\n",
    "test3 = test.drop(['id', 'hour_bef_windspeed', 'hour_bef_pm2.5'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a80fd2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train1.shape:  (1459, 9) \n",
      "\n",
      "X_train2.shape:  (1459, 8) \n",
      "\n",
      "X_train3.shape:  (1459, 7) \n",
      "\n",
      "Y_train.shape:  (1459,) \n",
      "\n",
      "test1.shape (715, 9) \n",
      "\n",
      "test2.shape (715, 8) \n",
      "\n",
      "test3.shape (715, 7) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of training and test data\n",
    "print('X_train1.shape: ', X_train1.shape, '\\n')\n",
    "print('X_train2.shape: ', X_train2.shape, '\\n')\n",
    "print('X_train3.shape: ', X_train3.shape, '\\n')\n",
    "print('Y_train.shape: ', Y_train.shape, '\\n')\n",
    "print('test1.shape', test1.shape, '\\n')\n",
    "print('test2.shape', test2.shape, '\\n')\n",
    "print('test3.shape', test3.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18a58cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare separate models\n",
    "model1 = RandomForestRegressor(criterion = 'squared_error')\n",
    "model2 = RandomForestRegressor(criterion = 'squared_error')\n",
    "model3 = RandomForestRegressor(criterion = 'squared_error')\n",
    "\n",
    "# Train the saparated models\n",
    "model1.fit(X_train1, Y_train)\n",
    "model2.fit(X_train2, Y_train)\n",
    "model3.fit(X_train3, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671690b1",
   "metadata": {},
   "source": [
    "### RandomForest Hyperparameters\n",
    "\n",
    "**n_estimators:** Number of decision making tree\n",
    "- Default = 10\n",
    "- When increase it, the performance may get better, but may cause too much train time.<br>\n",
    "\n",
    "**min_samples_split**: The minimum number of sample used to split node\n",
    "- Used to control overfitting\n",
    "- Default = 2: The smaller the value, the greater possibility of overfitting because of the increasing node split<br>\n",
    "\n",
    "**min_samples_leaf**: The minimum number of samples to be leaf node\n",
    "- Along to min_samples_split, it is used to control the overfitting\n",
    "- When the data is imbalanced, some data of a specific class may extremely small, thus it needs to be kept the small value<br>\n",
    "\n",
    "**max_features**: Maximum number of features for optimal split\n",
    "- Default = 'auto'\n",
    "    - Note: The default value of max_feature is none in decision tree\n",
    "- When specified in int type: The number of features\n",
    "- When specified in float type: The ratio of features\n",
    "- 'sqrt' or 'auto': Samples as many as $\\sqrt{The\\;number\\;of\\;whole\\;features}$\n",
    "- log : Samples as many as $\\log_2{(The\\;number\\;of\\;whole\\;features)}$<br>\n",
    "\n",
    "**max_depth**: Maximum depth of the tree\n",
    "- Default = none\n",
    "    - Split until the class value is completely determined\n",
    "    - Or until the number of data is less than min_samples_split\n",
    "- As the depth increases, it may overfit, so proper control is required.<br>\n",
    "\n",
    "**max_leaf_nodes**: The maximum number of leaf nodes\n",
    "\n",
    "### GridSearchCV initializer\n",
    "- estimator: classifier, regressor, pipeline, and so on.\n",
    "\n",
    "- param_grid: In the dictionary type, input the parameters that are going to be used for parameter tuning.\n",
    "\n",
    "- scoring: Method to evaluate the prediction performance. Usually set to accuracy.\n",
    "\n",
    "- cv: Specifies the number of divisions in cross-validation(The number of fold).\n",
    "\n",
    "- refit: The default value is True. When it is set default, it finds the optimal hyperparameter and retrains it.\n",
    "\n",
    "- n_jobs: The default value is 1, Set -1 to use all cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bec88cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "27 fits failed out of a total of 81.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "27 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 1315, in fit\n",
      "    super().fit(\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 308, in fit\n",
      "    raise ValueError(\"max_features must be in (0, n_features]\")\n",
      "ValueError: max_features must be in (0, n_features]\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.7629426  0.76528353 0.76713505 0.76176363 0.76112947 0.76229666\n",
      " 0.75510665 0.75623286 0.75604972 0.76521787 0.7650324  0.76533519\n",
      " 0.76463531 0.76444744 0.76518325 0.75945467 0.75838434 0.75819054\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.009126663208008\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "model = RandomForestRegressor(criterion = 'squared_error',\n",
    "                              random_state=2022)\n",
    "\n",
    "# Preference for GridSearchCV\n",
    "params = {'n_estimators': [200, 300, 500],\n",
    "          'max_features': [5, 6, 8],\n",
    "          'min_samples_leaf': [1, 3, 5]}\n",
    "\n",
    "# Separate GridSearchCV for each model\n",
    "greedy_CV1 = GridSearchCV(model1,\n",
    "                          param_grid = params,\n",
    "                          cv = 3,\n",
    "                          n_jobs = -1,\n",
    "                          verbose=2)\n",
    "\n",
    "greedy_CV2 = GridSearchCV(model2,\n",
    "                          param_grid = params,\n",
    "                          cv = 3,\n",
    "                          n_jobs = -1,\n",
    "                          verbose=2)\n",
    "\n",
    "greedy_CV3 = GridSearchCV(model3,\n",
    "                          param_grid = params,\n",
    "                          cv = 3,\n",
    "                          n_jobs = -1,\n",
    "                          verbose=2)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model with each train data\n",
    "greedy_CV1.fit(X_train1, Y_train)\n",
    "greedy_CV2.fit(X_train2, Y_train)\n",
    "greedy_CV3.fit(X_train3, Y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed8a8de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 97.74  214.305  82.48   38.185  72.06  125.79  181.56  304.565  44.4\n",
      " 117.175 302.65  257.535 100.005  43.15  200.18  163.025  27.14  169.93\n",
      " 335.46  159.795 224.225  85.985  27.475 140.08  133.92  116.725  26.93\n",
      " 118.745 111.11  160.51   77.215  36.575  62.76  131.765 283.16   39.705\n",
      " 135.25  103.15  233.845  84.745  61.55  123.305 163.88   88.385 327.975\n",
      " 178.39   96.23   62.465  19.245  87.145 227.205  92.36  168.635  88.74\n",
      " 197.24  139.945  51.425 175.54   25.905  19.72   92.345  82.185 258.2\n",
      " 303.2   146.265 312.255  27.94  244.4   112.38   32.455 102.925  34.335\n",
      " 124.425  15.395 318.785 227.815  35.975 171.735 231.08   27.545 248.265\n",
      " 134.395  86.215  83.295  91.04  324.685  49.845 171.745 112.2   273.94\n",
      " 292.135 159.215  65.48  101.275  41.865  80.64  102.63   27.885 220.145\n",
      " 143.53   18.935 153.795  36.73  120.515  78.54   72.54  101.385  25.77\n",
      " 176.775 123.345 179.    243.47  170.335 120.785  61.4   129.475 230.61\n",
      "  39.125 201.34   19.205 103.71  106.26  175.085 123.68   51.    119.845\n",
      "  58.895  15.89  219.85   62.77  131.585 161.56   19.905 143.775 297.41\n",
      " 125.43   45.415 143.66  232.365 354.55  206.345  34.645  37.23  106.03\n",
      "  89.065 113.595 107.745 126.325 154.34  223.285 135.895 294.825 313.385\n",
      " 222.755 156.88   13.24  117.805  23.015  93.165 144.78  146.12   28.41\n",
      " 207.985 219.94  125.915  23.85  202.625 157.22  282.93   24.615 123.87\n",
      " 131.35   87.63  128.71  100.42   99.615 223.94  201.065 161.795 113.275\n",
      " 200.    184.98  154.01  248.785 143.095 103.92  115.725 295.545 166.96\n",
      "  47.295  39.585 145.25  121.515 114.225 135.465 275.68  324.83  143.925\n",
      " 125.14  108.475  27.055 203.695  25.71  329.07  147.53   41.08  129.63\n",
      " 241.115 227.125  64.74  287.985 185.315 134.83  225.72  245.175 125.44\n",
      " 249.26  120.03  138.315  91.155  96.595  32.53  104.795  66.195 139.8\n",
      "  39.375 212.46  178.845  37.905 196.57  263.425 100.675  27.41   85.545\n",
      " 215.95  100.77  361.585 315.105 157.62  200.3    20.065 219.215  32.915\n",
      " 137.665 152.275  96.05  108.95  231.09  103.715  24.06   83.025 266.115\n",
      " 241.04  198.55  139.02  203.43   36.39  336.2    76.715  18.85  159.36\n",
      "  34.585 230.145  32.985 177.495 246.965  26.255 229.785  86.2   187.935\n",
      " 135.88  107.88   55.715 318.47   29.335 277.495  87.08  154.485 215.795\n",
      "  37.435 285.965 101.135  16.31  162.8   200.075 116.885 245.665 136.505\n",
      "  73.24  105.825 119.24  264.09  174.58  123.815 135.075  39.56  144.675\n",
      "  47.325 228.645  23.715 102.605 203.37  207.405 230.19  340.605 202.04\n",
      " 132.83   66.17  158.94   86.535 275.84  258.635 253.37   47.15  264.575\n",
      " 215.715 103.68   41.175  32.59   19.48  216.66   29.7   122.88  135.135\n",
      "  91.915 351.835 265.565  99.165  90.39   98.03   26.625 101.27   84.595\n",
      " 184.81  150.605  42.32  253.66  225.03  151.74  344.485  20.445  31.84\n",
      " 319.255 179.915 121.825  24.145  28.575 100.21  121.6   296.97  204.35\n",
      " 234.02   71.83   48.885 198.535  69.62   19.77   29.635 100.33  110.925\n",
      "  64.17  211.49  232.23  195.07  162.74   37.41   81.72  101.88   23.91\n",
      " 205.975  91.76   72.38  237.525 302.875 225.62   18.565  41.43  189.785\n",
      " 105.895  47.975 128.545 117.84  305.845 282.795 118.99   85.475 137.025\n",
      " 163.905 120.7   231.955  40.705 119.265 289.85   56.075 118.35  115.35\n",
      " 271.045  35.675  28.045 194.69   19.205  86.375  24.33  266.26   83.635\n",
      " 122.805 149.525 187.425 109.91  119.92  162.44  279.015 138.75  120.825\n",
      "  51.99  126.735 177.87  119.985  60.16   62.225  30.995  35.835  18.725\n",
      "  38.29  165.16  115.32   99.425 175.73  116.915 338.04  134.215 189.96\n",
      " 245.79  115.845 113.625 220.68  333.575  38.98  120.005 350.81  260.84\n",
      " 150.68   46.505  96.2   115.805  99.875 114.215  35.65  325.84  288.635\n",
      " 242.33  117.36  169.775 222.355 136.495 141.635 238.725 104.065  64.855\n",
      " 120.96  188.64  130.145 115.065 112.88   85.895 141.93  268.375  96.835\n",
      " 175.505 188.785 238.125  55.53  322.555  90.99  164.29  167.7    24.085\n",
      "  28.51  217.595 269.13  102.66  104.315  33.275  26.605 255.625 142.195\n",
      " 322.71  239.405  86.335 108.805 230.61  176.735  78.745  81.07   51.08\n",
      " 192.285 113.585 229.4   287.505 213.29   15.71  227.035  55.735  89.125\n",
      "  43.985 248.92  245.685  28.745 154.495  74.775 196.715 203.585 206.37\n",
      " 190.975 333.71   99.19   35.01  242.48  144.255 148.4    33.14  228.81\n",
      "  18.05   52.99   27.35   77.345  32.015 175.32  199.925 152.3    43.825\n",
      "  89.815  37.365 300.545  95.085  38.855 102.445  24.705  43.49   32.945\n",
      " 301.555  19.035 191.14  241.915 311.465 210.345 102.995 121.91  105.02\n",
      " 101.7   209.825  39.635  19.52  141.865  18.1   183.77  141.235  48.735\n",
      " 150.68  210.705 214.9   331.955  26.76  101.87   37.75   44.195  97.855\n",
      " 103.705  33.93  172.32   87.39  205.1   127.49  336.645 166.915 273.275\n",
      " 136.335 279.55   63.17   30.305  27.98   37.23   87.725  25.255  24.37\n",
      "  99.995  25.44  265.655 229.1   145.55   86.375 105.74   37.375 144.45\n",
      "  97.745 132.02  190.86   24.265 226.61  123.265  13.955 277.99  103.64\n",
      "  36.545  78.615 255.7   116.57  362.835 287.685 226.525 223.135 197.505\n",
      "  20.385 119.63  148.575  29.855  44.795 101.825 217.685 199.875  57.84\n",
      " 179.31   95.185 248.69  119.965 318.695  70.77   33.11   24.535 237.\n",
      " 240.435  91.85  111.635  30.33   93.715 142.315 170.61  112.87  108.565\n",
      "  45.445 116.375 102.59   73.16  106.945 314.055 134.44  263.045 129.915\n",
      " 290.8   192.975  79.81  121.26   94.97  303.32  259.015 258.19  111.245\n",
      " 119.28  128.49   20.795 129.375 248.065  35.35   85.64  120.275 118.065\n",
      " 189.975 129.915 295.795  88.715 100.13  132.49  304.21  110.31  155.905\n",
      " 137.495  39.825 197.955  27.745  30.63   25.795 225.45  103.745 156.54\n",
      " 246.315 138.44  121.24   32.35  191.475  20.09  135.33   30.1    44.065\n",
      "  95.065  51.76  191.095  18.045  94.58  192.125  30.11  304.795 270.22\n",
      "  16.12  367.64  117.625 265.07   37.705  87.315  38.095 275.115  59.815\n",
      "  81.155 124.365 160.215 208.395]\n",
      "[ 99.89388921 207.39685907  76.8280493   32.94948431  64.37223779\n",
      " 119.82035155 216.7894825  326.38500379  32.62027435 110.75554192\n",
      " 300.01985068 250.06134615  92.65344648  36.89254909 222.41844589\n",
      " 154.78645274  26.75551461 204.68158965 349.11373918 146.81557233\n",
      " 214.03765844  86.08784808  16.0317702  140.1468676  133.6135657\n",
      " 112.55360408  29.36291378 115.80291703 111.21641919 155.91040677\n",
      "  75.44273657  31.18402092  62.00265079 126.93045527 275.36388672\n",
      "  32.61852958 132.18812825 128.16596086 219.42383622  73.75786702\n",
      "  54.44610317 122.79961905 161.4937798   80.75917635 325.35774408\n",
      " 186.28493705  80.51157233  58.40903139  17.28546374  87.68310706\n",
      " 239.97144687  94.83407702 157.71756763  84.28696795 190.10462302\n",
      " 140.20536007  44.26826721 177.48426461  15.14192298  19.53590981\n",
      "  91.81621266  81.43987067 259.42307823 309.29909653 148.07159815\n",
      " 312.33402167  15.59587608 221.79105809 142.59188312  27.26860949\n",
      " 102.30985714  32.43753571 126.97890661  15.86270851 320.17259869\n",
      " 221.88137515  34.19461959 175.12618203 226.05479491  18.39309217\n",
      " 254.79318074 130.76139347  94.04389159  85.98297863  88.20489484\n",
      " 337.6132574   52.55005754 164.78223629  99.03758442 288.31333102\n",
      " 287.02093397 157.28157991  63.33514484 100.82130664  34.19972226\n",
      "  86.05588141  96.44556746  28.70564845 217.67212536 139.11654855\n",
      "  19.48393568 155.48382738  43.60795375 116.31653211  57.26946014\n",
      "  68.05350168 103.0703934   16.52320815 175.2372031  142.595671\n",
      " 174.09333038 229.01426371 169.37674552 114.75718723  63.35372817\n",
      " 118.57133417 253.6724421   34.30382562 199.36197598  18.48442663\n",
      " 104.31141144 109.07962716 207.99008712 117.7921414   44.50824935\n",
      " 116.360886    54.93964683  14.80912626 220.21801353  54.38458135\n",
      " 130.87898431 157.19457918  17.43474098 134.0572969  310.19827276\n",
      " 125.92848227  43.79417233 141.5353334  260.23482486 358.41089719\n",
      " 179.92865156  27.79609903  36.67458099  92.94530754  98.71122186\n",
      " 117.52247222 107.48856602 120.18362085 148.87181223 217.19673286\n",
      " 128.38160784 292.1728153  325.06309037 229.09583207 174.84511129\n",
      "  12.25287897 114.97954221  13.27542803  90.63923954 140.8846039\n",
      " 141.56089538  14.4704412  206.30223683 220.3644504  125.20200325\n",
      "  14.45757937 248.59839538 152.58291324 290.20323596  17.27197168\n",
      " 119.402182   129.66417454  85.3509044  127.21207756  97.8329062\n",
      "  99.49537915 243.49483514 203.57634163 150.07199549 107.26446712\n",
      " 205.09921194 171.16455518 159.39989412 239.72106747 142.99311382\n",
      "  90.90942857  97.82804473 309.37894393 155.60646916  44.501293\n",
      "  34.58275144 137.92469372 119.18496447 105.35999441 126.73473406\n",
      " 267.58804691 336.8135983  136.68970455 120.14195328 105.88566667\n",
      "  17.0741618  239.32164087  14.52238564 334.19750505 143.14964304\n",
      "  36.54206187 128.74646134 247.7705184  218.72281475  64.2361369\n",
      " 277.5993483  149.76406649 132.55098149 210.09186887 253.38868651\n",
      " 128.37857547 234.08040747 118.14079293 139.10436364  89.74600876\n",
      "  87.31958189  26.86850433 105.15235462  69.48054022 133.51627925\n",
      "  33.6179425  202.09162171 178.49914196  36.72238835 167.21405951\n",
      " 262.73005195 117.24973773  19.08582341  67.62401497 196.95020409\n",
      " 102.70693615 365.7000276  318.58311132 166.88916342 197.28096952\n",
      "  18.61033322 205.02789481  26.5826829  133.45406259 142.46662719\n",
      "  99.21644986 133.38791288 234.16728409 101.22894061  14.7013061\n",
      "  95.65108911 254.2201994  229.75693777 172.13864284 141.00449558\n",
      " 171.55796896  27.0160294  349.32191234  84.10375794  19.10204369\n",
      " 158.17691908  27.31808748 228.30990855  27.50815152 159.42042315\n",
      " 243.01544048  15.78286021 233.74876551  85.1630203  181.85613763\n",
      " 130.48253671 108.10055952  53.49726389 316.43374696  18.60251285\n",
      " 266.42752309  84.09130159 181.0163456  189.14205278  32.21856548\n",
      " 289.71069808 102.15144931  16.28015639 162.14257067 171.19414712\n",
      " 114.39239484 244.26516919 128.6757408   72.46204611 110.70903487\n",
      " 116.42690067 263.40604401 143.99984378 114.91146422 131.71312807\n",
      "  33.79286093 143.27004618  40.21502686 225.90198629  26.37622998\n",
      " 102.90044682 221.83877201 222.85052438 223.73027796 361.28582222\n",
      " 199.52232558 134.46388131  56.00414881 164.34323626  93.06571781\n",
      " 266.595136   247.55767929 243.33368117  36.34988202 259.76347457\n",
      " 206.06099317 140.32817623  34.5683759   27.07395455  18.73320588\n",
      " 225.33377958  31.80019048 134.98463709 151.01871771 100.43477165\n",
      " 356.56333965 277.45398776  97.60122385  81.92426804  99.86806349\n",
      "  26.96935372  96.11536201  80.26447589 185.92002074 147.08425361\n",
      "  37.35054203 251.73249784 242.40198701 144.60788104 350.56710354\n",
      "  18.55643038  19.58139989 314.48942553 182.1580285  120.71796825\n",
      "  15.53246338  32.5613438   97.61422835 118.71691829 295.32446991\n",
      " 226.35337049 227.07354294  53.35312536  46.45686194 198.42543236\n",
      "  61.91410119  19.14981999  20.45347426 100.83996266 113.49095238\n",
      "  59.52496032 212.99255556 250.28265891 159.748294   153.63855898\n",
      "  29.38996591  73.4032767  101.09514791  13.57072547 186.03557684\n",
      "  92.26908189  80.8459271  227.29201172 307.26280969 220.69158041\n",
      "  18.60415437  35.12533568 173.12914556 101.05606796  38.21567244\n",
      " 126.43228534 115.52896302 311.7039975  289.76090461 119.12463294\n",
      "  62.12973815 130.62669165 165.44613131 118.20571356 228.04367442\n",
      "  35.60058676 154.72847168 294.87222712  52.06771284 116.4253741\n",
      " 111.89995905 273.61494751  27.81593218  30.33076858 201.21175234\n",
      "  19.84578824  89.94647114  15.63195833 264.30753608  80.28448846\n",
      " 117.52098737 183.31625631 161.80424259 115.57800072 114.2228382\n",
      " 167.38838095 292.28318545 132.45567478 122.11231061  44.50824935\n",
      " 123.36387811 163.61154311 118.25844156  54.80042857  53.75187897\n",
      "  19.42224123  26.95906764  18.21686711  35.74275848 165.36813817\n",
      " 113.972454    98.86619156 166.94274639 113.3187675  341.53235288\n",
      " 125.7333578  177.60725309 261.58299387 109.08228595 115.173454\n",
      " 237.54966576 323.0448799   35.02102993 116.30512103 352.74417431\n",
      " 264.39685696 151.06150541  38.63172857  94.64758856 111.48230243\n",
      "  98.53195635 110.43611797  29.73147655 316.69223758 295.5711584\n",
      " 225.54126353 115.46343723 165.84812034 231.43323593 139.281504\n",
      " 139.02355915 235.86033802 102.93284993  58.24860317 120.57673611\n",
      " 208.26361057 129.387293   116.01558748 116.79954076  89.85409921\n",
      " 140.54739739 278.45218419  94.61663438 170.46806854 185.53890247\n",
      " 241.39585859  54.81950992 323.13899549  86.39978265 150.1772076\n",
      " 167.91142963  14.56728932  28.58085841 235.09612478 284.5860276\n",
      " 103.29387338 100.68661323  27.56767893  14.80258568 254.5990614\n",
      " 133.80506926 336.24367623 255.54553716  90.80381305 120.63444859\n",
      " 215.0128097  177.15470764  68.17483892  76.91773625  42.39435291\n",
      " 157.02336777 106.35284239 224.8359311  289.53000487 219.54414959\n",
      "  14.85885299 223.37744406  51.46450794  93.55542605  36.9094807\n",
      " 240.75280077 241.17757079  30.94512392 151.3273575   82.63400613\n",
      " 240.70447637 222.29137608 210.09783965 215.74314451 347.2168833\n",
      "  95.90515314  34.76087879 260.48314004 140.65022514 182.74489917\n",
      "  28.69791558 226.72448936  19.1891834   52.11217262  18.52839411\n",
      "  69.4355119   30.55281999 205.14444336 191.55276677 152.7658756\n",
      "  32.94436688  80.18564895  31.64294227 309.14740569  94.5240018\n",
      "  34.56877633  98.94882955  16.32106475  43.43098293  34.08254311\n",
      " 311.42235212  15.01961436 181.6902424  246.33708442 313.18537521\n",
      " 216.95491127 103.30234127 130.51696216 104.29798936 103.11155862\n",
      " 207.70257594  31.93459402  18.3072385  138.87826082  18.44818604\n",
      " 159.97093523 138.41319805  36.72509668 164.85193813 200.95940658\n",
      " 208.56010553 349.39297493  28.27438997  99.77844931  36.66125221\n",
      "  39.14113925 100.3734053   94.99252395  30.1478263  169.73735732\n",
      "  89.60548026 195.53773178 124.10294661 340.64240188 172.90862933\n",
      " 288.35872315 129.29394372 283.45552327  60.13959538  34.46035065\n",
      "  29.93558369  37.14448521  84.81096023  14.82109073  13.89318651\n",
      "  96.59038709  18.34971553 272.92715713 221.15150379 143.72333559\n",
      "  88.20594093  97.97081313  31.06311364 141.9438229   97.85953391\n",
      " 132.95177137 188.7486362   14.94047276 229.34402615 119.99368687\n",
      "  13.27909037 267.04237592 102.87091685  31.268296    93.14393921\n",
      " 244.15345853 106.62974105 367.65003084 287.48094607 216.78838149\n",
      " 212.22103555 190.26054372  19.17306861 115.64456926 148.94508045\n",
      "  18.50037306  45.42143622  92.41267496 208.61558676 164.73643722\n",
      "  53.67390927 168.47618079  95.81268543 254.52434307 112.37882738\n",
      " 314.89647153  61.06734402  30.67308712  16.21224026 234.12189291\n",
      " 241.0885588   92.24172312 107.36190756  27.64020887  93.6955211\n",
      " 141.42527271 171.45404365 106.10431029 101.70393164  36.75111346\n",
      " 116.04006548 104.14632828  64.59981169 108.89311111 321.18055295\n",
      " 129.27531734 258.26418272 133.63837805 299.78969537 206.93600415\n",
      "  69.09316879 118.21939791  94.04225487 308.45800093 244.58585047\n",
      " 257.9269412  107.1247482  116.39352489 123.69365149  17.68799528\n",
      " 136.94851136 268.58869682  27.64950992  86.96088474 115.46029221\n",
      " 118.76410606 184.24823712 124.04721946 303.42423178  85.10583578\n",
      " 101.3137264  133.72671014 301.5424782  107.58006097 158.16643722\n",
      " 130.86376046  32.86070292 164.92702343  17.45676894  30.63246447\n",
      "  16.46448341 227.74370346 102.85658207 150.34089845 267.14595905\n",
      " 131.82506629 121.95705017  26.31926605 164.15975594  15.26354437\n",
      " 132.0743925   29.70992857  34.74234578  95.93121843  46.45132479\n",
      " 194.62016901  18.1365445   89.69080941 180.24272637  32.54492027\n",
      " 307.58305252 271.19457241  15.02644968 370.5709623  116.37077994\n",
      " 265.98607963  32.95994841  88.50748603  31.08757395 277.45257823\n",
      "  56.96865873  67.02453463 119.50675577 156.33459235 178.48845165]\n",
      "[108.348 207.186  88.8    49.334  57.734 120.078 194.266 309.592  47.652\n",
      " 112.106 296.054 253.154  96.03   51.562 204.814 143.496  27.506 153.766\n",
      " 341.048 154.052 225.972  88.65   36.88  137.602 142.832 116.122  27.39\n",
      " 115.166 110.356 159.678  79.46   45.132  68.01  133.674 272.63   48.532\n",
      " 133.244 117.934 246.232  90.644  54.91  122.352 150.718  97.896 328.742\n",
      " 210.952  94.376  64.268  21.534  84.564 228.432  92.88  162.92   82.612\n",
      " 195.834 123.052  58.696 179.392  34.534  20.226  88.478  89.382 251.488\n",
      " 298.34  157.456 312.648  37.552 236.234 128.682  41.31  104.846  33.016\n",
      " 123.16   16.986 315.272 232.326  34.53  175.288 235.626  38.294 250.794\n",
      " 138.106  77.744  82.668  93.79  319.296  58.266 164.302 105.516 272.812\n",
      " 272.41  158.646  72.362 103.764  51.53   79.29  104.864  27.838 221.962\n",
      " 147.362  23.182 149.762  38.38  118.87   86.212  59.952 102.344  34.62\n",
      " 170.72  116.496 175.686 238.506 160.776 115.01   64.198 118.698 228.236\n",
      "  44.742 200.974  25.022 102.272 110.642 156.894 119.882  58.238 114.982\n",
      "  55.834  15.592 236.568  62.552 129.328 157.326  21.522 137.622 296.066\n",
      " 133.42   47.344 149.478 240.822 346.43  189.85   41.166  34.098  99.82\n",
      "  91.24  114.028 104.026 122.348 153.9   230.148 130.122 289.43  302.508\n",
      " 248.696 145.292  11.65  113.306  31.922  91.612 141.046 141.348  39.628\n",
      " 205.444 231.01  125.552  32.976 210.022 155.318 280.186  35.006 120.184\n",
      " 120.804  95.346 128.564 102.98  108.092 208.394 195.714 154.178 109.414\n",
      " 207.846 165.326 165.314 258.842 149.362  96.08  109.526 300.692 167.818\n",
      "  53.134  45.83  139.314 120.226 111.294 124.192 276.164 315.498 140.032\n",
      " 124.156 104.06   34.496 203.248  34.564 331.274 140.948  50.26  128.028\n",
      " 219.036 214.076  65.056 279.836 164.56  131.584 214.416 257.844 127.564\n",
      " 235.39  117.068 138.178  92.442  97.924  39.522 109.718  68.268 142.376\n",
      "  45.45  203.4   173.59   34.682 184.114 242.054 110.88   37.964  71.494\n",
      " 197.446 102.734 365.606 313.012 159.496 179.988  22.364 212.524  39.898\n",
      " 136.158 142.9   100.442 123.97  230.472 103.318  32.822  81.07  259.624\n",
      " 236.248 178.098 146.928 187.122  47.31  331.18   74.512  23.386 162.182\n",
      "  44.034 227.636  38.468 165.938 244.736  34.258 232.852  84.244 184.048\n",
      " 133.4   105.796  57.748 318.2    43.16  264.57   87.296 134.182 192.098\n",
      "  44.128 283.882 104.846  17.358 165.036 175.912 113.362 250.19  135.452\n",
      "  64.566 111.478 118.39  262.612 163.982 109.92  132.636  48.372 145.72\n",
      "  54.508 208.002  17.642 107.184 197.178 213.378 239.424 359.768 208.4\n",
      " 134.656  63.906 161.508  90.474 264.024 256.226 252.07   57.42  258.808\n",
      " 189.57  127.624  45.132  33.84   20.56  210.11   31.474 127.324 128.542\n",
      "  88.882 347.798 252.296 103.84   84.286 102.942  26.906  95.726  76.13\n",
      " 183.498 148.352  37.902 242.236 211.41  147.766 337.332  22.324  39.874\n",
      " 316.154 189.256 123.754  34.464  32.08  100.188 121.14  276.292 208.168\n",
      " 231.302  61.534  56.92  206.892  71.638  25.298  41.498 101.644 118.098\n",
      "  76.076 210.52  244.    173.342 164.598  52.214  81.746 101.34   32.848\n",
      " 188.246  94.424  65.438 232.862 279.106 222.612  19.802  46.202 179.11\n",
      " 106.632  51.964 119.226 113.904 307.462 283.716 115.296  84.128 130.546\n",
      " 165.36  119.82  247.614  46.072 130.16  268.088  56.874 114.438 111.906\n",
      " 263.612  47.754  29.672 221.262  18.674  91.752  32.994 257.172  97.35\n",
      " 121.576 141.852 172.416 119.654 111.672 170.87  286.    135.756 119.704\n",
      "  58.276 128.1   168.256 118.302  61.096  59.856  46.744  38.802  22.77\n",
      "  37.068 166.586 109.836 102.856 184.054 108.216 329.326 123.678 178.23\n",
      " 228.14  109.802 112.744 204.862 320.374  53.836 114.994 350.596 259.156\n",
      " 154.278  54.512  96.426 113.45  110.914 109.76   40.796 324.218 284.444\n",
      " 225.642 114.336 160.592 213.572 137.378 140.582 242.284 101.396  63.532\n",
      " 120.388 178.156 123.562 113.3   118.394  91.072 147.616 252.404 100.014\n",
      " 175.896 186.148 236.872  59.26  317.618  94.26  145.804 166.078  34.25\n",
      "  26.118 231.622 277.6   106.584 106.212  39.93   36.682 266.19  142.78\n",
      " 324.204 231.058  85.232 109.036 219.098 175.324  78.152  74.262  63.136\n",
      " 172.72  115.876 228.03  291.1   228.074  16.776 225.46   59.864  86.226\n",
      "  60.434 235.572 244.216  30.73  154.756  72.884 166.864 190.156 206.138\n",
      " 197.012 339.982  96.53   33.546 253.802 142.838 134.972  42.854 229.234\n",
      "  22.816  58.648  41.53   86.26   36.908 178.8   199.642 152.378  49.274\n",
      "  90.898  46.676 282.694  99.232  49.702 103.056  34.192  43.43   33.58\n",
      " 284.476  23.892 187.972 251.474 299.554 210.812 106.122 116.66  107.622\n",
      " 107.042 203.912  50.702  20.778 141.282  18.774 167.676 135.846  57.902\n",
      " 152.498 208.244 209.432 332.982  27.38  102.814  41.02   50.572 100.994\n",
      " 101.006  42.006 173.74   88.154 193.594 123.646 334.07  179.39  282.19\n",
      " 130.976 269.102  63.644  35.876  27.566  37.532  85.634  34.042  32.74\n",
      " 102.058  40.522 245.834 232.024 148.7    80.502 101.446  43.534 143.186\n",
      "  97.41  127.698 200.128  32.9   233.994 120.614  15.46  275.338 109.822\n",
      "  43.55   76.95  248.54  104.998 366.894 279.21  207.866 214.574 195.904\n",
      "  23.438 114.238 153.18   45.334  46.216  89.146 215.824 178.208  59.008\n",
      " 170.01   98.534 258.354 117.392 310.92   85.108  31.992  27.5   240.666\n",
      " 235.02   90.084 106.944  35.644  93.316 139.082 175.216 109.494 103.722\n",
      "  50.818 116.432 103.834  73.956 107.508 336.41  129.334 243.228 136.326\n",
      " 272.796 190.506  87.424 115.862  94.82  293.356 250.7   233.5   106.488\n",
      " 115.766 124.662  22.992 132.778 222.642  44.974  81.41  113.284 118.406\n",
      " 176.648 124.592 300.844  84.746 103.814 128.524 300.998 111.636 143.702\n",
      " 135.378  47.746 187.306  34.492  29.274  35.096 224.058 103.4   141.602\n",
      " 256.334 138.082 110.31   38.47  180.324  22.942 135.126  30.946  48.794\n",
      "  99.596  59.466 206.534  18.714 102.058 189.618  31.646 288.69  283.996\n",
      "  16.98  363.688 115.866 268.624  45.378  84.776  50.932 267.598  66.304\n",
      "  77.39  119.806 159.918 189.676]\n"
     ]
    }
   ],
   "source": [
    "# Predict with the model\n",
    "prediction1 = greedy_CV1.predict(test1)\n",
    "prediction2 = greedy_CV2.predict(test2)\n",
    "prediction3 = greedy_CV3.predict(test3)\n",
    "\n",
    "print(prediction1)\n",
    "print(prediction2)\n",
    "print(prediction3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "547ff382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results in csv files\n",
    "import numpy as np\n",
    "\n",
    "GridSearchCV_result1 = pd.read_csv('data/submission.csv')\n",
    "GridSearchCV_result1['count'] = np.round(prediction1, 2)\n",
    "\n",
    "GridSearchCV_result2 = pd.read_csv('data/submission.csv')\n",
    "GridSearchCV_result2['count'] = np.round(prediction2, 2)\n",
    "\n",
    "GridSearchCV_result3 = pd.read_csv('data/submission.csv')\n",
    "GridSearchCV_result3['count'] = np.round(prediction3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a255f490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id   count\n",
      "0   0   97.74\n",
      "1   1  214.30\n",
      "2   2   82.48\n",
      "3   4   38.18\n",
      "4   5   72.06 \n",
      "\n",
      "   id   count\n",
      "0   0   99.89\n",
      "1   1  207.40\n",
      "2   2   76.83\n",
      "3   4   32.95\n",
      "4   5   64.37 \n",
      "\n",
      "   id   count\n",
      "0   0  108.35\n",
      "1   1  207.19\n",
      "2   2   88.80\n",
      "3   4   49.33\n",
      "4   5   57.73\n"
     ]
    }
   ],
   "source": [
    "print(GridSearchCV_result1.head(), '\\n')\n",
    "print(GridSearchCV_result2.head(), '\\n')\n",
    "print(GridSearchCV_result3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48e3f6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GridSearchCV_result1.to_csv('GridSearchCV_result1.csv', index=False)\n",
    "GridSearchCV_result2.to_csv('GridSearchCV_result2.csv', index=False)\n",
    "GridSearchCV_result3.to_csv('GridSearchCV_result3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d94b1c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}