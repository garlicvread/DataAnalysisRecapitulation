{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d912416",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The concept of hyperparameter\n",
    "Hyperparameter tuning is one of the most important parts of a machine learning pipeline. A wrong choice of the hyperparameters‚Äô values may lead to wrong results and a model with poor performance.<br>\n",
    "\n",
    "Hyperparameters are model parameters whose values are set **before** training.<br> These hyperparameters might address model design questions such as:\n",
    "\n",
    "- What **degree of polynomial features** should I use for my linear model?\n",
    "- What should be the **maximum depth** allowed for my decision tree?\n",
    "- What should be the **minimum number of samples** required at a leaf node in my decision tree?\n",
    "- **How many trees** should I include in my random forest?\n",
    "- **How many neurons** should I have in my neural network layer?\n",
    "- **How many layers** should I have in my neural network?\n",
    "- What should I set my **learning rate** to for gradient descent?\n",
    "\n",
    "Let's make it simple. For example, **the number of neurons** of a feed-forward neural network is a hyperparameter, because we set it before training. Another example of hyperparameter is **the number of trees** in a random forest or the penalty intensity of a Lasso regression. As you can see, the hyperparameters are all numbers that are set before the training phase and their values affect the behavior of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb8eaee",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### IMPORTANT!\n",
    "Hyperparameters are **not** model parameters and they cannot be directly trained from the data. Model parameters are **learned** during training when we optimize a loss function using something like gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76db0ac3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "### The reason for tuning the hyperparameters\n",
    "Why should we tune the hyperparameters of a model?<br>\n",
    "\n",
    "That is because we don‚Äôt really know the models' optimal values in advance. A model with different hyperparameters is, actually, a different model so it may have a different performance. In the case of neural networks, a less number of neurons could cause underfitting and a more number of them could cause overfitting. In both cases, the models are not good, so we need to find the optimal number of neurons that cause the best performance.<br>\n",
    "\n",
    "If the model has several hyperparameters, we need to find the best combination of values of the hyperparameters searching in a multi-dimensional space. That‚Äôs why hyperparameter tuning, which is the process of finding the right values of the hyperparameters, is a very complex and time-expensive task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfeee4d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Hyperparameter tuning in practice\n",
    "Tuning hyperparameters means making decisions on the **stopping criteria**. There are several stopping criteria, but we're going to deal with four first, such as:\n",
    "1. The max_depth\n",
    "2. The minimum size of the node: min_samples_split\n",
    "3. The minimum lift: min_impurity_decrease\n",
    "4. The cost-complexity<br>\n",
    "---\n",
    "The **max depth** means the maximum number of depth in the decision tree. The tree structure cannot be deeper than this value we set using **`max_depth`**. The smaller it is, the smaller the tree will be.<br>\n",
    "\n",
    "The **minimum size of the node** is the number of data(samples) to split. The smaller the value, the larger the tree will be, and its default value is 2.<br>\n",
    "\n",
    "We can set this using **`min_samples_split`** A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The equation for min_sample_split is:<br>\n",
    "\n",
    "$$\\frac{N_t}{N} \\times (impurity - \\frac{N_{tR}}{N_t} \\times right\\;impurity - \\frac{N_{tl}}{N_t} \\times left\\;impurity)$$\n",
    "\n",
    "Where<br>\n",
    "$N$ is the total number of samples<br>\n",
    "$N_t$ is the total number of samples in current node<br>\n",
    "$N_{tL}$ is the number of samples in the left child<br>\n",
    "$N_{tR}$ is the number of samples in the right child<br>\n",
    "$N$, $N_t$, $N_{tL}$, $N_{tR}$ are all refer to the weighted sum, if `sample_weight` is passed.<br>\n",
    "\n",
    "The **minimum lift** is a criterion to see if the association rules between the items are coincidental or not. We can set the minimum lift using **`min_impurity_decrease`**.<br>\n",
    "\n",
    "When the lift is the same or smaller than the value set, the tree will not split more. The smaller the value, the larger the tree will be.<br>\n",
    "\n",
    "For pruning, we can think of two types of it. The first is **pre-pruning**, and the other is **post-pruning**. Pre-pruning is also called **early stopping**. It means literally stopping the training early. And we can do it by setting the max depth or the number of branches. Post-pruning is the process of performing pruning after we train the model. We can do post-pruning using the cost-complexity pruning technique.<br>\n",
    "\n",
    "The **cost complexity** is a concept that is used in **cost complexity pruning**. Pruning is a technique to prevent overfitting by limiting the model by setting penalty coefficients for the impurity and for the decision tree being larger.<br>\n",
    "\n",
    "In practice, we can do cost complexity pruning by finding the **$\\alpha$** value with the least influence and prune the node with that value. The equation for cost complexity pruning is:\n",
    "\n",
    "$$R_\\alpha (T) = R(T) + \\alpha |T|$$\n",
    "\n",
    "where<br>\n",
    "$R(T)$ is the learning errors of the leaf nodes<br>\n",
    "$|T|$ is the number of leaf nodes<br>\n",
    "$\\alpha$ is the complexity parameter\n",
    "\n",
    "When we focus on reducing the ¬†ùëÖ(ùëá) ¬†value only, the size of the tree gets bigger. It means the tree structure has more branches. ¬†ùõº decides the number of leaf nodes to be remained, thus we need to modify it to prevent overfitting. The bigger the ¬†ùõº ¬†value, the more nodes being pruned will be.<br>\n",
    "\n",
    "Note that we need to calculate the $R_\\alpha (T_t)$ for the sub-trees. The equation is very similar to above one.\n",
    "\n",
    "$$R_\\alpha (T_t) = R(T_t) + \\alpha |T_t|$$\n",
    "\n",
    "---\n",
    "Using the stopping criteria such as above, we can set the optimal conditions for model training, and this process is called hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfc1e54",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### GridSearch\n",
    "\n",
    "Amongst the hyperparameter tuning techniques, GridSearch, a sort of exhaustive search, shows the best performance. GridSearch is a technique that finds the best combination among the possible combinations. However, GridSearch also has cons because the training consumes a lot of time.<br>\n",
    "\n",
    "For now, we wikk implement an exhaustive search using GreadSearCV module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6962d5ac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-09-14 19:14:13--  https://bit.ly/3gLj0Q6\n",
      "Resolving bit.ly (bit.ly)... 67.199.248.11, 67.199.248.10\n",
      "Connecting to bit.ly (bit.ly)|67.199.248.11|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://drive.google.com/uc?export=download&id=1or_QN1ksv81DNog6Tu_kWcZ5jJWf5W9E [following]\n",
      "--2022-09-14 19:14:13--  https://drive.google.com/uc?export=download&id=1or_QN1ksv81DNog6Tu_kWcZ5jJWf5W9E\n",
      "Resolving drive.google.com (drive.google.com)... 172.217.25.174, 2404:6800:400a:80c::200e\n",
      "Connecting to drive.google.com (drive.google.com)|172.217.25.174|:443... connected.\n",
      "HTTP request sent, awaiting response... 303 See Other\n",
      "Location: https://doc-0c-10-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/bei7u1frn3k00uakci93li0u3phvs9il/1663150425000/17946651057176172524/*/1or_QN1ksv81DNog6Tu_kWcZ5jJWf5W9E?e=download&uuid=882961c8-a743-4246-91c0-abd21ebbb138 [following]\n",
      "Warning: wildcards not supported in HTTP.\n",
      "--2022-09-14 19:14:14--  https://doc-0c-10-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/bei7u1frn3k00uakci93li0u3phvs9il/1663150425000/17946651057176172524/*/1or_QN1ksv81DNog6Tu_kWcZ5jJWf5W9E?e=download&uuid=882961c8-a743-4246-91c0-abd21ebbb138\n",
      "Resolving doc-0c-10-docs.googleusercontent.com (doc-0c-10-docs.googleusercontent.com)... 172.217.161.193, 2404:6800:400a:813::2001\n",
      "Connecting to doc-0c-10-docs.googleusercontent.com (doc-0c-10-docs.googleusercontent.com)|172.217.161.193|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 39208 (38K) [application/zip]\n",
      "Saving to: ‚Äò3gLj0Q6‚Äô\n",
      "\n",
      "3gLj0Q6             100%[===================>]  38.29K  --.-KB/s    in 0.05s   \n",
      "\n",
      "2022-09-14 19:14:15 (794 KB/s) - ‚Äò3gLj0Q6‚Äô saved [39208/39208]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Downloading data\n",
    "!wget 'https://bit.ly/3gLj0Q6'\n",
    "\n",
    "# Unzip the downloaded data\n",
    "import zipfile\n",
    "with zipfile.ZipFile('3gLj0Q6', 'r') as existing_zip:\n",
    "    existing_zip.extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7a0e87f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Import pandas and RandomForestRegressor\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e92700d2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feca6915",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Train Data ============\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1459 entries, 0 to 1458\n",
      "Data columns (total 11 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   id                      1459 non-null   int64  \n",
      " 1   hour                    1459 non-null   int64  \n",
      " 2   hour_bef_temperature    1457 non-null   float64\n",
      " 3   hour_bef_precipitation  1457 non-null   float64\n",
      " 4   hour_bef_windspeed      1450 non-null   float64\n",
      " 5   hour_bef_humidity       1457 non-null   float64\n",
      " 6   hour_bef_visibility     1457 non-null   float64\n",
      " 7   hour_bef_ozone          1383 non-null   float64\n",
      " 8   hour_bef_pm10           1369 non-null   float64\n",
      " 9   hour_bef_pm2.5          1342 non-null   float64\n",
      " 10  count                   1459 non-null   float64\n",
      "dtypes: float64(9), int64(2)\n",
      "memory usage: 125.5 KB\n",
      "Train Data Information\n",
      " None \n",
      "\n",
      "Train Data Shape:  (1459, 11) \n",
      "\n",
      "============ Test Data ============\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 715 entries, 0 to 714\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count  Dtype  \n",
      "---  ------                  --------------  -----  \n",
      " 0   id                      715 non-null    int64  \n",
      " 1   hour                    715 non-null    int64  \n",
      " 2   hour_bef_temperature    714 non-null    float64\n",
      " 3   hour_bef_precipitation  714 non-null    float64\n",
      " 4   hour_bef_windspeed      714 non-null    float64\n",
      " 5   hour_bef_humidity       714 non-null    float64\n",
      " 6   hour_bef_visibility     714 non-null    float64\n",
      " 7   hour_bef_ozone          680 non-null    float64\n",
      " 8   hour_bef_pm10           678 non-null    float64\n",
      " 9   hour_bef_pm2.5          679 non-null    float64\n",
      "dtypes: float64(8), int64(2)\n",
      "memory usage: 56.0 KB\n",
      "Test Data Information\n",
      " None \n",
      "\n",
      "Test Data Shape:  (715, 10) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if the data loading is successful\n",
    "print('============ Train Data ============\\n')\n",
    "print('Train Data Information\\n', train.info(), '\\n')\n",
    "print('Train Data Shape: ', train.shape, '\\n')\n",
    "\n",
    "print('============ Test Data ============')\n",
    "print('Test Data Information\\n', test.info(), '\\n')\n",
    "print('Test Data Shape: ', test.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a09a9ab8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                          0\n",
      "hour                        0\n",
      "hour_bef_temperature        2\n",
      "hour_bef_precipitation      2\n",
      "hour_bef_windspeed          9\n",
      "hour_bef_humidity           2\n",
      "hour_bef_visibility         2\n",
      "hour_bef_ozone             76\n",
      "hour_bef_pm10              90\n",
      "hour_bef_pm2.5            117\n",
      "count                       0\n",
      "dtype: int64 \n",
      "\n",
      "id                         0\n",
      "hour                       0\n",
      "hour_bef_temperature       1\n",
      "hour_bef_precipitation     1\n",
      "hour_bef_windspeed         1\n",
      "hour_bef_humidity          1\n",
      "hour_bef_visibility        1\n",
      "hour_bef_ozone            35\n",
      "hour_bef_pm10             37\n",
      "hour_bef_pm2.5            36\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check if there are missing values\n",
    "print(train.isnull().sum(), '\\n')\n",
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df1595e0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Remove the missing values using linear interpolation\n",
    "train.interpolate(inplace=True)\n",
    "test.interpolate(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62d2a959",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                        0\n",
      "hour                      0\n",
      "hour_bef_temperature      0\n",
      "hour_bef_precipitation    0\n",
      "hour_bef_windspeed        0\n",
      "hour_bef_humidity         0\n",
      "hour_bef_visibility       0\n",
      "hour_bef_ozone            0\n",
      "hour_bef_pm10             0\n",
      "hour_bef_pm2.5            0\n",
      "count                     0\n",
      "dtype: int64 \n",
      "\n",
      "id                        0\n",
      "hour                      0\n",
      "hour_bef_temperature      0\n",
      "hour_bef_precipitation    0\n",
      "hour_bef_windspeed        0\n",
      "hour_bef_humidity         0\n",
      "hour_bef_visibility       0\n",
      "hour_bef_ozone            0\n",
      "hour_bef_pm10             0\n",
      "hour_bef_pm2.5            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check if the null values are replaced well.\n",
    "print(train.isnull().sum(), '\\n')\n",
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac77eb6c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare the model\n",
    "X_train = train.drop(['count'], axis=1)\n",
    "Y_train = train['count']\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestRegressor(criterion = 'squared_error')\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a206751",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02590041, 0.59557842, 0.181952  , 0.0171065 , 0.02524158,\n",
       "       0.03577957, 0.03356029, 0.03499246, 0.02998649, 0.01990228])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the feature importances\n",
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11606f48",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing the feature importances\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "importance_values = model.feature_importances_\n",
    "importances = pd.Series(importance_values, index = X_train.columns)\n",
    "importance_top10 = importances.sort_values(ascending=False)[:10]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title('Top 10 feature importances')\n",
    "sns.barplot(x = importance_top10, y = importance_top10.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77253a66",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create train datasets by removing the less important features\n",
    "X_train1 = train.drop(['count', 'hour_bef_precipitation'], axis=1)\n",
    "X_train2 = train.drop(['count', 'hour_bef_precipitation', 'hour_bef_pm2.5'], axis=1)\n",
    "X_train3 = train.drop(['count', 'hour_bef_precipitation', 'hour_bef_pm2.5', 'id'], axis=1)\n",
    "X_train4 = train.drop(['count', 'hour_bef_precipitation', 'hour_bef_pm2.5', 'id', 'hour_bef_windspeed'], axis=1)\n",
    "\n",
    "Y_train = train['count']\n",
    "\n",
    "# Create test datasets\n",
    "test1 = test.drop(['hour_bef_precipitation'], axis=1)\n",
    "test2 = test.drop(['hour_bef_precipitation', 'hour_bef_pm2.5'], axis=1)\n",
    "test3 = test.drop(['hour_bef_precipitation', 'hour_bef_pm2.5', 'id'], axis=1)\n",
    "test4 = test.drop(['hour_bef_precipitation', 'hour_bef_pm2.5', 'id', 'hour_bef_windspeed'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "253704f6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train1.shape:  (1459, 9) \n",
      "\n",
      "X_train2.shape:  (1459, 8) \n",
      "\n",
      "X_train3.shape:  (1459, 7) \n",
      "\n",
      "X_train4.shape:  (1459, 6) \n",
      "\n",
      "Y_train.shape:  (1459,) \n",
      "\n",
      "test1.shape (715, 9) \n",
      "\n",
      "test2.shape (715, 8) \n",
      "\n",
      "test3.shape (715, 7) \n",
      "\n",
      "test4.shape (715, 6) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of training and test data\n",
    "print('X_train1.shape: ', X_train1.shape, '\\n')\n",
    "print('X_train2.shape: ', X_train2.shape, '\\n')\n",
    "print('X_train3.shape: ', X_train3.shape, '\\n')\n",
    "print('X_train4.shape: ', X_train4.shape, '\\n')\n",
    "print('Y_train.shape: ', Y_train.shape, '\\n')\n",
    "print('test1.shape', test1.shape, '\\n')\n",
    "print('test2.shape', test2.shape, '\\n')\n",
    "print('test3.shape', test3.shape, '\\n')\n",
    "print('test4.shape', test4.shape, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dc3bb8e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare separate models\n",
    "model1 = RandomForestRegressor(criterion = 'squared_error')\n",
    "model2 = RandomForestRegressor(criterion = 'squared_error')\n",
    "model3 = RandomForestRegressor(criterion = 'squared_error')\n",
    "model4 = RandomForestRegressor(criterion = 'squared_error')\n",
    "\n",
    "# Train the saparated models\n",
    "model1.fit(X_train1, Y_train)\n",
    "model2.fit(X_train2, Y_train)\n",
    "model3.fit(X_train3, Y_train)\n",
    "model4.fit(X_train4, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb07143",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### RandomForest Hyperparameters\n",
    "\n",
    "**n_estimators:** Number of decision making tree\n",
    "- Default = 10\n",
    "- When increase it, the performance may get better, but may cause too much train time.<br>\n",
    "\n",
    "**min_samples_split**: The minimum number of sample used to split node\n",
    "- Used to control overfitting\n",
    "- Default = 2: The smaller the value, the greater possibility of overfitting because of the increasing node split<br>\n",
    "\n",
    "**min_samples_leaf**: The minimum number of samples to be leaf node\n",
    "- Along to min_samples_split, it is used to control the overfitting\n",
    "- When the data is imbalanced, some data of a specific class may extremely small, thus it needs to be kept the small value<br>\n",
    "\n",
    "**max_features**: Maximum number of features for optimal split\n",
    "- Default = 'auto'\n",
    "    - Note: The default value of max_feature is none in decision tree\n",
    "- When specified in int type: The number of features\n",
    "- When specified in float type: The ratio of features\n",
    "- 'sqrt' or 'auto': Samples as many as $\\sqrt{The\\;number\\;of\\;whole\\;features}$\n",
    "- log : Samples as many as $\\log_2{(The\\;number\\;of\\;whole\\;features)}$<br>\n",
    "\n",
    "**max_depth**: Maximum depth of the tree\n",
    "- Default = none\n",
    "    - Split until the class value is completely determined\n",
    "    - Or until the number of data is less than min_samples_split\n",
    "- As the depth increases, it may overfit, so proper control is required.<br>\n",
    "\n",
    "**max_leaf_nodes**: The maximum number of leaf nodes\n",
    "\n",
    "### GridSearchCV initializer\n",
    "- estimator: classifier, regressor, pipeline, and so on.\n",
    "\n",
    "- param_grid: In the dictionary type, input the parameters that are going to be used for parameter tuning.\n",
    "\n",
    "- scoring: Method to evaluate the prediction performance. Usually set to accuracy.\n",
    "\n",
    "- cv: Specifies the number of divisions in cross-validation(The number of fold).\n",
    "\n",
    "- refit: The default value is True. When it is set default, it finds the optimal hyperparameter and retrains it.\n",
    "\n",
    "- n_jobs: The default value is 1, Set -1 to use all cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01f7dd87",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "27 fits failed out of a total of 81.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "27 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 1315, in fit\n",
      "    super().fit(\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 308, in fit\n",
      "    raise ValueError(\"max_features must be in (0, n_features]\")\n",
      "ValueError: max_features must be in (0, n_features]\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.75764881 0.7577681  0.75749651 0.75403392 0.75196868 0.754678\n",
      " 0.74659628 0.74451073 0.74727318 0.75610486 0.75767967 0.75845493\n",
      " 0.75671913 0.75670383 0.7570376  0.74802081 0.74914767 0.74848187\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
      "27 fits failed out of a total of 81.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "27 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/joblib/parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/utils/fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/ensemble/_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 1315, in fit\n",
      "    super().fit(\n",
      "  File \"/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/tree/_classes.py\", line 308, in fit\n",
      "    raise ValueError(\"max_features must be in (0, n_features]\")\n",
      "ValueError: max_features must be in (0, n_features]\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/home/raymond/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.76052652 0.76293127 0.76273734 0.75917558 0.76053316 0.75974605\n",
      " 0.75176627 0.75184462 0.7503528  0.75953548 0.76337371 0.76265557\n",
      " 0.76224548 0.76055322 0.75938853 0.75227291 0.75357278 0.75245477\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing time:  28.198873043060303 seconds.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "import time\n",
    "\n",
    "model = RandomForestRegressor(criterion = 'mse',\n",
    "                              random_state=2022)\n",
    "\n",
    "params = {'n_estimators': [200, 300, 500],\n",
    "          'max_features': [5, 6, 8],\n",
    "          'min_samples_leaf': [1, 3, 5]}\n",
    "\n",
    "# Declare GridSearchCV for each model \n",
    "greedy_CV1 = GridSearchCV(model1,\n",
    "                          param_grid=params,\n",
    "                          cv = 3,\n",
    "                          n_jobs = -1)\n",
    "\n",
    "greedy_CV2 = GridSearchCV(model2,\n",
    "                          param_grid=params,\n",
    "                          cv = 3,\n",
    "                          n_jobs = -1)\n",
    "\n",
    "greedy_CV3 = GridSearchCV(model3,\n",
    "                          param_grid=params,\n",
    "                          cv = 3,\n",
    "                          n_jobs = -1)\n",
    "\n",
    "greedy_CV4 = GridSearchCV(model4,\n",
    "                          param_grid=params,\n",
    "                          cv = 3,\n",
    "                          n_jobs = -1)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train for each dataset\n",
    "greedy_CV1.fit(X_train1, Y_train)\n",
    "greedy_CV2.fit(X_train2, Y_train)\n",
    "greedy_CV3.fit(X_train3, Y_train)\n",
    "greedy_CV4.fit(X_train4, Y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Processing time: \", end_time-start_time, 'seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f62ebe21",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100.4040904  225.86973737  88.20780313  38.51110324  51.47285488\n",
      " 128.60173647 175.79844283 309.2731003   40.25818784 123.18983297\n",
      " 304.65404365 250.22369661 103.07202525  41.26059361 215.52227704\n",
      " 155.18364035  26.02031097 196.53097131 343.76620905 152.50668597\n",
      " 229.5318465   84.75360335  25.99849387 140.4740661  139.11927435\n",
      " 113.20358351  26.23883929 121.03391468 113.86465404 160.79786111\n",
      "  77.86191991  35.36272872  64.82954246 128.79994759 273.25326479\n",
      "  36.73068586 132.90700404 164.85310194 210.98526804  80.73024317\n",
      "  58.14644787 121.35473864 175.81336697  91.01392442 328.25512483\n",
      " 197.01642094  88.46927889  63.67951324  19.08376623  87.51686851\n",
      " 250.45127976  94.7045864  151.89714216 122.9426738  202.84467777\n",
      " 155.15358197  49.41272107 176.86019277  25.27518362  18.82674152\n",
      "  96.2857576   85.88617821 259.46370597 306.8340193  153.21916343\n",
      " 311.14462302  25.5605395  216.05400307 150.83580364  32.33388149\n",
      " 101.84844841  31.57371968 158.29063753  15.4133824  323.99048503\n",
      " 230.01614991  35.33758351 189.18942516 242.36082415  27.19890152\n",
      " 257.5907491  135.28975233  91.60661778  84.17492514  92.69356223\n",
      " 316.45319444  57.54684855 160.48653538  97.57876876 274.29779924\n",
      " 293.00566919 159.90217867  66.09185839 102.90383748  42.65548712\n",
      "  84.01405105  98.96627546  28.73549802 219.73556296 146.62300337\n",
      "  19.1655101  167.50097942  42.20431654 117.58633049  73.12139397\n",
      "  54.36438031 102.21316811  25.8608961  168.97035137 141.58259946\n",
      " 188.06888546 252.23756711 177.96068346 116.58120328  64.62151487\n",
      " 121.63513023 240.89389069  42.80699557 215.41558145  19.42391811\n",
      " 104.33801732 108.72693078 199.99920742 112.32580006  49.26642742\n",
      " 121.0266369   57.65659939  16.87055952 220.42473661  59.01630103\n",
      " 128.75884453 148.28596878  19.66501281 135.62641486 297.20676786\n",
      " 126.86766848  44.33075012 146.11983261 251.42371122 357.9079531\n",
      " 175.72779133  34.23244192  33.06934722  88.56281133  97.61597312\n",
      " 116.4878225   81.17973477 122.99573539 153.86991053 195.26311996\n",
      " 129.60615604 296.86264702 318.04951389 219.29736634 174.52122763\n",
      "  14.9700267  119.83640314  23.96119968  89.18154257 140.92150362\n",
      " 137.74848232  26.7640597  204.7165864  211.85982882 123.94521916\n",
      "  23.31734452 233.72848159 158.89931259 289.33218957  24.93417803\n",
      " 117.75085971 135.03052491  88.02679076 126.9112675   98.81892082\n",
      "  96.79571392 227.47657738 203.89438296 165.25805665 107.77354637\n",
      " 209.18688772 178.16118798 156.48333802 246.33171746 142.18990115\n",
      "  89.46220058  95.74150343 301.8606176  160.14194184  48.48076852\n",
      "  38.2736212  133.36277758 117.91349517 105.65339989 133.3177031\n",
      " 266.37849946 332.27468525 141.41254095 120.85958712 106.86860732\n",
      "  25.30661706 238.01914356  24.26944318 329.41205898 141.36171663\n",
      "  40.36895166 124.29706609 244.69551714 226.01289177  65.06416332\n",
      " 287.08563421 160.54960573 132.88651587 210.65659507 248.60855933\n",
      " 126.45606566 238.18890836 114.21112558 133.87561833  90.30275938\n",
      "  96.46513729  32.67597908 104.91594426  62.65344218 144.18973658\n",
      "  41.05192514 202.96125301 191.08192825  34.29618218 165.94795205\n",
      " 263.75951425 117.44587206  25.46209993  77.09836024 201.61569081\n",
      " 100.83000234 358.11608802 323.67561094 161.99537085 210.85497999\n",
      "  19.69456134 218.58176984  32.58324585 135.6101333  149.80570256\n",
      "  98.77586418 143.00107172 233.20082082 100.72378015  24.93929347\n",
      "  92.75462897 264.33484091 230.94677388 163.4547944  133.25107665\n",
      " 170.75192066  35.33431295 344.97978373  84.51623936  19.66774224\n",
      " 171.66840712  35.09454311 224.31038772  32.95250289 158.71251428\n",
      " 233.66295292  25.95940296 231.08873088  84.83324152 197.01180872\n",
      " 129.49238185 110.86074242  56.13635803 319.91932431  27.17703517\n",
      " 275.39424928  85.26642749 172.29234126 174.30436655  38.28444661\n",
      " 288.13628734 101.89460678  16.66019913 172.36466576 159.58076356\n",
      " 112.59443601 251.54704437 134.47600703  69.95131457  92.47534235\n",
      " 119.65146861 264.84752615 153.41234292 119.6401068  133.95840296\n",
      "  37.48981999 146.66260823  44.18273195 231.5589006   25.05388366\n",
      " 106.94193236 220.0536809  213.39208387 217.60839882 346.98306169\n",
      " 212.60035868 129.90767496  61.87703137 165.45141821  91.01463654\n",
      " 271.94650433 251.60917947 247.98237266  42.57956277 262.82524964\n",
      " 210.4902175  148.91625234  41.73354007  31.91000703  19.63153084\n",
      " 220.88487084  34.06467183 137.43902814 119.65220878  93.54864628\n",
      " 353.68221519 271.54864646 100.07933261  88.53680213  93.65637734\n",
      "  27.82527645  97.29935555 105.7457674  185.05818975 140.20238835\n",
      "  36.52407233 243.34262852 229.20850938 146.55935408 350.39014177\n",
      "  20.24967623  28.53585372 310.92644553 188.27318691 118.97686075\n",
      "  23.45518788  30.18831746  99.77655231 115.8821737  294.286079\n",
      " 218.17074223 238.0399026   66.69674224  51.33518117 200.22395276\n",
      "  64.20280302  20.55932179  27.13566216  98.1188474  109.34689214\n",
      "  63.91707286 201.2130168  244.36449567 164.48232577 146.17117316\n",
      "  36.81103048  76.54546645 101.83799693  24.52818001 188.23809959\n",
      "  96.17460317  76.26610967 231.18274167 304.34172908 218.0511057\n",
      "  17.90138564  38.23874566 176.1620619  100.44336024  43.22063402\n",
      " 133.07659796 114.88273629 313.80475108 288.3262601  116.30678499\n",
      "  81.88338801 134.07596104 171.97529973 116.42882913 232.27546952\n",
      "  42.15749929 154.60949116 293.34343344  56.97523395 113.73536656\n",
      " 112.09940837 275.73307251  36.20161562  30.32162843 203.56943705\n",
      "  20.42064935  89.83160552  24.46906674 259.24358189  81.59849949\n",
      " 121.13235245 176.89763292 160.34862034 114.19836183 112.71542857\n",
      " 164.69550289 277.24900216 135.76424152 122.34087968  49.21434552\n",
      " 122.68039291 180.28575271 116.60070978  57.53953462  55.29296247\n",
      "  28.74774278  34.3975202   19.45230213  34.65701948 162.67702438\n",
      " 111.72208536  98.51372024 181.24022745 113.12779532 339.36175506\n",
      " 135.20637608 188.79848915 247.73899549 113.27152904 115.29550203\n",
      " 224.23066305 320.09677237  39.13766658 115.47126948 343.67803281\n",
      " 262.07917947 153.6383512   44.2147803   96.84839502 111.27950108\n",
      "  89.26239702 109.42166396  37.48884506 319.59265765 290.99149964\n",
      " 235.36934919 121.07735913 173.77904602 217.96694551 148.88140491\n",
      " 128.27838636 247.02869444 100.35048914  61.78937987 119.99072601\n",
      " 193.82175919 130.48933351 113.98487829 112.60789015  91.96696429\n",
      " 141.27208027 270.18577435  96.77652305 180.15323954 180.50530339\n",
      " 236.31680206  58.0320348  321.47237933  87.59002503 158.33584235\n",
      " 176.01888016  24.74821338  27.35817226 220.21894154 276.84027561\n",
      " 100.12716675 101.38971971  34.47722475  25.86813348 235.14549206\n",
      " 143.41846051 324.84303932 232.47198359  91.59206097 127.01748232\n",
      " 217.09787286 173.41796699  69.70626209  77.72746374  47.68100008\n",
      " 158.43274444 105.54133702 225.57988826 288.30590007 226.98240657\n",
      "  16.23432504 219.10271483  58.23993613  97.68032506  42.17213203\n",
      " 238.1278667  242.77927381  29.98637428 153.71091486  81.57009794\n",
      " 222.24457033 206.24839699 213.10530071 197.98029436 341.15601479\n",
      "  94.71550776  31.19450541 235.18696735 149.84301987 173.83540673\n",
      "  34.62644841 226.92838233  18.79834921  57.48591467  26.62971555\n",
      "  81.47420184  33.11139953 189.85983188 202.16351046 150.03622547\n",
      "  39.10328626  87.74617211  36.30982538 306.50924351  92.50932449\n",
      "  38.34144282  98.97548214  24.23064015  42.44963449  34.64284578\n",
      " 309.58632504  16.22820808 181.09236734 242.47313817 310.06862429\n",
      " 215.65684812 101.2412224  145.26686877 104.08187356  99.61949606\n",
      " 216.23558694  41.45793488  20.03836923 145.14103265  19.16943831\n",
      " 156.95171395 143.30716558  42.15107991 167.54192193 197.19290115\n",
      " 206.2285763  328.99363471  28.03832269  97.80542172  41.3755019\n",
      "  41.95189087  99.34264196  98.77025508  35.61603535 165.84731817\n",
      "  89.30721338 201.41607863 124.00619895 332.75805485 175.48032624\n",
      " 272.95043903 136.92010012 284.96521356  62.52262139  32.19531981\n",
      "  27.38870201  36.7710294   85.91109668  23.74598936  24.49279636\n",
      "  97.86232867  25.52159163 263.64898864 227.23561253 147.08100884\n",
      "  87.40048954  95.61739177  34.85402381 149.58831388 100.45487825\n",
      " 129.01362009 182.91037825  24.44520581 226.14800738 119.58012933\n",
      "  13.74895294 250.28116703 104.00311562  43.21509488  84.57877255\n",
      " 239.8307307  130.68188574 346.65026082 284.63521753 220.41986232\n",
      " 216.59555501 177.83666649  20.43684181 114.01660209 129.06971663\n",
      "  28.74102633  47.08234047  99.29378058 213.09134811 159.2872742\n",
      "  58.19028571 174.91029873  97.8331645  249.22745996 115.16561692\n",
      " 308.62876136  65.35820976  27.41014845  23.07750433 220.20685606\n",
      " 242.75483621  88.6158373  116.38316685  28.0956096   97.36373431\n",
      " 128.82249152 182.04399387 115.70458063  99.35624946  42.14000667\n",
      " 114.74549788 104.02586815  66.74469913 108.50756241 305.53585732\n",
      " 130.6889923  260.64907536 127.26511454 294.65226479 192.8837265\n",
      "  75.49096936 119.57351641  98.22419553 299.77606494 250.12056006\n",
      " 253.13594192 107.18472222 118.42623413 127.86300452  19.92986382\n",
      " 139.41307272 241.02310029  35.1928539   83.33776006 116.3195597\n",
      " 116.47326533 164.7691887  125.37789363 291.89349115  86.09008965\n",
      " 104.56320473 135.91632377 294.49584127  88.70569156 147.71578141\n",
      " 141.22413548  36.72609657 164.27680504  25.74826147  27.96740368\n",
      "  25.83734867 221.2616506  103.5313667  159.54868019 246.676182\n",
      " 135.26482638 124.53688711  31.73705267 152.53755324  17.31285274\n",
      " 141.92024442  29.74308478  40.72837121  95.87703369  54.75004454\n",
      " 209.77647168  18.42693254  88.22003193 190.76093579  29.8544466\n",
      " 301.24385011 247.51738059  16.05669571 349.91012608 116.53225613\n",
      " 237.85781385  43.10060496  86.49988871  38.44308604 272.33360047\n",
      "  62.39291683  67.63698828 117.48792857 159.07600523 160.76835373]\n",
      "[101.61053437 223.96263463  98.66828864  31.96748762  50.4156194\n",
      " 121.30173487 168.97459196 321.21617714  33.92051571 118.54080472\n",
      " 305.00430254 251.46346176 108.91308196  36.86632583 214.34449776\n",
      " 153.17510361  25.8157842  197.45624955 359.39144977 156.64469584\n",
      " 219.5550969   81.55046084  15.64490343 144.44693405 146.50662713\n",
      " 112.7441956   27.36447504 115.85597594 116.01091484 150.29432471\n",
      "  76.21455705  29.96792785  59.87817101 132.35952239 263.01199929\n",
      "  30.48748958 132.0693601  178.86471585 220.12960043  71.27107133\n",
      "  54.63748229 118.39105673 169.39362637  80.40110659 332.97323444\n",
      " 200.00262289  79.76051921  59.43137883  17.76792842  88.96490263\n",
      " 236.25702172  94.01578645 143.31476704 117.59422244 201.82768836\n",
      " 153.70723352  43.69702298 179.35998058  15.07858733  18.84109004\n",
      "  99.27241178  86.98588786 266.45293938 315.22243606 155.25681609\n",
      " 314.09402929  14.75609984 211.50533333 154.62876304  25.71778202\n",
      " 102.29957655  32.19084892 155.00545408  15.00264316 319.92893105\n",
      " 231.20509815  36.59307036 182.53620202 234.20229371  17.96703651\n",
      " 254.90837973 135.7817438   93.25772693  83.35308833  92.27374317\n",
      " 319.24936974  49.09006929 163.01701513  97.56423328 288.18952316\n",
      " 297.98361942 156.92474359  61.21484639 102.76475209  35.85642913\n",
      "  88.92512718  99.67439519  28.46451478 213.30541658 152.09335446\n",
      "  18.47377691 154.03635438  43.38761497 116.54113777  62.61376969\n",
      "  50.27824963 101.60115758  16.46849602 178.15185641 142.26395437\n",
      " 178.24042554 236.87436898 173.98245647 114.75290543  61.84593117\n",
      " 122.66669775 250.25138066  34.82425007 215.42919795  17.27762763\n",
      " 103.43475101 108.02581951 204.93416001 109.86638095  43.84800811\n",
      " 117.90518095  54.92666296  16.58977828 216.45687944  53.09124196\n",
      " 126.72493947 153.86367171  19.27750128 135.15568873 312.73761893\n",
      " 132.6608974   43.15303827 145.87755361 264.13756733 365.62151127\n",
      " 179.41330133  28.45702625  33.95530977  93.52475592  97.75508175\n",
      " 118.95689502  83.93418598 121.81087    148.29366894 202.57336739\n",
      " 130.75802056 291.89172534 332.50144488 219.01536017 172.14266121\n",
      "  14.2073653  116.8385697   14.55852726  90.49266841 139.51066551\n",
      " 132.25462827  16.78113835 206.90414003 212.92449444 122.32841222\n",
      "  15.11827433 256.82683132 152.29879368 295.99184105  16.74387928\n",
      " 122.25111634 126.39177777  86.94478559 126.33902687 100.21056291\n",
      "  99.43587424 241.74215092 196.57011633 154.64030517 113.25538456\n",
      " 217.89665603 180.05829168 158.45840714 256.21748781 138.05568384\n",
      "  90.95930281  98.32052007 305.06991769 160.34831316  44.190197\n",
      "  35.00641291 133.85190258 121.7904483  109.20130142 132.85801542\n",
      " 268.92630339 341.46779905 131.32804567 118.32142335 103.50167194\n",
      "  16.769305   254.67186098  15.48403893 339.53791329 141.43443669\n",
      "  33.60055678 126.90625094 255.90098799 222.99607178  60.94446378\n",
      " 285.20945179 156.58897764 130.03447797 217.13235087 252.17322788\n",
      " 125.7962323  238.31779529 120.14424347 134.48830934  94.86723241\n",
      "  92.88650125  26.34927251 105.84822684  63.85085915 144.13293067\n",
      "  34.67343802 199.09234169 199.94631184  36.32141448 175.68889671\n",
      " 275.87712055 118.68318153  18.50144281  75.54128556 205.63531565\n",
      " 102.02012345 366.49567663 327.65907575 158.32015921 213.61419331\n",
      "  18.65764167 217.00069106  25.65785425 135.40298087 142.83316788\n",
      "  99.73612244 147.3651111  233.78248326 102.65665919  15.0342123\n",
      "  92.59210197 261.63000627 232.00636313 164.43545333 137.49718381\n",
      " 182.43333935  27.98311326 350.61230584  86.2218303   18.66005592\n",
      " 163.96019996  27.37556436 226.78632922  26.0472399  149.91043855\n",
      " 226.84557482  16.91127942 232.15960433  87.46737713 189.07266429\n",
      " 130.38110584 107.41721064  48.77435998 325.08929535  18.26916988\n",
      " 280.46828837  80.80647639 175.82751186 187.70455467  32.43983492\n",
      " 293.37882935 103.69355512  15.76461483 168.26370354 161.60075703\n",
      " 115.23212384 250.1244588  133.71363887  67.66732835 102.36234252\n",
      " 119.0657678  264.57701854 144.917183   117.86448062 133.33108201\n",
      "  30.54445654 144.02465662  40.10145039 230.53512771  23.39950592\n",
      " 103.30744359 226.06200016 214.26905101 218.65937785 357.97778616\n",
      " 214.55693162 131.88809668  56.81493468 167.53256798  93.92899798\n",
      " 276.4782541  253.01468124 245.81459683  37.67676931 250.93936118\n",
      " 202.53225473 152.84923616  35.53258142  29.01826918  19.09733521\n",
      " 220.61566992  31.7965868  141.49505916 114.10823329  93.13973732\n",
      " 362.83091285 284.43156534 100.08279236  84.7871753   94.50756031\n",
      "  26.71036421  91.26359144 105.3543096  183.95192227 141.69685415\n",
      "  36.9883763  237.42593802 244.15157795 141.52481697 360.63801803\n",
      "  19.25983792  20.3631013  319.12114054 184.88873871 116.8903936\n",
      "  15.79551176  29.29589497  98.07086667 119.29118938 302.37624551\n",
      " 224.35453404 239.36708367  61.37345396  44.84597395 195.77081696\n",
      "  59.03196437  18.96071559  18.79499463 102.61212655 108.90318074\n",
      "  58.37634589 200.58726441 244.44781853 158.51437095 146.93928813\n",
      "  28.71336363  68.7934849  100.94887518  14.85273085 193.71898355\n",
      "  98.13704029  76.35126658 231.83465216 311.34666967 220.01709192\n",
      "  17.84375988  33.585829   170.1863686  103.16326553  36.82508686\n",
      " 124.64900824 111.90279843 317.16309164 296.4456756  117.25200883\n",
      "  74.91731248 130.16590034 169.61436817 119.72600083 225.29777403\n",
      "  35.0013983  160.28057666 303.71121874  48.33850822 116.41039989\n",
      " 107.17980593 278.25380424  29.82611464  30.46930511 210.88621301\n",
      "  21.26994524  90.38588391  15.28931207 244.86379805  75.13407443\n",
      " 117.47392246 189.99584063 149.84601754 114.79896748 115.00835003\n",
      " 176.31374286 286.46563362 135.68308035 121.65655382  43.9107421\n",
      " 125.35957468 173.57104276 112.23569751  49.34376805  50.17951675\n",
      "  18.97886154  27.85963413  17.56295476  34.90981376 161.27538247\n",
      " 114.43407572  99.89993636 184.99436652 116.08448849 346.66473868\n",
      " 129.84535867 185.8882411  266.25992002 114.97365408 114.27735407\n",
      " 232.34951956 328.3915839   32.10963165 111.29759735 348.79319193\n",
      " 249.61020586 149.32453594  39.73423656  96.73767281 119.23801429\n",
      "  90.85601871 107.83245354  29.89227179 322.44662843 298.2124939\n",
      " 235.45365332 120.31767734 170.22743367 225.52449942 137.74647332\n",
      " 120.92671688 237.56993384 100.36944749  53.2121099  116.85530269\n",
      " 206.32421759 128.30920719 114.74904368 115.45108018  92.84413241\n",
      " 143.96189286 282.53294751  95.84143508 176.51214497 175.00888374\n",
      " 231.99417345  47.97675401 318.51023218  85.82258347 156.47994212\n",
      " 177.57753751  15.18662867  27.23997479 218.84529643 283.72948907\n",
      " 105.93824957  98.60288581  28.70396941  14.96002628 228.23878319\n",
      " 145.78665449 332.48172554 248.69941844  91.73229341 125.88841336\n",
      " 215.80344004 167.85961729  63.92014548  68.89809255  41.83289442\n",
      " 160.72535845 117.40781262 228.77319732 295.17090869 225.21844844\n",
      "  15.49314549 219.2715371   48.00005285  96.05993595  33.4796735\n",
      " 231.7796014  238.38118776  30.03006464 154.1206822   76.41560671\n",
      " 238.56442793 220.64489905 214.45333745 206.36456039 358.08876305\n",
      "  96.74490144  31.35596701 236.54170332 144.63920896 184.74981689\n",
      "  28.36123766 232.25353846  18.36543211  46.87491887  17.54895757\n",
      "  91.09888125  31.06425886 197.79582156 186.08325901 150.02071361\n",
      "  32.76162229  81.26740005  30.52163304 313.41671616  95.02782332\n",
      "  31.89854736 100.53917027  15.57704574  41.92797776  32.43599704\n",
      " 316.27711911  15.45957237 183.08119792 242.46657821 317.41099148\n",
      " 220.66299935 102.25088345 135.34005373 105.25811407  99.03933906\n",
      " 214.04669999  33.76571912  18.88984463 147.15369204  18.65439921\n",
      " 143.98540701 142.50851367  36.16192748 164.30013742 193.65079412\n",
      " 208.44862898 338.24723802  28.92886941 100.37877085  37.0205647\n",
      "  36.52303851 102.0849127   96.58068667  29.20870836 163.88151753\n",
      "  90.67522135 181.28505079 120.52079568 341.79756884 171.49160004\n",
      " 283.20761658 130.16823045 286.765095    65.17716448  31.67241116\n",
      "  26.99336903  34.92423369  88.44787532  14.89970516  15.55855844\n",
      " 101.3701304   17.35866117 277.57596495 226.90484495 147.99838959\n",
      "  87.99906103  96.22250772  31.45455093 147.80364156  97.78807671\n",
      " 131.07395729 180.84163876  14.76942039 230.05861421 123.3820163\n",
      "  14.03578932 254.72842149 106.42281876  38.37732641  83.56159071\n",
      " 238.60187323 125.25318687 368.47795001 283.24484574 220.20661232\n",
      " 213.12020373 178.65629196  18.70293709 114.88742334 117.35218261\n",
      "  17.80482958  46.37323043 102.46029204 214.04225076 156.65449965\n",
      "  49.50301469 175.7998596   97.97044814 245.09357446 115.54215227\n",
      " 317.38007688  58.70455818  28.9376103   18.1678026  215.26357815\n",
      " 243.84935426  88.09426672 111.35390224  27.42883073  95.93120759\n",
      " 119.20146775 180.44146787 114.85781524 100.15112345  36.06256038\n",
      " 116.90872804 104.97694134  64.43003431 109.79589928 313.78342425\n",
      " 129.44181927 265.21235078 117.20805873 303.46055776 197.29756439\n",
      "  65.56255027 116.73343586  97.14289309 307.06615675 242.48605635\n",
      " 259.87874215 106.12389848 114.24422    126.69400058  17.73235949\n",
      " 136.55582468 264.21230851  27.07459618  83.02051086 113.27484404\n",
      " 118.15059928 162.97653684 127.71157288 304.2785245   87.62988429\n",
      " 103.59610675 135.43437797 300.68160548  92.90891448 140.49630982\n",
      " 143.21689517  29.61341093 158.05007187  16.28130965  28.14016978\n",
      "  16.08295644 226.88677792 106.09057749 157.66102145 256.70451066\n",
      " 140.48652317 122.6041137   26.95575447 155.36908435  15.99859508\n",
      " 142.01069597  29.86033044  33.72452545  98.69164488  47.01983128\n",
      " 211.57153654  19.73154937  82.90521919 172.14454669  30.14106531\n",
      " 309.46182032 251.81701926  14.49448626 362.26180766 120.20510586\n",
      " 246.66789401  38.35540873  85.28739785  31.43909696 275.16800828\n",
      "  52.65817262  65.23535047 122.78302868 153.10395355 158.38695622]\n",
      "[102.804 216.752  88.914  39.846  45.654 123.504 170.608 310.836  44.936\n",
      " 121.848 292.012 250.724  99.496  43.2   201.988 152.564  26.038 178.086\n",
      " 354.472 154.402 220.89   84.378  27.688 140.516 140.032 114.546  26.502\n",
      " 118.146 110.064 160.318  79.694  37.154  61.51  131.704 266.92   39.604\n",
      " 133.216 165.088 243.586  88.704  59.892 122.07  150.236  88.884 333.314\n",
      " 205.958  93.87   60.34   20.99   88.516 229.526  90.34  168.794 122.994\n",
      " 193.388 129.348  52.198 177.082  25.666  19.422  94.184  85.712 259.47\n",
      " 307.84  151.552 311.264  29.862 226.57  132.638  33.33  104.364  32.806\n",
      " 122.236  16.234 318.71  225.648  37.174 174.456 225.97   29.402 247.982\n",
      " 138.106  81.494  84.432  92.758 320.794  48.336 170.358 110.028 268.484\n",
      " 285.494 159.702  65.852 101.364  44.38   83.83  105.292  29.468 227.094\n",
      " 150.2    18.446 143.524  39.602 121.056  83.444  47.818 100.548  27.538\n",
      " 177.87  129.82  178.71  230.932 168.652 117.35   60.508 125.684 230.708\n",
      "  40.626 196.708  18.684 102.646 107.444 190.378 115.102  50.886 118.696\n",
      "  55.764  15.118 229.696  61.616 132.112 159.004  21.282 143.554 293.668\n",
      " 137.002  44.504 145.262 248.744 359.482 203.954  36.002  35.458 106.876\n",
      "  90.042 113.602  87.182 122.69  153.4   227.118 129.734 292.142 320.748\n",
      " 234.758 152.05   11.836 115.958  22.57   96.16  142.484 144.696  30.862\n",
      " 197.612 229.544 124.194  24.788 226.988 152.442 282.162  25.846 122.644\n",
      " 125.214  90.196 128.112  99.808  99.252 226.338 204.61  161.954 109.424\n",
      " 207.544 171.934 158.252 254.658 144.428 105.224 112.586 295.704 172.544\n",
      "  48.83   43.234 139.418 121.862 109.676 128.984 263.3   331.982 143.436\n",
      " 120.74  104.472  26.782 224.852  26.372 340.398 144.622  42.068 129.382\n",
      " 238.526 219.234  61.402 286.834 178.496 133.294 215.42  246.49  129.184\n",
      " 238.726 119.702 135.396  95.366  99.938  32.52  108.326  63.488 148.19\n",
      "  41.496 205.152 177.594  37.432 200.572 262.258 110.37   28.868  82.954\n",
      " 197.2   101.77  372.248 318.04  156.368 198.736  20.674 216.384  32.308\n",
      " 135.224 142.8    98.356 126.67  228.056 102.73   24.246  85.6   255.814\n",
      " 235.238 192.75  146.89  203.586  39.832 336.474  79.162  18.544 160.984\n",
      "  37.658 227.222  32.324 173.918 232.958  26.868 232.302  89.68  193.124\n",
      " 136.682 108.912  54.084 324.248  31.908 278.048  86.388 163.958 209.652\n",
      "  37.826 282.032 104.908  16.418 162.586 196.914 117.452 251.608 136.874\n",
      "  72.024 102.084 120.44  257.258 177.898 115.546 135.636  39.98  143.974\n",
      "  48.434 223.58   21.816 103.89  207.744 212.364 225.538 354.914 200.912\n",
      " 133.284  62.694 160.728  88.802 272.422 257.418 241.902  50.066 255.19\n",
      " 199.63  124.388  41.948  30.448  18.808 222.382  29.988 125.618 117.13\n",
      "  89.572 359.554 265.56  101.052  84.312  97.144  25.896 101.168 107.374\n",
      " 175.684 148.934  40.968 238.632 225.356 153.212 352.86   21.43   32.026\n",
      " 322.622 179.574 123.086  25.26   29.954  99.058 120.93  294.134 209.824\n",
      " 235.93   72.536  48.578 193.124  64.896  20.612  30.676 102.114 113.99\n",
      "  63.372 216.982 234.144 195.936 159.228  40.424  75.234 103.708  24.532\n",
      " 196.98   97.474  74.228 239.134 302.406 223.546  17.836  42.59  180.056\n",
      " 109.072  46.728 139.132 114.758 308.504 285.642 116.794  92.042 134.652\n",
      " 161.996 119.612 235.216  41.976 140.282 288.634  54.304 119.866 113.76\n",
      " 269.278  40.726  27.266 216.422  19.31   88.67   25.222 245.698  88.102\n",
      " 122.046 166.494 180.416 117.806 116.052 163.15  277.168 139.672 119.492\n",
      "  52.242 125.684 174.958 118.606  58.892  60.366  31.482  34.17   17.856\n",
      "  37.648 164.722 113.74  102.528 188.736 115.496 341.994 133.198 186.6\n",
      " 243.244 113.918 114.102 215.9   331.734  42.202 116.79  353.304 251.336\n",
      " 150.438  49.69   97.692 114.964 109.428 109.83   36.074 329.44  286.078\n",
      " 235.052 117.944 169.578 218.766 136.152 127.02  239.092 103.488  62.404\n",
      " 121.134 187.984 127.62  115.338 117.23   88.368 133.992 268.196  98.272\n",
      " 178.32  184.252 234.218  53.58  324.044  88.144 156.552 170.966  23.826\n",
      "  26.9   223.114 273.772 104.286 109.414  35.244  28.764 247.66  144.356\n",
      " 327.028 238.85   84.232 112.256 230.126 165.902  74.628  73.436  54.974\n",
      " 187.86  112.642 230.928 294.468 226.634  16.218 216.942  56.084  84.434\n",
      "  45.624 246.528 238.924  27.806 152.634  74.47  213.332 197.278 207.52\n",
      " 199.906 350.168  99.606  33.472 251.566 144.948 165.06   33.614 230.516\n",
      "  18.298  53.89   29.942  87.37   32.452 186.96  196.354 149.662  45.36\n",
      "  91.232  39.154 296.138  95.084  40.638  99.744  25.606  43.364  32.226\n",
      " 297.744  18.086 188.73  245.576 310.42  218.846 105.034 116.834 107.41\n",
      " 105.458 207.448  43.116  19.75  143.856  17.526 179.304 142.026  50.152\n",
      " 147.898 206.38  217.37  329.38   26.882  99.488  38.692  44.618 100.536\n",
      " 105.592  34.262 171.15   87.17  194.322 124.532 343.954 175.668 271.788\n",
      " 126.704 278.788  67.92   31.516  27.73   38.842  90.848  25.866  23.738\n",
      " 102.966  27.952 259.686 223.514 152.722  88.73  103.062  35.1   146.116\n",
      "  97.05  126.89  191.238  24.3   233.946 125.728  13.936 273.234 105.798\n",
      "  39.22   77.488 251.646 128.682 376.452 280.294 224.054 219.098 185.314\n",
      "  20.82  116.022 127.744  30.73   45.318  99.03  213.044 192.492  56.732\n",
      " 176.122  98.96  249.264 117.814 318.97   69.034  30.954  24.04  228.806\n",
      " 240.178  89.174 109.242  31.67   97.246 120.552 173.316 115.85  105.962\n",
      "  46.196 118.926 104.052  71.814 108.178 325.656 128.052 264.596 121.5\n",
      " 292.578 191.33   82.646 119.3    96.842 307.572 245.83  255.076 107.558\n",
      " 118.856 126.302  18.108 123.904 242.682  37.19   86.196 116.856 116.026\n",
      " 190.678 126.832 304.826  90.804 100.856 131.808 302.764  91.896 155.794\n",
      " 138.252  40.068 195.13   27.674  30.88   27.912 229.046 106.692 143.356\n",
      " 249.6   137.874 114.76   32.964 196.366  18.5   140.274  31.126  44.474\n",
      "  95.098  52.33  190.234  19.662  88.71  183.37   30.328 305.976 271.264\n",
      "  16.118 370.196 116.798 263.426  38.922  85.322  39.49  273.266  57.766\n",
      "  76.346 119.374 157.942 202.896]\n",
      "[106.83       213.48666667  98.30333333  30.77666667  45.63333333\n",
      " 124.39666667 171.48666667 324.86666667  37.4        124.13666667\n",
      " 300.43333333 249.40666667  79.37333333  35.73       213.58333333\n",
      " 147.18666667  26.99333333 202.71666667 369.06333333 147.66333333\n",
      " 225.87666667  86.52666667  15.78       140.47666667 147.10666667\n",
      " 113.55333333  33.25       116.90666667 112.45666667 145.52333333\n",
      "  74.02        29.07666667  60.09666667 132.14333333 265.88333333\n",
      "  36.63666667 130.06666667 172.27333333 248.25        80.82333333\n",
      "  53.35       118.43333333 147.81666667  85.48666667 332.82\n",
      " 214.57666667  92.62666667  56.94        17.03        84.7\n",
      " 220.18333333  92.76333333 176.57       124.09666667 190.54\n",
      " 132.09333333  45.03333333 166.84666667  12.86333333  20.39\n",
      "  85.97666667  83.27333333 263.29666667 314.92       154.29333333\n",
      " 319.85666667  15.22666667 227.66       146.13666667  26.10333333\n",
      "  99.95666667  30.90333333 131.42666667  15.62666667 318.57\n",
      " 223.88666667  35.08       184.93666667 231.30666667  17.83666667\n",
      " 246.26666667 132.66333333  79.64        86.1         85.79666667\n",
      " 325.36666667  43.97666667 169.60333333 115.72       290.28\n",
      " 296.65       150.98        64.17333333  99.38        33.44333333\n",
      "  84.69333333  89.73333333  27.94       216.91666667 146.74\n",
      "  17.90333333 145.47666667  43.59666667 120.02333333  74.66666667\n",
      "  50.20333333 101.58333333  17.01666667 175.74       137.75\n",
      " 174.39       231.36666667 156.27       114.22        60.71666667\n",
      " 123.93666667 244.58333333  32.99       191.46        16.15\n",
      " 101.03       107.88333333 202.32       115.58333333  45.26666667\n",
      " 117.38333333  56.34666667  14.29666667 237.13333333  59.24666667\n",
      " 131.20333333 160.49333333  17.25       139.58666667 310.37333333\n",
      " 134.35        45.71333333 139.27666667 263.23666667 377.76\n",
      " 209.70666667  27.29333333  38.03666667 107.32333333  87.41\n",
      " 112.69        99.20333333 124.67       153.87666667 223.96\n",
      " 132.22       282.37333333 329.48666667 239.42333333 164.99\n",
      "   9.98333333 114.22666667  11.41666667  96.04       137.16\n",
      " 134.58        16.01666667 212.66333333 238.15666667 121.33333333\n",
      "  15.25333333 255.41333333 154.89333333 294.16666667  16.61333333\n",
      " 121.28333333 121.35666667  88.62666667 128.64666667  99.48\n",
      " 101.50666667 240.84       211.69333333 158.97       106.51666667\n",
      " 221.72333333 171.75       162.12666667 245.35333333 142.40666667\n",
      " 111.59333333 113.98       301.88       154.91666667  44.82\n",
      "  36.18333333 134.05666667 121.21       103.87666667 122.04666667\n",
      " 274.83333333 344.05333333 138.73333333 119.44333333 100.94666667\n",
      "  17.37       240.81        14.19666667 341.62       145.16\n",
      "  38.02333333 132.37333333 246.62666667 215.58666667  62.33666667\n",
      " 282.06       174.3        132.02666667 221.29333333 258.37333333\n",
      " 127.02666667 240.78666667 117.60333333 138.87333333  93.48666667\n",
      "  98.16666667  26.03333333 108.57666667  74.85       140.97\n",
      "  33.13666667 209.32666667 186.85333333  36.46666667 194.77\n",
      " 258.21333333 103.90666667  19.88        67.06       191.32333333\n",
      " 102.78333333 384.09       323.06333333 158.82666667 206.07666667\n",
      "  18.89333333 215.05333333  26.17       134.78       146.06666667\n",
      "  99.85       122.65       234.02333333 102.68666667  15.49\n",
      "  83.12333333 251.31333333 235.86666667 197.46666667 143.48\n",
      " 198.63666667  26.49333333 346.02        69.03        18.44666667\n",
      " 164.19666667  27.04333333 232.35666667  26.92       187.17666667\n",
      " 245.70333333  15.86333333 237.14333333  85.83333333 179.41\n",
      " 133.18666667 103.80333333  54.07666667 331.13333333  19.13\n",
      " 268.22666667  85.21333333 180.37       215.42333333  33.89\n",
      " 293.16       104.83        15.93       166.92666667 189.46\n",
      " 114.98666667 251.01       130.06333333  74.83666667 105.16\n",
      " 120.22333333 251.45666667 178.71666667 109.17333333 129.99\n",
      "  36.87       143.5         43.28333333 221.99333333  19.92333333\n",
      "  97.98333333 215.03333333 216.49333333 228.07333333 374.60666667\n",
      " 195.27666667 134.32333333  58.65666667 168.99666667  93.65\n",
      " 266.76       249.87       250.9         39.38333333 254.53\n",
      " 182.25333333 128.29        34.77333333  25.17333333  18.74333333\n",
      " 224.51333333  30.83333333 132.58666667 113.13333333  93.34666667\n",
      " 377.32       280.72        97.95666667  76.22333333 101.06333333\n",
      "  29.26666667  96.11333333 117.41666667 189.18333333 149.56\n",
      "  39.42       238.02333333 245.38666667 145.65333333 370.81333333\n",
      "  19.66        21.07       317.31666667 176.61333333 121.39\n",
      "  15.71        33.31        99.34       119.37333333 307.26666667\n",
      " 224.37666667 241.47333333  58.62666667  45.66666667 202.65\n",
      "  63.92        19.04666667  18.65666667 102.23666667 110.20666667\n",
      "  61.05666667 205.48666667 242.85666667 182.83666667 164.68666667\n",
      "  28.56333333  76.53333333 100.14333333  13.27       195.5\n",
      "  87.49666667  68.92333333 230.59       314.58       226.57\n",
      "  19.09        38.97       163.98333333 103.97333333  39.04333333\n",
      " 143.8        112.29       303.47666667 300.98333333 115.98\n",
      "  73.35       130.71333333 154.25       120.51       234.03333333\n",
      "  35.55       151.79666667 294.36333333  50.03666667 118.4\n",
      " 110.43666667 266.61333333  26.61666667  28.50666667 224.01666667\n",
      "  23.06        88.57666667  15.38333333 251.03333333  82.61666667\n",
      " 121.48       187.30333333 180.89666667 113.23333333 115.65333333\n",
      " 164.46333333 291.06333333 129.25333333 119.18333333  45.44333333\n",
      " 131.77       173.87333333 119.36        57.31666667  56.27\n",
      "  17.49        30.40666667  16.05666667  39.21333333 163.05333333\n",
      " 115.08333333 100.89       177.6        110.13333333 343.43666667\n",
      " 125.94       167.3        261.87666667 107.61666667 114.39666667\n",
      " 229.18666667 329.62333333  35.44       116.55       356.48666667\n",
      " 253.1        149.          39.48        96.5        111.93\n",
      "  97.23666667 109.91666667  29.17       327.09666667 298.45666667\n",
      " 228.62666667 114.22666667 155.75       239.27333333 133.64666667\n",
      " 130.61333333 238.92       105.57666667  57.07       119.54\n",
      " 206.20666667 123.34       115.86       117.98666667  94.45333333\n",
      " 138.09       279.52666667  99.41666667 163.75       187.95666667\n",
      " 235.87        51.82       328.63        90.14666667 155.24333333\n",
      " 169.39        12.79666667  29.41333333 236.72333333 286.06666667\n",
      " 105.47333333  99.22333333  27.70666667  15.40333333 261.\n",
      " 141.39666667 339.09333333 252.83333333  84.34       112.72333333\n",
      " 212.1        163.51333333  71.76333333  72.          44.43666667\n",
      " 185.96       108.46       229.59       302.02666667 223.57666667\n",
      "  14.89       223.65666667  52.27        71.92666667  39.54333333\n",
      " 237.04       241.21        30.37333333 153.79666667  68.62666667\n",
      " 232.57333333 224.65666667 209.79       207.56       374.22666667\n",
      "  99.91333333  36.7        250.78333333 140.75666667 181.36333333\n",
      "  30.98666667 232.04333333  18.28666667  51.1         17.95333333\n",
      "  94.65333333  29.72333333 205.96       189.64333333 152.98333333\n",
      "  35.63        86.39        33.85666667 308.04666667  96.19333333\n",
      "  38.09333333  98.77666667  16.31333333  44.78        31.94333333\n",
      " 308.31        17.63333333 181.29       250.79       312.11666667\n",
      " 225.61       103.97       119.53666667 107.53333333  99.61\n",
      " 205.95        31.88333333  18.05666667 146.49        16.79333333\n",
      " 175.37666667 141.48666667  39.21333333 148.52666667 204.13\n",
      " 223.21       338.18666667  27.68333333  98.72333333  35.\n",
      "  40.64333333  99.06666667 104.52333333  27.98       170.81333333\n",
      "  88.63666667 193.75       121.64       346.59       166.57\n",
      " 291.16333333 124.21666667 283.38        64.61        35.26333333\n",
      "  28.02666667  38.64666667  87.46        13.96        11.05333333\n",
      "  96.37666667  17.48666667 276.78333333 230.4        155.39666667\n",
      "  86.33333333 104.59333333  30.16333333 139.77666667  96.14333333\n",
      " 129.28333333 194.11333333  15.40333333 236.78666667 120.78333333\n",
      "  12.49       270.07666667 103.61666667  33.47333333  76.61666667\n",
      " 246.71666667 127.61333333 386.38666667 284.24       223.82666667\n",
      " 223.22333333 186.43        17.67666667 114.71666667 126.74666667\n",
      "  16.24        45.50333333  97.25666667 202.30333333 188.69666667\n",
      "  51.78333333 177.84333333  98.01333333 254.56       114.91666667\n",
      " 325.81        64.61666667  35.52666667  16.77       231.44333333\n",
      " 237.91333333  92.99666667 109.87666667  31.33333333  93.70333333\n",
      " 125.71333333 174.93        97.84333333 109.53666667  38.97666667\n",
      " 117.34       101.70333333  68.44333333 108.21       329.66333333\n",
      " 132.35       258.8        119.27666667 297.87666667 209.56333333\n",
      "  76.44333333 116.62        94.17333333 315.37333333 243.74666667\n",
      " 253.86       104.89       117.31333333 127.26333333  17.46666667\n",
      " 124.7        267.87333333  27.44666667  90.31666667 113.84333333\n",
      " 117.95333333 183.97       127.26666667 316.71666667  85.70666667\n",
      " 100.72333333 130.61333333 304.19666667 105.53666667 150.76\n",
      " 131.54666667  35.42       186.03333333  17.79666667  30.11333333\n",
      "  17.39333333 232.26       105.52666667 142.20666667 261.52333333\n",
      " 134.71333333 112.77333333  28.35666667 189.66666667  16.03\n",
      " 133.68        29.28666667  37.45333333  93.64666667  47.81\n",
      " 194.94333333  19.74666667  88.00666667 180.52333333  33.19666667\n",
      " 308.94       275.10333333  15.03666667 378.65666667 117.57666667\n",
      " 268.37333333  31.66333333  87.18666667  30.3        279.17333333\n",
      "  53.50333333  71.82333333 119.52666667 155.91666667 208.86      ]\n"
     ]
    }
   ],
   "source": [
    "# Predict with each trained model\n",
    "prediction1 = greedy_CV1.predict(test1)\n",
    "prediction2 = greedy_CV2.predict(test2)\n",
    "prediction3 = greedy_CV3.predict(test3)\n",
    "prediction4 = greedy_CV4.predict(test4)\n",
    "\n",
    "print(prediction1)\n",
    "print(prediction2)\n",
    "print(prediction3)\n",
    "print(prediction4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f64b7f9d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id   count\n",
      "0   0  100.40\n",
      "1   1  225.87\n",
      "2   2   88.21\n",
      "3   4   38.51\n",
      "4   5   51.47 \n",
      "\n",
      "    id   count\n",
      "0   0  101.61\n",
      "1   1  223.96\n",
      "2   2   98.67\n",
      "3   4   31.97\n",
      "4   5   50.42 \n",
      "\n",
      "    id   count\n",
      "0   0  102.80\n",
      "1   1  216.75\n",
      "2   2   88.91\n",
      "3   4   39.85\n",
      "4   5   45.65 \n",
      "\n",
      "    id   count\n",
      "0   0  106.83\n",
      "1   1  213.49\n",
      "2   2   98.30\n",
      "3   4   30.78\n",
      "4   5   45.63\n"
     ]
    }
   ],
   "source": [
    "# Save the prediction results\n",
    "GridSearchCV1 = pd.read_csv('data/submission.csv')\n",
    "GridSearchCV2 = pd.read_csv('data/submission.csv')\n",
    "GridSearchCV3 = pd.read_csv('data/submission.csv')\n",
    "GridSearchCV4 = pd.read_csv('data/submission.csv')\n",
    "\n",
    "import numpy as np\n",
    "GridSearchCV1['count'] = np.round(prediction1, 2)\n",
    "GridSearchCV2['count'] = np.round(prediction2, 2)\n",
    "GridSearchCV3['count'] = np.round(prediction3, 2)\n",
    "GridSearchCV4['count'] = np.round(prediction4, 2)\n",
    "\n",
    "print(GridSearchCV1.head(), '\\n\\n',\n",
    "      GridSearchCV2.head(), '\\n\\n',\n",
    "      GridSearchCV3.head(), '\\n\\n',\n",
    "      GridSearchCV4.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "392713b2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save the results\n",
    "GridSearchCV1.to_csv('GridSearchCV1_result.csv')\n",
    "GridSearchCV2.to_csv('GridSearchCV2_result.csv')\n",
    "GridSearchCV3.to_csv('GridSearchCV3_result.csv')\n",
    "GridSearchCV4.to_csv('GridSearchCV4_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1b33f0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}