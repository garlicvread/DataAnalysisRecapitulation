{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d912416",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### The concept of hyperparameter\n",
    "Hyperparameter tuning is one of the most important parts of a machine learning pipeline. A wrong choice of the hyperparameters‚Äô values may lead to wrong results and a model with poor performance.<br>\n",
    "\n",
    "Hyperparameters are model parameters whose values are set **before** training.<br> These hyperparameters might address model design questions such as:\n",
    "\n",
    "- What **degree of polynomial features** should I use for my linear model?\n",
    "- What should be the **maximum depth** allowed for my decision tree?\n",
    "- What should be the **minimum number of samples** required at a leaf node in my decision tree?\n",
    "- **How many trees** should I include in my random forest?\n",
    "- **How many neurons** should I have in my neural network layer?\n",
    "- **How many layers** should I have in my neural network?\n",
    "- What should I set my **learning rate** to for gradient descent?\n",
    "\n",
    "Let's make it simple. For example, **the number of neurons** of a feed-forward neural network is a hyperparameter, because we set it before training. Another example of hyperparameter is **the number of trees** in a random forest or the penalty intensity of a Lasso regression. As you can see, the hyperparameters are all numbers that are set before the training phase and their values affect the behavior of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb8eaee",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### IMPORTANT!\n",
    "Hyperparameters are **not** model parameters and they cannot be directly trained from the data. Model parameters are **learned** during training when we optimize a loss function using something like gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76db0ac3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "### The reason for tuning the hyperparameters\n",
    "Why should we tune the hyperparameters of a model?<br>\n",
    "\n",
    "That is because we don‚Äôt really know the models' optimal values in advance. A model with different hyperparameters is, actually, a different model so it may have a different performance. In the case of neural networks, a less number of neurons could cause underfitting and a more number of them could cause overfitting. In both cases, the models are not good, so we need to find the optimal number of neurons that cause the best performance.<br>\n",
    "\n",
    "If the model has several hyperparameters, we need to find the best combination of values of the hyperparameters searching in a multi-dimensional space. That‚Äôs why hyperparameter tuning, which is the process of finding the right values of the hyperparameters, is a very complex and time-expensive task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfeee4d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Hyperparameter tuning in practice\n",
    "Tuning hyperparameters means making decisions on the **stopping criteria**. There are several stopping criteria, but we're going to deal with four first, such as:\n",
    "1. The max_depth\n",
    "2. The minimum size of the node: min_samples_split\n",
    "3. The minimum lift: min_impurity_decrease\n",
    "4. The cost-complexity<br>\n",
    "---\n",
    "The **max depth** means the maximum number of depth in the decision tree. The tree structure cannot be deeper than this value we set using **`max_depth`**. The smaller it is, the smaller the tree will be.<br>\n",
    "\n",
    "The **minimum size of the node** is the number of data(samples) to split. The smaller the value, the larger the tree will be, and its default value is 2.<br>\n",
    "\n",
    "We can set this using **`min_samples_split`** A node will be split if this split induces a decrease of the impurity greater than or equal to this value. The equation for min_sample_split is:<br>\n",
    "\n",
    "$$\\frac{N_t}{N} \\times (impurity - \\frac{N_{tR}}{N_t} \\times right\\;impurity - \\frac{N_{tl}}{N_t} \\times left\\;impurity)$$\n",
    "\n",
    "Where<br>\n",
    "$N$ is the total number of samples<br>\n",
    "$N_t$ is the total number of samples in current node<br>\n",
    "$N_{tL}$ is the number of samples in the left child<br>\n",
    "$N_{tR}$ is the number of samples in the right child<br>\n",
    "$N$, $N_t$, $N_{tL}$, $N_{tR}$ are all refer to the weighted sum, if `sample_weight` is passed.<br>\n",
    "\n",
    "The **minimum lift** is a criterion to see if the association rules between the items are coincidental or not. We can set the minimum lift using **`min_impurity_decrease`**.<br>\n",
    "\n",
    "When the lift is the same or smaller than the value set, the tree will not split more. The smaller the value, the larger the tree will be.<br>\n",
    "\n",
    "For pruning, we can think of two types of it. The first is **pre-pruning**, and the other is **post-pruning**. Pre-pruning is also called **early stopping**. It means literally stopping the training early. And we can do it by setting the max depth or the number of branches. Post-pruning is the process of performing pruning after we train the model. We can do post-pruning using the cost-complexity pruning technique.<br>\n",
    "\n",
    "The **cost complexity** is a concept that is used in **cost complexity pruning**. Pruning is a technique to prevent overfitting by limiting the model by setting penalty coefficients for the impurity and for the decision tree being larger.<br>\n",
    "\n",
    "In practice, we can do cost complexity pruning by finding the **$\\alpha$** value with the least influence and prune the node with that value. The equation for cost complexity pruning is:\n",
    "\n",
    "$$R_\\alpha (T) = R(T) + \\alpha |T|$$\n",
    "\n",
    "where<br>\n",
    "$R(T)$ is the learning errors of the leaf nodes<br>\n",
    "$|T|$ is the number of leaf nodes<br>\n",
    "$\\alpha$ is the complexity parameter\n",
    "\n",
    "When we focus on reducing the ¬†ùëÖ(ùëá) ¬†value only, the size of the tree gets bigger. It means the tree structure has more branches. ¬†ùõº decides the number of leaf nodes to be remained, thus we need to modify it to prevent overfitting. The bigger the ¬†ùõº ¬†value, the more nodes being pruned will be.<br>\n",
    "\n",
    "Note that we need to calculate the $R_\\alpha (T_t)$ for the sub-trees. The equation is very similar to above one.\n",
    "\n",
    "$$R_\\alpha (T_t) = R(T_t) + \\alpha |T_t|$$\n",
    "\n",
    "---\n",
    "Using the stopping criteria such as above, we can set the optimal conditions for model training, and this process is called hyperparameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GridSearch\n",
    "\n",
    "Amongst the hyperparameter tuning techniques, GridSearch, a sort of exhaustive search, shows the best performance. GridSearch is a technique that finds the best combination among the possible combinations. However, GridSearch also has cons because the training consumes a lot of time.<br>\n",
    "\n",
    "For now, we will implement an exhaustive search using GridSearchCV module."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Downloading data\n",
    "!wget 'https://bit.ly/3gLj0Q6'\n",
    "\n",
    "# Unzip the downloaded data\n",
    "import zipfile\n",
    "with zipfile.ZipFile('3gLj0Q6', 'r') as existing_zip:\n",
    "    existing_zip.extractall('data')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import pandas and RandomForestRegressor\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Import numpy\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load data\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check if the data loading is successful\n",
    "print('============ Train Data ============\\n')\n",
    "print('Train Data Information\\n', train.info(), '\\n')\n",
    "print('Train Data Shape: ', train.shape, '\\n')\n",
    "\n",
    "print('============ Test Data ============')\n",
    "print('Test Data Information\\n', test.info(), '\\n')\n",
    "print('Test Data Shape: ', test.shape, '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check the features of the data\n",
    "train.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Handling missing values\n",
    "\n",
    "We have four options for now, such as:<br>\n",
    "\n",
    "1: Delete the missing value<br>\n",
    "2: Replace the missing value with a specific scala value<br>\n",
    "3: Replace the missing value with the mean of the feature<br>\n",
    "4: Replace the missing value using interpolation<br>\n",
    "\n",
    "Since the Jupyter notebook does not allow us multiple interactions in a single cell, we will write our code in separate cells.<br>\n",
    "\n",
    "- I noticed this after I wrote the bellow code. I was stupid."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print('Missing value handling options\\n')\n",
    "# print('You have four options now to handle the missing value.')\n",
    "# print('Input the number of process you want to, then hit the return key.\\n')\n",
    "# print('  1: Delete the missing value')\n",
    "# print('  2: Replace the missing value with specific scala value')\n",
    "# print('  3: Replace the missing value with the mean of the feature')\n",
    "# print('  4: Replace the missing value using interpolation')\n",
    "\n",
    "# user_input = input()\n",
    "\n",
    "# if user_input == 1:\n",
    "#     train.dropna(inplace=True)\n",
    "#     print('You have deleted the missing values!')\n",
    "# elif user_input == 2:\n",
    "#     print('Input the value you want to replace the missing value')\n",
    "#     inputted_value = input()\n",
    "#     train.fillna(inputted_value, inplace=True)\n",
    "#     test.fillna(inputted_value, inplace=True)\n",
    "#     print(f'You have replace the missing values of train data and test data with {inputted_value}.')\n",
    "# elif user_input == 3:\n",
    "#     train.fillna(train.mean(), inplace=True)\n",
    "#     test.fillna(test.mean(), inplace=True)\n",
    "#     print(f'You have replace the missing values of train data {train.mean()} and test data with {test.mean()}.')\n",
    "# elif user_input == 4:\n",
    "#     train.interpolate(inplace=True)\n",
    "#     test.interpolate(inplace=True)\n",
    "#     print('You have replace the missing values of train data and test data using linear interpolation.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check if there are missing values\n",
    "print(train.isnull().sum(), '\\n')\n",
    "print(test.isnull().sum())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Delete the missing values\n",
    "train.dropna(inplace=True)\n",
    "print('You have deleted the missing values!')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Replace the missing values with a specific value, or a string\n",
    "inputted_value = input()\n",
    "train.fillna(inputted_value, inplace=True)\n",
    "test.fillna(inputted_value, inplace=True)\n",
    "print(f'You have replace the missing values of train data and test data with {inputted_value}.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Replace the missing values with the mean of each feature\n",
    "train.fillna(train.mean(), inplace=True)\n",
    "test.fillna(test.mean(), inplace=True)\n",
    "print(f'You have replace the missing values of train data\\n{train.mean()}\\n\\nand test data with\\n{test.mean()}.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Replace the missing values using linear interpolation\n",
    "train.interpolate(inplace=True)\n",
    "test.interpolate(inplace=True)\n",
    "print('You have replace the missing values of train data and test data using linear interpolation.')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check if the null values are replaced well.\n",
    "print(train.isnull().sum(), '\\n')\n",
    "print(test.isnull().sum())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import libraries for visualiztion\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the minus figure to be printed properly\n",
    "plt.rc('axes', unicode_minus=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Hide warnings that do not necessary for the analysis\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize features\n",
    "for column in train.columns:\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.title(column)\n",
    "    sns.histplot(train[column])\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# When the data distribution is not even, we can use Min-Max Normalization\n",
    "train['hour_bef_pm2.5'] = np.log1p(train['hour_bef_pm2.5'])\n",
    "train['hour_bef_pm10'] = np.log1p(train['hour_bef_pm10'])\n",
    "\n",
    "test['hour_bef_pm2.5'] = np.log1p(test['hour_bef_pm2.5'])\n",
    "test['hour_bef_pm10'] = np.log1p(test['hour_bef_pm10'])\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, nrows=2, figsize=(20, 20))\n",
    "\n",
    "\n",
    "sns.histplot(train['hour_bef_pm2.5'], ax=ax[0, 0])\n",
    "sns.histplot(train['hour_bef_pm10'], ax=ax[0, 1])\n",
    "\n",
    "sns.histplot(test['hour_bef_pm2.5'], ax=ax[1, 0])\n",
    "sns.histplot(test['hour_bef_pm10'], ax=ax[1, 1])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute pairwise correlation of columns, excluding NA/null values.\n",
    "train.corr()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize the correlation\n",
    "plt.figure(figsize = (12, 12))\n",
    "\n",
    "# annot: optional. If True, write the data value in each cell.\n",
    "# If an array-like with the same shape as data, then we can use this option to annotate the heatmap.\n",
    "# The annotation will replace the heatmap's data.\n",
    "# Note that DataFrames will match on position, not index.\n",
    "sns.heatmap(train.corr(), annot = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# We also can visualize the data using bar plot\n",
    "sns.barplot(x = 'hour', y = 'count', data = train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Modeling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Declare the model\n",
    "X_train = train.drop(['count'], axis=1)\n",
    "Y_train = train['count']\n",
    "\n",
    "# Train the model\n",
    "model = RandomForestRegressor(criterion = 'squared_error')\n",
    "model.fit(X_train, Y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print the feature importances\n",
    "model.feature_importances_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualizing the feature importances\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "importance_values = model.feature_importances_\n",
    "importances = pd.Series(importance_values, index = X_train.columns)\n",
    "importance_top10 = importances.sort_values(ascending=False)[:10]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title('Top 10 feature importances')\n",
    "sns.barplot(x = importance_top10, y = importance_top10.index)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create train datasets by removing the less important features\n",
    "X_train1 = train.drop(['count', 'hour_bef_precipitation'], axis=1)\n",
    "X_train2 = train.drop(['count', 'hour_bef_precipitation', 'hour_bef_pm2.5'], axis=1)\n",
    "X_train3 = train.drop(['count', 'hour_bef_precipitation', 'hour_bef_pm2.5', 'id'], axis=1)\n",
    "X_train4 = train.drop(['count', 'hour_bef_precipitation', 'hour_bef_pm2.5', 'id', 'hour_bef_windspeed'], axis=1)\n",
    "\n",
    "Y_train = train['count']\n",
    "\n",
    "# Create test datasets\n",
    "test1 = test.drop(['hour_bef_precipitation'], axis=1)\n",
    "test2 = test.drop(['hour_bef_precipitation', 'hour_bef_pm2.5'], axis=1)\n",
    "test3 = test.drop(['hour_bef_precipitation', 'hour_bef_pm2.5', 'id'], axis=1)\n",
    "test4 = test.drop(['hour_bef_precipitation', 'hour_bef_pm2.5', 'id', 'hour_bef_windspeed'], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Check the shape of training and test data\n",
    "print('X_train1.shape: ', X_train1.shape, '\\n')\n",
    "print('X_train2.shape: ', X_train2.shape, '\\n')\n",
    "print('X_train3.shape: ', X_train3.shape, '\\n')\n",
    "print('X_train4.shape: ', X_train4.shape, '\\n')\n",
    "print('Y_train.shape: ', Y_train.shape, '\\n')\n",
    "print('test1.shape', test1.shape, '\\n')\n",
    "print('test2.shape', test2.shape, '\\n')\n",
    "print('test3.shape', test3.shape, '\\n')\n",
    "print('test4.shape', test4.shape, '\\n')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "392713b2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save the results\n",
    "GridSearchCV1.to_csv('GridSearchCV1_result.csv')\n",
    "GridSearchCV2.to_csv('GridSearchCV2_result.csv')\n",
    "GridSearchCV3.to_csv('GridSearchCV3_result.csv')\n",
    "GridSearchCV4.to_csv('GridSearchCV4_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1b33f0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}